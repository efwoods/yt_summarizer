[
  {
    "video_id": "CJn0WRKwg34",
    "transcript": "",
    "status": "error",
    "error": "No transcript available"
  },
  {
    "video_id": "Ux5AiKmra-E",
    "transcript": "right welcome I hope this is working uh welcome to neurolink live update uh we're going to tell you about uh the progress of the first patient with neurolink umand and uh sort of do a recap of of the progress there uh then talk about what uh changes we were're making for the second patient which we're um uh hoping to to do an implant in the next week or so um and this is for our first product which is called tathy which enables you to control a computer or phone just by thinking so let's uh in fact so so we'll start off with just some introductions DJ want to start hi everyone my name is DJ H I'm an electrical engineer and a chip designer by training uh I led the design of first several generations of the neuralink implant um currently I was on the founding team and currently a president I'm Matthew McDougall I'm a practicing neurosurgeon and head of neurosurgery at nurlink uh yeah go ahead yeah head of brain interfaces applications and I'm Bliss I'm a software engineer at nink trying to figure out how to turn brain activity into cool stuff in the world all right thank you well let's see so we'll just get going into the presentation so our first product is something I said we call telepathy which is enables uh the uh person with a neuralink implant to control their phone or computer just by thinking um and once you can control your phone or computer you can essentially control almost anything uh just and literally just by thinking so there's no uh eye tracking or anything it is purely uh purely your thoughts uh so uh this is a like really qu quite a profound uh device that can help a lot of people who have lost uh the connection between their the brain and body so U you imagine people like uh Steph hulking who uh you know imagine if he if he could communicate at the same speed as someone who had still had the connection to the brain and body um so it's it's really uh something that can help millions of people around the world and it's a it's part of our our overall goal of enabling a very high bandwidth connection between the the brain and uh and your and the rest of the world and your computers um the long-term goal which sounds little esoteric is to at the uh the risk of of uh the civiliz civilizational risk of AI uh by having a a sort of closer symbiosis between human intelligence and digital intelligence but uh but that that that'll take many years uh along the way we're we're going to help solve a lot of uh brain injury or spinal injury issues U so U and without first product telepathy that's that's going to be really quite profound uh that there is also potential longterm for uh Bridging the the the Gap so if there are damaged or severed neurons being able to to expand the gap between the brain's motor cortex to the spine to enable someone to uh use their body again I think that would be very exciting um and it's you know that that is something that is possible in long term um and they're not our second product which we've demonstrated to work with the monkeys is uh Blindside which would enable someone who is completely blind or lost both eyes or completely lost uh their optic nerve uh to be able to see so that's uh that's something that we hope to demonstrate in the future so this just gives you a sense of what the device is uh a way to think about the ne device is kind of like a a Fitbit or an Apple Watch with with tiny wires or electrodes those those tiny wires are implanted in the in the brain and they uh read and write electrical signals so uh a lot of people think the brain is this incredibly mysterious thing it's it it is mysterious in a lot of ways but but it is actually U it does operate with electrical signals so if you can read and write those electrical signals uh you can interface with the brain and um the the device is is sized so so that it is the same size as the as the piece of scull that is removed so if it's like a few centimeters diameter of SK skull that's removed we replace that with the device uh after implanting the the tiny wires with the surgical robot and that enables uh read R capability to the neurons completely wirelessly yeah yes exactly um it's completely wirelessly so like I could I could have a neural link right now you wouldn't know um and it it charges inductively so you could just uh basically have a electromagnetic pad that that that you charge the device with so yeah it's like an Apple Watch exactly so uh except that it's actually a much harder technical uh challenge to solve given that there's limit as to how much eat the brain tissue whereas in for phones and you don't actually really care if it's sitting on a table sure uh yeah it's Al it's got to go through skin and stuff as well in our case so it's it is a tougher challenge to to charge and to uh have high band with Communications given that it's got to go through uh skin and hair and stuff we have solved it but we have solved it yeah so yeah yeah so our first step with the telepathy is basically to unlock a digital Independence for people uh with paralyses and to allow them control the computer just with their mind without moving their body and uh our goal is to provide them the same level of control functionality and reliability that I have when I'm using a computer even better than uh the level of control I have it's not a high bar for you actually just to be clear this guy he's controlling this with his brain so he's not like you can't see his hands in this video but he's not using a mouse and keyboard just you know thinking about how to move the cursor and playing civilization no ey tracker right there's no ey tracking from he's liveing like you watch this on Twitter just thinking that's it just thinking this just a couple days cursor move here yeah yeah yeah this like last night or two nights ago something yes yeah I think I think the way he also described it is he's using the yeah he has many more videos on his uh yeah on the platform definitely check them out um yeah so he can he's streaming that live and also can talk and like move his head without problem mul yeah you also like if you join this live stream you can ask him questions he'll he'll tell you all about what it's like to move also I think U I haven't played civilization myself um but I think this is actually not easy mode this is expert this Emperor mode Emperor mode if you have played s Emperor mode is like the highest difficulty level just the point is like this is a ctively demanding task while live streaming playing the hardest mode of the game and uh he's able to do that while moving talking engaging with uh you know the audience while playing one of the other games he likes to play a lot is chess and I think it gets lost sometimes that he's actually playing speed chess against me yeah which requires an incredibly High Fidelity degree of control and and speed of control in order to be able to win so also another cool stuff about about our device is that you can use it anywhere anytime also on a plane during a flight while uh creating really cool memes of c um also our device unlocks things that previously were impossible for our participant for example uh we're able to connect him to his gaming console switch and play Mario Kart with friends and family and it was lovely to see them playing together after years uh that he couldn't do it since he's injured imine if you're sitting one ra over from this guy on the plane look over he's making a cat M No Hands no movement live in a world world yeah it's strange strange time yeah and uh he loves using the device and using independently daily to watch videos uh read uh play games uh using the browser and the key metrics that we care is to make sure our device is actually useful is to is basically the amount of hours we use the device daily and weekly and we track it uh weekly since the since the surgery and on weeks that he's not too busy and not traveling he can even reach 70 hours uh of using the device is a week and this is amazing um he would of course love to use it more but need to run resarch sessions uh he needs to sleep sometimes and also of course to charge the device once in a while hopefully we'll improve that over time I think maybe not obvious to people who are watching this like it's a normal MacBook he's controlling this isn't like some limited edition thing where there's only a few options like he can just do anything that you can do on a MacBook Pro same one I have on my desk actually it's the exact same one and maybe another interesting point is that on the first day he used BCI turn control he was able to break the previous world record for cursor control just by uh uh using the brain and recently he even doubled it and was able to uh outperform about 10% of our engineered neuralink and you can be sure that we are very good in this game and very quick and if you want to check out how well how well you can do it you can do it on our website and it's very uh addictive games yeah it's a very simple game you just have to click on the Square but uh but it's it's it's it's actually even though it sounds silly it's it's quite a yeah it can be quite it sounds like it can be quite addictive and it's especially if you get a low score and you think there's no way I got to so I I mean any anyone who wants to try this I recommend going to the neurol link.com website and seeing seeing if you can beat Nolan's record and it's that you will find that's actually quite difficult to do so um and this is really with version one of the device and with only a small percentage of the uh electrodes uh that that are working so this is uh this is really just the beginning but even the beginning is twice as good as the world record this is important to emphasize um the you know the media has a habit of of uh saying that the glass is 10% empty uh but but actually it's 90% full so uh I think it's really quite an accomplishment of the nuring team to um have achieved with ver with the first patient the first device uh uh twice the world record for the uh brain to computer uh bandwidth that's a really an astonishing an amazingly great outcome um and it's only going to be get better from here so the potential is to ultimately get I think to megabit level um so that's that's part the long-term goal of of improving the the bandwidth of the computer interface if you think about like how low the bandwidth normally is between a human and a device it's the average bandwidth is extremely low it's I say less than one bit per second over the course of a day so if there are 86,400 seconds in a day uh you're outputting less than um that number of bits to any any given device except in perhaps very rare circumstances so uh the this is um actually quite important for um for AI you know basically for for human uh AI symbiosis is just being able to communicate at a at a speed the AI can follow so yeah just to emphasize again he's performing at this extremely high level with about 15% of his channels functional um and so we want to mitigate any of the problems that led to that situation so you know the brain is a fascinating organ uh I'll share with you some of the secrets about the brain during any typical brain surgery a small amount of air is introduced into the skull um that's because neurosurgeons like to have as much room as possible around the brain and so uh there's this little known control mechanism of allowing the CO2 concentration in the blood to rise a bit uh which allows the brain to either expand or contract depending on where you target that CO2 so typically neurosurgeons will have the brain shrink by lowering CO2 what we're going to do in our future surgeries is keep the CO2 concentration actually quite normal maybe even slightly elevated that'll allow the brain to stay its normal size and shape during surgery that should eliminate this air pocket that we saw in the the first participant that air pocket we think may have contributed to eating up some of the thread slack uh as as the air bubble migrated to be under the implant push the brain away from the implant and so that's easy enough to fix another consideration that we want to focus on for our upcoming participants uh is that the brain think of it like a really complex folded onion it's layer upon layer of sheets of neurons all over the surface of the brain folded into this um you know oddl looking shape the folds of the brain travel down deep into the brain and and along with it Go those onion layers of neurons and if we insert very close to one of the folds where there may be very useful information encoded in neurons we may end up traveling with our threads parallel to some of the layers of neurons that were most interested in avoiding them entirely uh to avoid that possibility we're going to insert uh in our future participants more close to the middle of the apex of the folds uh ensuring that we're crossing the layers of Interest layer five of the CeX I also think that it's important to um highlight here those tiny wires that Elon mentioned uh they're they're fraction of a human hair they're very flexible uh intentionally so because you know brains constantly moving and you want the electrodes to be moving with the brain causing less of the scarring and um it's it's actually impossible uh for a human neurosurgeon how however talented Matthew is to actually maneuver them by so we have a surgical robot that we built that can actually precisely Target them in any threedimensional space XY as well as Z with Micron level Precision while avoiding vasculature so that you don't disrupt um the the and and cause immune response from happening so uh we we actually have the technology to be able to place them exactly where we want them in yeah it was truly amazing to see the surface of the brain after the robot had inserted all the electrodes on the first participant without a drop of blood in sight um is really quite an achievement yes so something that probably most people don't realize is that the the brain appears to be sort of somewhat undifferentiated so if you look at the cortex it looks like a whole bunch of folds that were you know maybe like it's it's it's not obvious just looking at a say a picture of the brain that uh that that it's the brain is highly differentiated that there's you you you pretty much know exactly where the part of the brain is that uh controls your right hand and your left hand and your leg and that that kind of thing or or Vision it's it's actually U quite precisely located it's not uh some people like might might think look at the brain like oh could be could be anywhere but but actually we it it's it's your brain is is highly differentiated even though it doesn't look it's yeah do you want to describe how we actually we like how we identify where the drill the yeah so we can we can put a patient that is considering this implant uh into an fmri so a functional magnetic resonance imaging machine and ask them to imagine hand movements that you know because of the spinal cord injury don't happen but just imagining those hand movements causes these areas of the brain to light up in the fmri scanner and so we have a pretty good idea based in in fact for each individual participant which part of their brain is going to um you know respond to imagined movements of the hand and so we can map those imagine movements much as we all do uh when moving a mouse to controlling a cursor on a screen even without the use of a mouse yeah but anyway I think this is kind of an important point that like it's not like your the part of your brain that controls your hand might be anywhere in the cortex it's this is not the case it's going to be in a very specific region and it's going to be um extremely common across people Precision is key too yeah um the left-handed right-handed my mind too like if you're right-handed you want the device on the left side yeah lateral side to the hand that's your dominant yeah the left side of your brain controls right side of your body yeah everything's CR yeah another of the risk mitigations we're looking at in the future yeah is that you know that the implant has a certain size the depth of the bottom of the implant is actually thinner than the average human skull and so what we want to be able to do is control the size of the Gap under the implant give the threads that travel from the implant into the brain as much slack as possible uh we didn't do this in the first uh participant because we didn't want to you know manipulate any of their issue that we didn't absolutely have to in upcoming implants our plan is to of sculpt the surface of the skull uh very intentionally to minimize the Gap under the implant such that the bottom of the implant travels perfectly flush with the normal Contour of the inner side of the skull that will put the implant closer to the brain it will eliminate some of the tension on the threads and we think it will reduce some of the tendency of threads to retract brain and we actually built a tool to do right yeah this this is actually this is a very important detail uh you really want the the inner Contour of the skull to be flush so the implant there's there's no the brain doesn't want to Puck her up into the into the Gap that's really quite a big deal so like like minimizing the air pocket um and the implant being flushed with the the the inside Contour of the skull is are two very important uh improvements the additional benefit here is that uh you know you do see some amount of stick up what we call stick up so you minor bump in the head but this actually eliminates it even further yeah yeah I mean it's like really our goal is that that if you run your hand over the top of the skull you don't feel any any bump you don't feel any any device um and that even if someone was bold you wouldn't really even notice it um and uh and then from the the in inner cont of the skull that the the brain from a physical standpoint doesn't really notice that there's a divot in the skull mhm because there's no divot okay another aspect of uh of the human brain that you know obviously differs from any of the animals that we tested in uh is that the human brain is a lot bigger and so you may not realize that that means the the human brain moves quite a bit more uh than uh any of these other smaller brained creatures and so when we open the skull uh we see the brain travel toward and away from the robot about 3 millimeters in total as the heart beats and and the breathing takes place and so that movement uh you know it it adds a small challenge for the robot uh in precisely choosing a depth to insert each thread it's not an enormous Challenge and we've already upgraded the robot capabilities to be able to even more precisely Target depth in in even a very rapidly moving brain uh with a high amplitude of movement you may think the most obvious mitigation for Threads that pulled out of the brain is to insert them deeper we think so too uh and so we're going to uh broaden the range of depths at which we insert threads so you know for the very first participant we had an enormous amount of data from our animal work and we had very highly optimized our insertion depth to maximize uh the crossing of layers of interest in the cortex with the electrodes that we're recording from now that we know retraction is a possibility we're going to insert um at a variety of depths that even in several cases of differing amounts of retracting threads we're going to have electrodes at the proper depth and with the deepest threads be able to track how much retraction has occurred across the surface of the brain um from from each thread and so we're going to you know both have more threads in the right layer and have better data on how much retraction has occurred if you're a BCI nerd you might know that being able to control individual Z depth per thread is not something that most uh neural interface devices offer most neural interface devices are kind of a static fixed rigid array that you push in and all the electrodes are at one depth right to be able to do this is actually pretty pretty novel part of the robot yeah the historical approach is to actually pound in a sort of bed of nails with an air hammer into the brain it looks crazy that that that is yeah just with a with a Pneumatic Hammer that's the that's this is it sounds somewhat barbaric this is not what we do but this is the what's been done before is literally just hammering in what looks like a better Nails into the brain which actually works it's astonishing that it actually works but I mean some people like manually like DBS probes you're just sticking in by hand search is just guiding them in those are several several orders of magnitude more volume of brain tissue that you're destroying compared to what we're doing but that deep brain simulation stuff does actually work it actually helps people a lot Yeahs of thousand yeah yeah that's a great product yeah but I mean I think we'll we'll be able to do um a much more finessed version of that down the road um so uh I mean it's really difficult like the the the neuralink device is something that really absolutely minimizes damage to the brain absolutely minimizes the load on the patient um and the goal is to allow someone to live a completely normal life um they they you won't even notice that someone even has the device um so like I said restoring the ability to control your computer and phone that's telepathy and then next device being able to allow people to see that could not see before in fact you could you could allow people to see kind of like dordy Le Forge in Star Trek in any what whatever infrared yeah infrared ultraviolet um radar um so so so I think another way of saying it is that we want to give people superpowers so it's it's not just that we're restoring your prior brain functionality but that you actually have functionality far greater than a normal human that's a super big deal MH and and I also think you know often times the questions that we get a lot is why do you have to actually go into the brain what if you place it on the surface or outside the skull basically the long story short the physics of how it works you really need to get the sensors which are these placing in the brain next to the source which neuron as close to it as possible otherwise what you get is you get a population response and not be able to kind of do the level of controls that we believe of yeah I mean May uh a good sort of analogy would be like if you're trying to understand what goes on in a factory you kind of need to go into the factory you can't just put a stethoscope on the wall um and try to figure out what's going like anything on the outside of the the trying to read things from the outside is like put putting a stethoscope on the wall of a factory trying to understand what's going in the factory it's not going to be effective be in you got to be threads are got to be in there um so um but I just want to be emphasize again like the goal is to give people people superpowers um not not just to restore prior functionality so I that's very exciting um and I think that should give hope to a lot of people in the world that the future is going to be exciting and inspiring and uh the technology is going to give them superpowers I mean that's that's amazing so yeah I guess these off yeah could can you multitask with it yeah in fact if you look at Nolan's streaming and you can just uh check out Nolan streams on on the xplatform um he's multitasking all the time so he's playing video games while talking and uh listening to podcasts listening to podcast yeah yeah exactly so uh it's it's really just like if you you're using your hands and you you you can be you know playing a video game while talking so I mean don't take a word for it there just go watch I mean yeah he's out there on the internet doing his thing yeah yeah exactly uh so can you do keyboard shortcuts or is it just the mouse yeah that's actually what we're working on right now oh sure So currently he's walking the mouse but we are also exploring recording more Dimensions out from the new activity multiple clicks uh so to do shortcuts or just able to control more games like control Co games with an Xbox controller uh but also in the future we expect we plan to expand to decode uh text not just the mouse control but also allow our participant to type much faster and yeah yeah actually so maybe uh going back to the discussion of thread retraction you know one of the very exciting parts to me about this story is that we're able to do so much with 15% of channels when you have more channels what that actually offers you is not just faster Mouse control because in the motorex neurons don't all represent the same thing so if you're trying to understand like uh you know what an individual finger trying to do uh you might or might not have an electro next to it and the more channels you have in the brain the higher likelihood you have know representation or decodability of all fingers on the hand and so if you're trying to do something like output text at a fast rate is something that matters a lot for people who are completely locked in who cannot speak at all who are trying to you know just say I love you to them to a loved one in their family or ask for a glass of water or a scratch or whatever you know being able to type it a fast R it's extremely important and the more fingers you have access to higher probability you can do that efficiently and so yeah you know I'm super excited about high how high the ceiling is we can uh that we can get to as we resolve this dat retraction issue yeah I mean we're C we're currently at approximately 10 10 bits per second uh PE great but uh ultimately we want to get to a megabit uh and I think say ultimately whole brain interface I think you know many years from now I think gigabit level is possible um so that's uh that's pretty astonishing um now you know with there's a still version one about device as we mentioned it's version one with only 15% of the threads working U the current device has uh 64 threads with 16 electrodes on each thread um our next device has 128 threads with with eight electrodes per per thread um because as we get more confident about how where exactly to place the uh the electro the thread you you need fewer electrodes per thread so we can essentially with the current Dev without substantial changes I potentially double the bandwidth if if we are accurate with with the placement of of the threads um and then our next Generation device will have maybe even more channels yeah yeah of year yeah so our next device we aiming for yeah uh 3,000 channels uh so this will just keep getting better and better uh really moving up I think in or magnitude in factors of 10 basically and what kind of bandwidth so I think won't be it won't be all that long before uh someone with a neuralink device can communicate faster than someone who is has a fully functional uh body and yeah so uh I think you know faster than the fastest speed typist or auctioner the Esports tournaments are going to be like won't be able to speak faster than someone can communicate with a a neuralink leftly device maybe a very interesting part of this basically we currently um uh connect to standard uh inputs to the computer through Mouse and keyboards but very soon as we will have a much hard bandwidth we need to think about new ways to actually build the interface for the devices this is something that we very excited yeah no that's that's a good point um because the the current um input devices are centered around human hands yeah so it's like we've got these you know little meat sticks that we move and um the this certain rate at which you can move your little move your fingers and and so we've got like The Mouse and the keyboard and or the joystick control you know like Xbox controller or something like that uh but you really don't need that you can actually uh you don't you don't you don't need since you're no long if you're if you're not trying to use your hands you don't you actually don't need those uh conventional uh control mechanisms um and so this is why like ultimately I think you'll be able to do uh conceptu ual uh telepathy like where you can communicate entire Concepts uh uncompressed to someone else with a neuralink or to the computer even today we have some problems here where like you know if you don't feel the mouse clicking under your finger how do you know it actually happened you know you're you're seeing it on the screen but you don't actually feel the mouse click you don't have the appropriate accepted feedback of you know the keys under your fingertips or the trackpad under your so there's all sorts of interesting ux challenges how to actually give the user some sense of what their decoder is actually doing what they what the earing is actually doing when they're trying to use so Wireless yeah it's Bluetooth um it's just a Bluetooth connection just like how your normal Apple mouse or like apple magic keyboard connects to your computer same exact thing in fact in yeah we can basically have this exposed as an HID interface if we want hiid is just the name of the protocol for like sending bits from a mouse into a computer uh yeah I plug into basically anything yeah yeah I I think we we chose that interface because it's ubiquitous basically any devices are are have Bluetooth capabilities our our long-term goal is to actually have our own protocol you know that is safe and secure U but for now you know we've chosen it for interoperability so the question is can a neuralink chip repair the paralysis in the long term you know we can't do that right now we have done sort of preliminary work implanting a second neuralink in the spinal cord and we can restore naturalistic looking hand and leg movements in animal models um but this isn't something that is you know don't don't hold your breath waiting for it it's going to be a while we've got a lot of work to do but yes there's no reason in theory that we can't repair paralysis yeah um I mean essentially to to I mean there's there's no there's no physics barrier to fully solving paralysis that is perhaps a way to say it that you've got signals coming from your motor cortex uh that uh if they are transferred past the point where the the nerves are damaged essentially just it's basically a Communications Bridge um so you bridge the communications from the motor cortex um past the the point uh in the necr spine where the nose are damaged and you should like it just phys it is possible from a physics standpoint to restore full body functionality from a physic standpoint it's a very hard technical problem but it but it is there is nothing that prevents it happening from a physics standpoint so in terms of next phase of roll out well um we really want to make sure that uh we make as much progress as possible between each neuralink patient so this is we're only just moving now to our second neuralink patient um but we we hope to have uh you know if things go well High single digits this year uh and uh I don't know maybe this is somewhat dependent on regulatory approval U and how how much technical progress we make but within a few years hopefully thousands yeah and I think one thing that is important to highlight is that you know it's not that we' built only one device and one surgery we've done hundreds of surgery we' built thousand thousands and thousands of devices even for just the the ability to unearth any sort of lowf frequency failure mode so we have already been investing very heavily in infrastructure to be able to scale this thing on the device manufacturing side as well as on the surgery side of things we want to be able to help as many people as quickly as possible we go through obviously the appropriate hurdles right that are regulatory challenges and proving out the device with yeah and the the device implantation really needs to become um almost entirely if not entirely automatic uh in the same way that say lasic ey surgery is done U you know you don't have an opthalmologist with a a laser cutter by hand that that would be crazy uh but the opthalmologist oversees the uh the lasic machine and make sure that the settings are correct and then the the machine does everything and restores your eyesight uh it's really remarkable how many people have had their eyesight restored uh with with lasic and I think there's another one called smile it's they keep making it better we need to have something similar for a neuralink implantation so that you basically sit down and whatever the the what whatever kind of upgrades or you know brain fixes are needed um that's that's reviewed uh by medical expert OB we want to make sure that that is reviewed correctly but but it really needs to be automatic so you sit down and and within 10 minutes uh you have a neur link device installed very very fast I mean it's very sort of cyber punk you know dosx if you play those games when we new start to interface with other devices like wheelchair is great question we currently focusing on uh controlling computers and unlock Independence in the virtual world of course our plan is as we mentioned earlier robotic arm and wheelchair to unlock Independence in physical world this of course add additional risk if you make M your computer there's some to that but we are working with the FDA to allow us to quite soon well it seems like if if the wheelchair has a an app well the wheelchair just needs to have have an interface it does so if if the wheelchair has a Bluetooth interface uh you you could just Bluetooth interface to the wheelchair yeah and and um but that's probably something we should do we're pretty soon it's really a matter of paperwork of showing that you can do it safely you don't like drive off a cliff well I think we well we can the speed so it doesn't go King off into disaster um but uh you know so just make it go slowly at first uh but yeah so uh being able to sort of really the the nework device just should work generally for anything that's got a Bluetooth interface including potentially an Optimus asking uh yeah you yes you could communicate with Optimus uh yep absolutely Optimus will well we Al also be able to talk to Optimus but like why not just beam it but you could just yeah instead of talking just you could just beam it directly or if if someone has lost the use of speech then then they can still communicate to an Optimus uh they can communicate telepathically to Optimus or by bluetooth um and um and and so even if someone has you know completely less the ability to speak they could still uh control Optimus or with their computer or phone I mean also like if you have an optimist and you have a neuralink you can just directly map the brain signal to control of the physical arm of the robot and that's a very meaningful thing like if you're you know folks that have spin cord injury one of the biggest requests is to be able to scratch yourself this is something that quite annoying actually um and if you have a scratch on your face like you can't fall asleep until you scratch it uh you know it's very convenient to be able to move something physically towards you to be able to scratch similar things like eating food you know if you need somebody to feed you very hard to have know dinner with friends in a way that is you know sort of a normal social experience and so if you can feed yourself pick up a fork and actually eat a piece of chicken on your own uh you know that's a big deal uh it prevents and saves a lot of interactions with caretakers and other people in your life that you rely on to take care of you it really increases your I think an exciting possibility long term also is to say um if you take parts of the optim Optimus humanoid robot and you combine that with a neuralink let's say somebody has lost their arms or legs uh well we we could actually attach an Optimus arm or Optimus legs uh and uh do a neuralink implant so that the the motor commands from your brain that go would go uh to your your biological arms now go to your robot arms or robot legs um and again you you'd have basically cybernetic superpowers actually so the latency from the nurlink to your hand would probably be slightly faster than it is just to go to your physical hand so you can imagine like if you're a piano player or a I don't know anything that requires extremely fast know hand movements that you could actually have a pretty imbalanced right-and robotic arm control versus leftand physical Arm Control because one of them yeah like I said this is kind of a cyberpunk DEX in future where you have cybernetic upgrades that are actually better than your biological uh lims and uh it's certainly the we'll have a much you know as as particularly as we expand to a large number of of um of of customers or patients for neuralink uh the understanding of the brain will improve dramatically uh because the really there isn't a fine very fine grained understanding of the brain today because there just the sensors aren't good enough you got fmri which is pretty good but it's still not as good as actually having um high bandwidth electroids in the brain yeah I think this is underappreciated as a research tool to to move that whole effort forward of really knowing you know what the physical substance of human thought is we don't know uh to the to the degree that we need to so neuralink is actually a a very powerful research tool yeah I mean we I think we can ultimately uh understand and and fix uh quite severe psychosis or like if if somebody's got like the if somebody's got like a a like a delusion that they have a chip in their brain yeah I was wondering if you're going to mention that one um we just want to be clear that there's only one person with a neur link chip in their brain um so for people out there who think we've put their chip in their brain we wouldd like to assure you for what it's worth you probably won't believe us but we did not put each chip in your brain okay um so there actually a remarkable number of people who think we have put a tri in their brain but we have not um but in the future if you would like us to put a chip in your brain which will perhaps help with the issue of thinking that you have a chip in your brain uh then we will be able to do so uh so there there are people that have uh severe schizophrenia they've got basically things that um their brain is malfunctioning in some way and um and this is actually due to really like physical circuitry issues you can think of the brain as like uh really it's a it's a biological computer and if if some of the circuits are crossed it's going to uh you know it's going to crash or it's going to have issues that caus it to not work um but with a neuralink device we can fix those issues and uh you know give someone who I think pro has say severe schizophrenia or or psychosis of some kind uh allow them to live a normal life I think that is one of the likely things in the future so uh yeah I mean yeah you can certainly imagine like uh I'm sure people have like parents grandparents who've uh you know have memory that's uh not working as well as it used to be sometimes they they forget who who their grandchildren are or or what day it is and this is something that a neur link device could help fix I mean that that's actually one of the personal reason um in many way like forms of you're literally losing your and part of your identity yeah which is a just a very very go through yeah and it's really just it's a glitch in the biological computer that is uh a fixable uh glitch it's like like a it's a short circuit essentially how does the device charge and how long does the charge last yeah so the current version that Nolan has it lasts between four to five hours on a sing charge and it takes about 45 minutes to charge one thing we've learned from Nolan is that that's actually one of the main limiters for him using it more uh it's actually pretty hard to use a product more than like 70 hours a week but that's about what he has used it for it in some weeks yeah 70 hours in the week yeah I mean just for context like you sleep roughly eight hours a night so that's you know we're doing better than the bed like the bed is 56 hours a week of use roughly and uh so 70 hours a week of uses I challenge you to think about products that you've actually use for that duration but that's again some of these points are worth like emphasizing again like the that noan our first neuralink recipient has used the rolling device for 70 hours in a week which is incredible you probably won't enjoy that I'm sharing his computer use publicly but well I mean I assure you it's for productive things only no um but actually so one of the things we've learned is that in the next version of the device we really need to like double or you know increase that battery life and so I think uh DJ the next version is going to be double actually double without without increasing the Char correct same charging time double the battery life meaning you should get roughly 8 hours of use and the goal is to actually get to all they use so you can just charge um you know maybe in your sleep sleeping pillow exactly as soon as you got like 16 hours of usage then you basically have 24 hours of usage because it can charge while you're sleeping one other thing that's important I think to call out here is if you're paralyzed you can't you know put the charger over your head yourself and so it's important to think about like it's not just duration of bettery use but also can you recharge it yourself independently so we spend a lot of time thinking about how to make that feasible because then that means that you can this is what no one does you can use the device charge it use the device charge it use the device without needing anybody to come in and sort of help you with that which is a big deal if you're trying to play C until 5:00 a.m. at night when your family's asleep and the way in which he does that is that there is a charger coil that's a bigger you know about this big um and we actually put it in the um sleeve of a of a hatan or a beanie and then he wears it and then says with the voice command charge charger energize that's the one he likes how writing work uh so so uh yeah the current device that Nolan has is is is is reading um so it's trying to read his essentially like wrist movement from from one one hand um that's also you know with with like in the future like would pretty cool to give Noland a a second implant that would allow the other hand to be used and also have higher uh obviously higher active electrode count so then you can play two essentially play games two-handed because that's nor only how you play games um and uh but then with with writing uh it's really just uh it's an electrical impulse instead of like reading electrical impulses from the neurons you you issue an electrical impulse uh which is obviously critical for vision so vision is is writing which is just triggering electrical impulse in the vision part of the brain um and that like activates a a pixel so we we actually do have this working um in monkeys and we had have we've had it working with monkeys for a while now uh where you can sort of flash a pixel and then you watch where the monkey obviously the monkey's like what that monkey a little surprised to see like hey there's a flash here and a flash there but it's gets used to it after a while uh but it just you you can see that that the pixel in the right location because the monkey's eyes will will Dark to that location it's not on on the screen like there's no pixel on the screen there's no pixel on the screen in your brain yeah just like just verify that that the the that you're triggering a pixel in the right part of the brain so um you know the initial resolution for uh Vision will be relatively low you know sort of Atari Graphics type of thing but over time it it could potentially be better than normal vision and then I guess in terms of some additional applications for where writing to the brain can be useful is M uh applications as Bliss mentioned there is feedback there's appropr acceptive feedback there's a ttile feedback especially for robot arm like you're trying to grasp a cup you need to know you got it one one an egg to know it's a very much a delicate balance of not just initiating the movement but getting the feedback and controlling it accordingly so there there there is a subeta sensory cortex that's right adjacent to motor cortex could could be benefit motor movements so any changes in neural growth after the device is inserted we don't see any any signs of neural damage uh but I and I guess we we have seen some rebound on some of the electrodes right correct and then also I mean I guess I guess you know rain is very plastic yeah it's not that plastic well it it does diminish quite a bit after age 20 throughout childhood uh especially when you get to age about 25 brain really is done cooking yeah but there are there is a little bit of damage done with each insertion uh but it's a minuscule amount compared to anything else U there and so um it's an easy amount of damage to recover from and it's really only detectable on cutting pieces of the brain after uh after the animal's no longer alive and looking at them under a microscope you can't really tell during life that there's been any brain another way to interpret this question have the if you find any changes in neural growth after the device is inserted one way to interpret that is like the user learning how to use the device and I think on that side of things there's been tremendous progress he's put in hundreds of hours trying to figure out the best way to use this device cuz he really thinks that you know if he can figure this out he can help share this knowledge with I mean he's like on Friday night at 800m you know he's starting a session of like you know figuring out himself how to how to push his own performance to the next level and uh that's really a unique learning process cuz there's not many people in the world that had the experience of moving something with your bra and so there's a lot of nuance to like okay how exactly should I imagine or attempt to move my wrist to get that thing to uh yeah he's really died that into also just the sheer number of hours that he's used even in the past six months right yeah um in many ways like I me he's using it in his travel in his plane right effectively BCI has left the lab yeah yeah yeah I mean one of the questions is how close are we converting thoughts into text um I mean right right now it's more about Moving Co from the screen on on a virtual keyboard um but um long term you should be able to really transmit entire wordss faster than anyone could possibly type able to type hello world today directly from but we're still in the early days making that a polished experience I mean the other things that we're looking at is sign language right at the end of the day it is a movement of a of and into yeah that's true was the brain trying to naturally push the threads out I mean this is sort of a universal feature of any implant in the body the body tries to reject it uh and the goal of the surgeons and the technology team is to fight that and so with artificial hips and with you know screws in the spine we've done a really good job of finding biocompatible materials and techniques to uh fix those implants in the body I mean past a certain age it's getting hard to find someone without some kind of implant you know a knee hip uh some kind of screws in their spine um and so we've got this problem pretty well solved uh so to answer your question yes the body is trying to get rid of any implant but we can ensure that basically can't it's also worth highlighting that the threads have not actually moved um in the past five months um there's there's some still minor movements in terms of like some maybe maybe getting pushed in a little bit pushed out a little bit but it's it's more or less very stable and been stable for and the reason for that is you know once you once you do um a brain Sur surgy it takes some time for the tissues to come in and then and then you know the start tissue or the neom membrane to actually come in and then anchor the threads in place and once that happens everything has been stable and seen much movement that's where the world record performance starts to come in yeah that was a couple weeks ago yeah yeah the threads like it is important that the threads be extremely tiny if they're extremely tiny then the the brain uh does not the smaller they are the less likely the brain is to react to to them so that's why you want the threads to be extremely tiny and also to minimize any damage to neurons um so by way on that note we we do plan to actually share some of the um you know the tissue response in detail in some of the the later upcoming updates yeah it is quite a challenging um it's challenging on many fronts to do something like this uh because you're you're trying to read read and write electrical signals but you need to have the threads themselves need to be uh uh like electrically isolated um and and not subject to corrosion in the body so like the you know just metal by itself is so much subject to corrosion or or being attacked um so it's it's it's like in terms of the various Coatings and things to actually make this electroid work while not actually eroding its performance over time is is very difficult human bodies are very very harsh it's it's a bag of salt water with B sensors that's elevated temperature that is well regulated mean I'm sure people have experience dropping their electronic devices in a seawater and in an instance yeah yeah so we we better sort of wrap this up soon um I if there's like a few few last questions um yes so a good question so what about uh upgrades um so yeah we we we do think it's going to be important to be able to upgrade the device over time uh just like you wouldn't want um like an iPhone 1 stuck in your brain forever uh you you know if you've got an iPhone 15 you probably want the iPhone 15 not the iPhone 1 um so U I think people over time will uh be able to upgrade their their neural link so we'll take the neural link device out um and put a new one in um and uh we we have done this with um some of our um animals and they've actually in one case we did it with we we upgrade our device three times and and uh with a pig we did with monkey as well able to do BCI yeah so and he's he's doing fine yeah has impant actually hit his I think his record with the last yeah with the with an upgrade no still beat him though no still beat him yes this is true humans are top of the species leaderboard right now pag is like what like eight or something p is like 8.5 BPS okay well it's a very high score I'm not trying to put Pedro down and also to train a monkey to do that is like a whole Challenge on its own we have like the best animal care team world yeah I just do want to emphasize we we we we do our absolute best to take care of the the animals um and uh uh when we had like a USDA inspector come through she said that uh this was the the nicest animal uh facility she's ever seen in her entire life I mean they breakfast on an app like the the monkey orders room service yes I'm not even kid we we have monkey room service which is a rare um in fact we're the only ones who offer monkey room service so we really do everything we can to maximize the welfare of the animals so all right with that uh thank you everyone for tuning in uh hope you found this interesting",
    "status": "success",
    "error": null
  },
  {
    "video_id": "88I7gLR5v_A",
    "transcript": "PRESENTER 1: Welcome\nback, everyone. It's a great pleasure to\nintroduce Mengmi Zhang. She's one of the top post\ndoctorate scholars in my group. And she's going to do a\nvery exciting presentation. I don't want to spoil\nit, but she's the person that has multiple faces. So if you see one of these\nstrange avatars here, that's Mengmi. And she will talk\nabout something that's extremely cool\nand super exciting with a tutorial\npresentation for you to learn about deep\ngenerative models. So without further\nado, Mengmi, please. MENGMI ZHANG: Yeah,\nhello, everyone. Welcome to this tutorial. Yeah, my name is Mengmi. I'm a post-doc\nfrom Gabriel's lab. For those of you who have\nnot seen me before, just to clarify, this is not\nactually how I look like. Since I'm going to talk about\ngenerative models today, I thought it might be fine to\ndo a live demo of how we can use generative models\nin our daily life, for example, in\nthe Zoom meetings. Instead of mimicking fictional\ncharacters in a movie, the generative models could also\ngeneralize to a real person. So I'm going to switch my\nfaces just for now, like this, like Steve Jobs. And also it could, like, extend\nto a person in a painting. And I can also move\nmy head a little bit and make funny\nfacial expressions. What is more interesting is\nthat the generative models has been trained on celebrity\nfaces, and yet they have no problem generalizing\nto, for example, animal faces. Like, now I'm\nmimicking a monkey. Or a cartoon character\nlike SpongeBob. All right, for those\nof you who wonder what is the DeepFake\ntechnology behind the demo, here is a peak\nabout the algorithm. I also attached the link\nhere and the paper for you to check them out. Here's the source image, which\nis the avatar you want to be. And this is the driving\nframe, which is the real me. And then we can calculate\nthe optical flow, which tracks the movement\nof individual pixels between adjacent frames. We can also calculate\nthe occlusion map, which details the part\nof the background that needs to be repainted. Together with the features we\ntracked from the source images, these three components form\na latent representation. And then we could pass\nthis to the generator. And the generator could\nthen synthesize new frames and broadcast to Zoom. And now I'm presenting you\nan overview of deep learning frameworks in machine learning. In the previous tutorial,\n[? Andrei ?] and [? Boris ?] talk about the basics of\nconstructing a convolutional neural net and establish\nconnections between activations of the artificial neurons and\nthe neurons in mouse brains. In this tutorial, I will move\non to unsupervised learning. In particular, we\nwill be focusing on generative\nadversarial networks and how we can leverage\non these generative models to reconstruct images\nfrom brain signals. Here is the outline\nof the tutorial. I will first talk\nabout the basics of generative\nadversarial networks in an advance version\nof the [INAUDIBLE] which is BigBiGAN, followed\nby a review of existing image reconstruction methods\nfrom brain signals. Before I go straight\naway to talk about GANs, I want to introduce you\na very important concept being in deep learning,\nwhich is deconvolution. In the past, we\nhave been talking a lot about convolution. So the blue patch\nhere is the input. The cyan patch is the\noutput feature map. We observe that the 2D\nconvolution typically reduces the output\nfeature map dimension. In this particular example,\nthe feature map size decreases from 5 by 5 to 3 by 3. In contrast, deconvolution\ndoes the opposite. Here again, input is in\nblue and the output is cyan. The deconvolution\noperation upsamples the input a feature map\nfrom 2 by 2 to 4 by 4. In PyTorch, here is a\nfunction for deconvolution. And here is an\nexample usage below. All right, now, let's\ntalk about GANs. Just a little bit\nof history, GAN was first invented by\nIan Goodfellow in 2014. Since then, GAN has been\na popular research topic. So what is GAN? As this name indicates,\nit's a plural term. GAN consists of two networks,\none is the generator, the other is a discriminator. The generator takes\nthe random noise vector and generates an image. The law of the\ndiscriminator is to tell whether a given image\nis real or fake, which is generated by the generator. These two networks fight\nagainst each other. In game series, it is\ncalled Min-max game. In other words, each\nof these two parties is trying to minimize\ntheir own losses, given their opponent is perfect. For example, here, if\nI am a discriminator, I'm assuming I'm facing\na perfect generator which can generate realistic\nimages to fool me. Thus, I'm trying to minimize\nthe number of mistakes I'm going to make\nin misclassifying fake images as real images. Now, let's take a closer\nlook at the architecture of the discriminator. Same as other object\nrecognition networks that you have\nprobably seen so far, it's just another one\nnetwork consisting of a stacks of\nconvolution layers. It takes that image as\nthe input and outputs of probability vector,\nindicating whether it is real or a fake image. A couple of practice notes here. So this is PyTorch code\nloading the images. To make the network\n[INAUDIBLE] variations, you have to perform the image\naugmentation, such as scaling and rotation. And since we have\nthe ReLU layers, which we use a 0 as\nthreshold, we also want to normalize the image\npixel values from 0 to 1 to minus 1 to 1. In the end of the network, to\nmake sure the network outputs probabilities, we add\na Sigmoid function, which normalizes the\nvalues to 0 to 1. By the way, instead of\nonly connected layers, since this network consists of\nstacks of convolution layers, we also named this GAN\nas deconvolution GAN, which is in short of the DCGAN. Here's a diagram\nof the generator. It does exactly the opposite\nof the discriminator by replacing convolution layers\nwith deconvolution layers. The generator takes\na-- sorry, so here, what you see on the\nright is a snippet of how we constructed\nthe discriminator. And the arrows, shown\nhere, are corresponding with the different layers\nof the convolution. And take note, that\nin the end, we had to add in a Sigmoid function. All right, so here's the\ndiagram of the generator. It does exactly the opposite\nof the discriminator by replacing the\nconvolution layer with deconvolution layers. And the generator takes a vector\nof random noise as inputs. And that is for each element\nin this one dimension vector, we randomly sample number from a\nnormal distribution with mean 0 and variance 1. In PyTorch, this is\nhow we implement it. The generator outputs a tensor\ndimension of 3 by 64 by 64. However, the\ngenerator itself has no notion about what\nthis tensor represents. Thus, we need a hyperbolic\ntangent function to normalize all the\nvalues in the tensor to be from minus 1 to 1, as a\nrepresentation of the image. This is consistent with\nthe real input image pixel values to the\ndiscriminator, which also ranges from minus 1 to 1. Take note that this\nnormalization is essential, since we do not want the\ndiscriminator to easily capture the difference offset\nbetween the real pictures and the generated tensors. Here is the snippet\nof code constructing the generator,\nthe arrows to show the corresponding\ndeconvolution layers. In the end, we added\na tangent function. Now, we have the\nconstructed discriminator and generator let us see how\nwe can train this neural net at the same time. Again, to remind you that the\ngenerator and discriminator are playing against each\nother in a Min-max game. Thus, we can divide their\ntraining into two parts. So first, we assume we have\na perfect generator which generates a bunch of images. We can train a discriminator,\ndifferentiating the fake images from the real ones. Just like training other\nobject recognition network, the discriminator takes\neither a batch of real images with the ground truth\nlabeled as real. It performs a binary\nclassification of these real images and back\nprop the gradients like this. Similarly, we could also label\na batch of generated images as fake, compute the laws\nagain, and then back prop the gradient. The part of the code\nthat shows the training of the discriminator\nwith all the fake images labeled as fake, this\nis the first part. And then the second\npart shows the training of the discriminator with\nall the real images labeled as real. So far, the training story\nhas been very straightforward. And let's now see how we\ncan train the generators. First, without the\nreal images, we could simply concatenate the\ndiscriminator and the generator as one being network. Thus, we can compute\nbinary classification laws and perform the gradient\nback propagation from the very end\nof the discriminator all the way back to the\nbeginning of the generator. So the objective is to\ntrain the generator. We want the generated images\nto fool the discriminator into classifying them\ninto real images. Therefore, the ground truth\nlabel for the generated images are now going to be real. From the point of view\nof the generator, that is to say how much the generator\nneeds to correct itself in order to make sure\nthe discriminator outputs the real labels\nfor its generated images. Note here, this is\nimportant difference from training the discriminator\nwhere the ground truth label for the generated\nimages have now changed from fake to real. And this is the code for\ntraining the generator. And this is where the\nlabels have been assigned to the generated images. Since 2014, the study of\nGAN has become so popular, for the past five years,\nthere are many GANs out there. And these are several examples. There was one time\nthat GAN become so fashionable that there\nwas a joke in AI conferences that people would say, if you\nwant to get your paper accepted in top AI conferences in 2017,\nyou'd better put a word GAN somewhere in your paper. In this tutorial, among\nall these type of GANs, I want to introduce\nBigBiGAN, in particular. Now, we understand\nthe basics of DCGAN. However, why do we need to study\nmore GANs, rather than DCGANS? Like, why do we propose\nnew GANs such as BigBiGAN? Isn't DCGAN good enough? Here, I summarize a couple\nof disadvantages about DCGAN. So the first disadvantage\nis that the sizes of these generated images are\ntypically very small. It's even smaller in the\nsizes of the ImageNet images that we often input to the\nobject recognition network. Second, one generator is often\nresponsible for generating only one object\nclasses of the images. In the lecture given by\nAntonio Torralba last week, we knew that if we want to\ntrain one generator generating buildings, it's very unlikely\nthat these generators can give you dog images, for example. This is very unsatisfying. Typically, we want a generator\nwhich can generate images across multiple object classes. But the reasons that we\ncannot do this is because we don't have constraints on the\nrandomly sampled latent vector in the generator. And another disadvantage is\nthat these images are typically of low resolution and its\nlack of high visual details. Not only that, even training\nDCGAN is very brittle. For example, sometimes\nthe loss for the generator and the discriminator\nall simulate over numbers of epochs. At the testing stage, it is\ncommon to see the generator collapse, meaning they lost\nthe ability of generating diverse number of samples. And typically, they\noften generate, like, for example, five image\nexamples from a class, and then that's it. Since this is a battle\nbetween the generator and the discriminator, it's easy\nthat the discriminator always wins the game. And thus, the gradient\nof the generator diminishes, since\nit always loses. It never wins no matter what. So there are several\nempirical evidences suggesting that\nthese networks are very sensitive to hyperparameter\ntunings during training. That's why, here,\nI want to introduce you an advanced version\nof GAN, which is BigBiGAN. This is the work published in\nNIPS last year from DeepMind. As its name indicates,\nit consists of two parts. It is BigGAN. And it is also bi-directional. So let me first introduce\nthe part where it is big. As its name literally suggests,\nit is a big neural net. It is trained with\nlarger batch sizes. And it has more\nnetwork parameters. The authors have also\nintroduced several architectural modifications. For example, they\nintroduce skip connections of the latent vector. That is, they bypass\nthe latent vector to the next layer after the\nfirst deconvolution layer, and so on. Self-attention module\nturns out to be useful in many applications. The intuitive way to interpret\nthis self-attention model is the following. Imagine, you are\nan artist and you want to paint a dog as\nhe's sitting on the grass. So what the\nself-attention model does is to pay more attention\nto the dog regions, instead of the grass. So this is exactly what the\nself-attention model does. It helps you guide the network\nto focus on important region to paint such that we can\ngenerate more realistic images. Next, let's talk about\nthe bi-directional part, which is the part I found\nwhich is very interesting. So here is what we\nsee in the DCGAN. In order to control what\n[INAUDIBLE] in the latent code, authors introduce a encoder\nside by side the generator. This is exactly the opposite\nof the generator, which takes the real images\nand encodes this latent representation zed hat. Ideally, if the\nlatent vector carries the same essential information\nas the abstract information, then the generated images\nshould look close enough as the real images. Thus, the discriminator\nhas two extra jobs to do. In addition to the objective\nof telling the generated images from real or fake, it\nalso has to distinguish the extracted representation\nof the real image from the latent coding in\nthe generator, as shown here. Moreover, it also\nhas to distinguish whether the joint\ndistribution combining the encoded representation\nin this image is real or fake,\nwhich is it's going to take pairs of the encoding\nas well as the image. With those two\nadditional constraints, BigGAN can generate high\nquality images of larger image size, up to 512 by 512. There are more tricks for\nconstructing realistic using GANs. For example,\nresearchers have found that updating discriminator\nmore often during training is very helpful. Taking the average of the model\nparameters is also beneficial. So here is what I meant. For example, in the\nfirst epoch, you update your generator parameter\nand denote it as W_G1. Then, in the second epoch,\nyou update the parameter for the generator again\nas W_G2 and so on. And so the testing stage, you\ncompute the final parameter for the generator by taking the\naverage of all the parameters over the number of epochs. Here are more tricks. We know that for each\nelement in the latent vector, we randomly sample from\nthe normal distribution with a mean 0 and a\nstandard deviation of 1. At the testing stage,\ninstead of sampling from a normal\ndistribution again, we are going to sample from\nthe parts out of 1 sigma, let's say. This would help us input more\nextreme values into the latent vector, and thus it constructs\nmore realistic images. But then there is a caveat. So this would sacrifice\nthe sample diversity, so the space we can sample\nfrom becomes smaller. During training the\ngenerator, we typically want to regularize the\nweights to be orthogonal. So here's an\ninformal explanation, but I will encourage you to\ncheck out the actual paper. They provide empirical\nanalysis about this. So imagine you have\ntwo weight vectors. If they are orthogonal\nto each other, then they will share\nless similarities, compared with the two\nvectors shown on the right. Thus imposing this\northogonal regularization, you actually push the\nnetwork parameters to learn as many distinctive\nfeatures as possible. All right, we finish the machine\nlearning part of the tutorial. Let us now move to the brain\nreading part in neuroscience. With all the basics of deep\ngenerative models in mind, we could ask ourselves\nthe following question. If this random vector\ncan generate images, can we actually plug any\nbrain codes into the generator and reconstruct its\n[INAUDIBLE] images? One could imagine that the\nbrain code could be any type. For example, it can be\nmeasured with any techniques in neuroscience and recorded\nfrom any brains of animal species. And in the future, instead\nof restricting ourselves into vision, we could also\nextend it to five senses, for example, sound,\ntouch, and smell. And we could also extend\nthis to high level cognitive functions such as emotions,\nmemories, and languages. Before we completely switch gear\nto review image reconstruction measures, let me now stop\nhere for a couple of seconds and take one or two\nquestions, if any. PRESENTER 2: Great, we've got\na question from Anthony Chen. Can you explain why\nhaving ReLU activation means we need to normalize\npixel values to negative 1, 1? MENGMI ZHANG: Sorry, can\nyou repeat the first part? What is IOU activation? PRESENTER 2: Can you\nexplain why having R-E-- so capital R, little e, capital\nL, capital U, ReLU activation means? MENGMI ZHANG: Oh, I see. Got it. Yeah, so typically, we have\nthis linear ReLU activations in the network. So that's where we zero\nout all the negative values in the layer. So imagine, now, if\nyou don't normalize your pixel distribution\nfrom minus 1 to 1, let's say, we stick\nwith 0 to 1, then your ReLU function basically-- so basically, you can treat\nReLU as a thresholding mechanism that decides\nwhether the value is going to be above 0 or below 0. And if it is below,\nthen I'm going to zero out all the values. Therefore, you probably want\nto shift your image pixel distribution from minus 1 to 1. Hopefully, this would\nprovide a satisfying answer to your question. PRESENTER 2: The next one\nis, anonymous is asking, why is it called self-attention\ninstead of just attention? MENGMI ZHANG: Well,\nthat's because the self-attention\nmodel, like, the network itself learns attention. So typically, when people\ntry to say attention, you sort of like provide human\nsupervision or human judgment into the model, like, which\nparts of the image that you think are important. But here, the network\nactually just ultimately computes attention values\nin the attention maps. So yeah. PRESENTER 2: Thanks. And we've got one\nmore from [INAUDIBLE].. Can you explain more\nabout the tricks for image reconstruction? I mean, what could\nbe the strategy if you want to look at the\noutput of some patterns? MENGMI ZHANG:\nWell, I don't quite understand what you mean by\nthe strategy of the patterns. But I would imagine,\nso let's see if you want to\ninterpret a target unit in the generative\nmodels, maybe we could-- I haven't done this before,\nbut I would imagine, let's see, we could\nuse similar ways as what's DeepDream\nhas been doing, which is you try\nto compute the loss of certain features of the\ntarget unit in certain layers. Then take the gradient\nand iteratively amplify it with respect to the\ngenerated images. All right, so this is a\nrelatively new and fast growing research area. After a literature\nreview, what I found was only papers on\nimage reconstruction from brain signals. Unfortunately, I didn't\nfind any literature about reconstruction of other\nmodalities, for example, music. In this later half\nof the tutorial, I will briefly go through some\nof the representative works on image reconstruction methods. I divided the reconstruction\nmethods into two groups. One is based on gradient\nback propagation, which was mainly inspired by DeepDream\nand then TextureSynthesis. First, I should clarify\nthat these gradient back prop methods are not\ngenerative models. This tutorial, I'm going to\nfocus on generative models. I will only briefly\nintroduce the key ideas behind this algorithm. But I strongly recommend you\nto check out these papers by yourself. So the first two pieces\nof work shown here are inspired by the idea\nsimilar as DeepDream. This algorithm is\nused for visualizing the patterns learned\nby a particular unit or layer from a neural net. It over-interprets and\namplifies the patterns it sees in an image. DeepDream does so by\nfirst forwarding an image through the network, then\ncalculate the gradient of the image with respect\nto the activations of a particular layer. Then, the image is modified\nto increase those activations, enhancing the patterns\nseen by the network, resulting in dreamlike images. Another piece of\nwork worth mentioning is the study of neural responses\nin [INAUDIBLE] where they use this texture like pictures. What these authors did is first,\nthey take a natural image, pass it to pretrained\nneural net. It takes another\nwhite noise image and passes through\nthe neural net again. Then, the texture\nsynthesizer computes the mean squared area of the\ngrand matrix as a target layer, for example, Conv\n3, in this case. And it back propagates\nthe loss with respect to the white noise image. The generated image has\nthis texture looking like. So to put this in\nlayman's terms, the goal of the\ntexture synthesizer is to preserve the structure\ncontent of the target layer in a neural net. What is interesting\nis that these authors found that these synthesized\ntexture images share a similar V1 neural\nresponses and what has been observing the natural images. OK, so now let's move on to the\ngenerative model based method. One of the key problems\nwe need to solve is how we can translate\nthe brain signals into this latent\nrandom codes which the generator understands. One of the\nstraightforward solutions is we don't need to perform\ntranslation between the brain signals to this latent code. Instead, we could directly\ntake this brain codes as inputs to the\ngenerator and fine tune the parameters of the\ngenerator to make it adapt to our new brand codes. So here is a\nrepresentative work where the authors asked participants\nto see a bunch of images and obtained their brain codes. Here, each latent\nvector represents a piece of code\ncorresponding to an image. Then, the generator takes the\nbrain codes directly as inputs and then reconstructs\nthe images. Just as a standard\nGAN training, authors imposed discriminator losses\nto tell the real images or reconstructed ones\nfrom the brain signals. In addition, the authors\nintroduce another two losses to impose constraints at both\nthe pixel level and feature levels. That is, the\nreconstructed images should look as similar as\npossible as the real image. At the testing stage,\nthe generator is fixed. And then, we could take\ndirectly the brand codes and plug it in to\nconstruct the image. On the right, the\ntwo bottom rows shows the reconstructed\nimages from the two subjects using fMRI signals. Again, I want to emphasize\nthat one could easily imagine that we can replace\nthe fMRI signal with other type of brain codes and adopt\nsimilar approaches for image reconstruction. Of course, the\ngenerator [INAUDIBLE] contains many parameters\nand this method would require intensive\namount of data in order to fine-tune\nthe generator. So next, we will explore\nalternative approach, which is to fix the\ngenerator parameters, and instead, try to\nlook for the best latent code that would drive\nthe neurons to fire as much as possible. Here is how the algorithm works. So let's start from a\nbunch of noise vectors, just as what we did before. The generator uses\nthese old latent codes to generate a bunch of images. And then we present\nthese generated images to monkeys and record\ntheir neural responses in the form of spike trends. Just a quick recap on\nEthan's tutorial last week on processing neural data, we\ncould treat these spike trends as a binary vector and compute\nits average firing rates. For example, we could take\nthe mean of this spike trend and we get a number. For example, this is 30\nhertz for the first image and 20 hertz for\nthe second image. Since our goal is\nto drive the neuron to fire as much as\npossible, we use those computed mean firing\nrates as the fitness score for each image code. In this paper, authors proposed\nto use the Genetic Algorithm to select the best\nparent, then crossover mutate them to get the\nnext generation of codes. After the optimization, we\nhave a new population of code, and then the generator could\ngenerate a new batch of images. And this process iterates. So what I'm showing here\nis a single synthetic image involving over generations. This method is a\nclosed-loop system. Because it involves monkey\nbrains in the loop, and then it requires many iterations in\norder to find the best stimuli. Next, I will introduce a simpler\nway of establishing the mapping function. It turns the latent code\ninto the brain signal via linear regression. Again, the generator\nare fixed in this case. Just a quick recap. So in BigGAN, we\nhave encoder which encodes the abstract\nrepresentation of the real images. And after training, this\nlatent representation carries similar features as\nthe latent code [INAUDIBLE].. Thus, we could make use\nof this relationship to help us find the\nbest mapping function. So let's see. We have total, N\ntraining images. For each image, we could pass\nthem through the pretraining coder from the BigBiGAN. Let's concatenate this feature\nvector from all the images as a big matrix called\na generator matrix. It is of dimension\n120 by N. Next, we could similarly represent\nthese images to animals and extract those brain\nsignals, concatenate them, and name it as brain matrix. It is of dimension nv by N. Since we have the generator\nmatrix and the brain matrix, we could easily perform a linear\nregression between two of them. Specifically, this is to\nestablish a linear mapping by computing the weight matrix,\nwhich is of dimension nv by 20. And then we also have\nthe generator matrix, which is 120 by N. So we could\ncalculate the weight matrix. Here are some practice issues\nthat we have to take note. For example, W my\nnot be invertable. So we could take\nthe pseudo-inverse. And the brain matrix\nsize might be very big. And we can perform\ndimension reduction using PCA to\npre-process the data. Since the scale,\ntheir brain signal might be different\nfrom the latent code. We also want to\nperform normalization of the brain signal first. All right, at the\ntesting stage, we could simply make use of the\nweight matrix we just computed and transform this brain\nsignal to latent code and then pass this\nlatent code back to the generator in BigBiGAN\nfor image reconstruction. Yeah, so on the right,\nwhat you see here are the reconstructed\nimages using fMRI signal. Again, the linear\nregression method could be generalized to\nother forms of brain signals, as well. Then, after reviewing all these\nimage reconstruction methods from brain signals,\nyou might have your own personal\njudgments about which brain reader is the best. In order not to be\nsubjective, the next question we want to ask is can we come up\nwith a quantitative evaluation metrics to evaluate all\nof these brain readers? And the answer is, clearly, yes. So similar as brain scores\nproposed in Jim DiCarlo's lab, evaluating the relationship\nbetween state-of-the-art object recognition network and the\nneural responses in the brain, here we propose to add in\nadditional metrics to evaluate the deep generative models\nand report their relationship to the brain signals. Yeah, I think we still\nhave plenty of time. So let me just introduce\nyou each of this metrics individually. So first inception score, it\nmeasures both the image quality and diversity. Here's how the\nscoring is computed. First, it reflects\nthe image quality. So let's see, we have\na generated image, and then we pass it to\nobject recognition network. Here we are using Inception\n3, but it could be any other recognition network. It outputs a vector indicating\nthe classification probability for each object classes. If the reconstructed image\nis of very good quality, that is the network has\nhigher confidence recognizing what\nthe object is, then the label distribution would be\nuni-modal, resulting in a lower entropy score. For those who don't\nunderstand what entropy is, it basically reflects the\nuncertainty of a distribution. Conversely, if\nthe network barely recognize what the\nreconstructed image is, then we will end up\nwith a uniform label distribution, which results\nin a higher entropy score. In this case, the lower\nentropy score, the better the image quality is. Moreover, we want to evaluate\nwhether the brain reader has enough image diversity. For example, if the network\nalways generated dog images no matter what latent codes\nyou give to the network, then I would say this\nnetwork is very bad because it fails to capture the\ndiversity of object classes. Thus, if we can take the sum of\nthe classification probability for all the\nreconstructed images, we would expect it to end\nup with a more focused distribution, since\nthe generator always generated dog images. In contrast, if the generator\ncould evaluate multiple object classes like showing here, we\nhave elephants, cats, dogs, et cetera, then the sum of all\nthese classification vectors would give us a more uniform\ndistribution, therefore, a higher entropy, which is good. Overall, we want a more\nfocused label distribution for better image quality, but\na more uniform distribution for diversity. In order to combine\nboth aspects together, inception score finally\ncomputes the KL divergence between these two distributions. The higher the KL\ndivergence score, the better it reflects\nthe model's ability to generate good images with\ndiversified object classes. Note that this inception\nscore discards the information about the real images. Because you only calculate\nthe image quality and image diversity based on\nthe generated images. It has nothing to do\nwith the real images. Therefore, we are\nproposing a new metric which is the Frechet inception\ndistance, short for FID. For both the reconstructed\nand the real images, we could use the inception\nnet to extract their feature vectors. Thus, for each real and\ngenerated image pairs, we have a pair of\nfeature vectors. The FID calculates the\ndistance between these two distributions. A better brain reader gives\na lower distance score. So here's a mathematical\nformulation of FID which basically\ncalculates the distance between the mean\nvector and the trace of their covariance differences. So here is the third metric\nfor evaluating brain readers. Again, this is a\nBigBiGAN architecture. I want you to focus on\nthe encoder part again. So for each generated\nimage, the encoder could take them as inputs, and\noutputs their feature vectors. If this encoder representation\nreflects the information about [INAUDIBLE]\ncategorization, then these features from\nthe generated images should look good enough for\nclassification on ImageNet. We, therefore, could\ntrain a simple classifier, predict their cost\nlabels on ImageNet, and report their top-1 accuracy. If the brain reader\nconstructs meaningful, natural looking images\nwish corresponds with the real images, then the\ntop-1 classification accuracy would be very high. In the BigBiGAN paper, I'm\nquite surprised that they also report a 61% top-1\naccuracy on this ImageNet. And using the purely\nunsupervised learning. All right, so we could\nalso conduct human behavior experiments to evaluate\nthese brain readers. Here is the experiment paradigm. Let's see, we have the input\nimage presented to the humans and then as targets. We could then randomly choose\none reconstructed image based on the brain signals\nfrom that target image. As a negative sample,\nwe could randomly pick another reconstructed\nimage from a non-target image. With these three\nimages, we proceed to conduct the behavioral\nexperiments using Amazon Mechanical Turk. This is how it looks\nlike in an example trial. So in the experiments,\nthe subject is instructed to\nchoose either option A or B that is more visually\nsimilar with the target image. After we collect the human\nchoices on many trials, we could simply report the\naccuracy of human judgment. Now, I want you to pay attention\nto the bar on the right, highlighted in red here. The dash line is the\nchance level, which is 50%. In the paper, authors\nreported there about 90% of people\nchoosing correctly. That is, they prefer\nthe options which are reconstructed images based\non the brain signals generated after the subject [INAUDIBLE]\ntarget image instead of the non-target image. All right, at last we could\nalso assess the controllability of these reconstructed\nimages at a neural level. So here is an illustration\nof what I meant. We first record the average\nneural firing rate of monkeys after they see\nthe real pictures. Then, we could test monkeys\non their corresponding reconstructed images and\nrecord their firing rates. Next, we show a scatter\nplot where the x and y-axis indicates the firing rates\nof the real and reconstructed images. We put a dot for each pair\noff the average neural firing rates between the real and\nthe reconstructed images. And here are more dots for\nthe second pair of images and third pair, and so on. Now, we can fit\na line and report their linear correlation. So the higher the\nlinear correlation implies reconstructed images\ncapture more essential features. All these brain\nscore assessments, what we discussed\nso far, could also be applicable with other\ntypes of brain signals. All right, just to conclude. Nowadays, there have been\nfascinating progresses about deciphering\nbrain and codes using machines and\ninterpreting the machine codes using brain responses. Hopefully, not far\nfrom the future, we could find a\nperfect bijection which could translate between\nthese two types of codes. With this bijection,\nit is possible that one day we could\nstimulate and control any part of the brain. Last, but not least, I want\nto conclude the tutorial with an illustrative\nfigure, showing you a hypothetical diagram of\nthe brain-machine interface for the visually impaired. With the joint efforts of\nthe neuroscience and AI researchers, one could\nimagine that one day we could help the visually\nimpaired by installing a camera in front of the eyes. And the neural\nnetworking could be embedded in a pocket processor. It extracts the machine\ncodes and translate it to brain-understandable\nlanguage. And this could be\ntask dependent. And it could be text reading or\nface identification or obstacle avoidance. In the end, it could translate\nto the brain or standby codes to wire a wireless\ntransmitter on the scalp and use the electrode\narrays embedded in the brain to stimulate the brain. PRESENTER 2: A general\nquestion, how do we know brain signals contribute\nto the generated images? The generator also can\ngenerate similar images by inputting a random\nlatent code space, right? MENGMI ZHANG: Yeah, that's\na very good question. But if let's see if I mean I'm\nnot sure about the DCGAN case, because it always generates\nimages from one object classes. But, let's see, for\nthe BigBiGAN case, so if the generator ignores\nwhatever in the latent code, it's probably going to generate\nimages from different object classes, compared with the\nimage that is actually presented to the animals, right? So for example, if I'm\npresenting the monkeys with a dog image and if\nI'm just randomly plugging another random noise\nvector to the generator, the generator is probably\ngoing to generate a cat image instead of dog. And that's why in the evaluation\nmetrics have been introduced, the top-1 classification\nscore on ImageNet. And hopefully, that could\nclarify your question. PRESENTER 2: Thanks. Next one is from Jed. Is fMRI more adaptive\nthan EEG to this problem? Can we expect more precise\nreconstructions with EEG? Is there any way to combine\ninput from both EEG and fMRI? MENGMI ZHANG: Oh, that's\nan interesting question. So just to clarify, I'm not\nfrom neuroscience background. I couldn't say\ntoo much about it. But I would imagine, yes. And why not? I mean, for me, any\ntype of brain signal is just like a\nseries of numbers. And then if we could interpret\nthem correctly and analyze them, normalize them correctly,\nthen I don't see the reasons why these brain codes cannot\nbe represented in one way or another. PRESENTER 2: Why does\nincreasing the activation create dreamlike images? MENGMI ZHANG: So the\nname is DeepDream. I think, it's\nprobably just saying, you know, this weird-looking\ndog looks like a dream that you just had, right? But then at the algorithmic\nlevel, what it basically does is just to amplify the\nactivations of the target unit in the target layer. So I mean, this is\na subjective course. It actually depends on how\nyou interpret them, right?",
    "status": "success",
    "error": null
  },
  {
    "video_id": "YreDYmXTYi4",
    "transcript": "foreign [Music] [Music] foreign [Music] [Music] thank you [Music] [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] [Music] foreign [Music] foreign [Music] [Music] [Music] [Music] [Music] foreign [Music] foreign [Music] [Music] thank you foreign [Music] thank you [Music] foreign foreign [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] foreign [Music] [Music] foreign [Music] foreign [Music] [Music] [Music] thank you all right [Music] [Applause] thank you [Music] foreign [Applause] [Music] all right welcome to the neurolink show and tell so we've got uh an amazing amount of new uh developments to share with you that I think are incredibly exciting as well as tell you about the future of what we're planning to do here it's uh now this is meant to be a technical podcast or sort of like a work I'm going to provide an overall summary and then we're going to have a number of members of the the neural link team come in and give a deep technical overview of the various areas so uh yeah so let me move forward with the the overall summary now some of the things I'm going to say are things you've well if you've been following your link you've already heard before that uh for for a lot of people out there they've no idea what neurolink does and so I'll be a little bit repetitive of things you may already know but that others do not so um the the overall the overarching goal of neurolink is to create a uh ultimately a whole brain interface so uh a generalized input output device that in you know in the long term literally could interface with uh every aspect of your brain and in the short term uh can ask we can interface with any given section of of your brain and and solve a tremendous number of things that that cause debilitating issues for people so uh you know so our long term is like I mean I'll talk a little bit about a long-term goal it's going to sound a little esoteric but it's the it was actually the sort of my Prime motivation which was you know kind of what what do we do about AI like what do we do about artificial general intelligence uh if we have digital super intelligence that's you know just much smarter than any human how do we mitigate that risk at a species level how do we mitigate that risk and then even in a benign scenario where the AI is uh very very benevolent um then how do we even go wrong for the go along for the ride how do we participate um and the conclusion I the the thing that the biggest limitation in going along for the ride and in aligning uh AI I think is the is the the bandwidth the the how quickly you can interact with the computer so we're we are all already cyborgs in a way in that your your phone and your computer are extensions of yourself and if you I'm sure you found like if you leave your phone behind uh you end up tapping your pockets and and it's like having missing limb syndrome like where you know the phone is it is leaving your phone behind is kind of like a missing limb at this point you're so used to interfacing with it you're so used to being a de facto cyborg um But but so what's the limitation on on a on a phone or a laptop limitation is the rate at which you can receive and send information especially the the speed with which you can send information so if you're interacting with a phone it's limited by the speed at which you can move your thumbs uh or the speed which you can talk into your phone this is an extremely low data rate um you know maybe it's like 10 optimistically 100 bits per second but a computer can can communicate at uh you know gigabits terabits per second so this is the fundamental limitation that I think we need to address to mitigate the long-term risk of artificial intelligence and also just go along for the ride and uh yeah so if it likes it that's that's that's an esoteric explanation that I think will appeal to a niche audience um uh some of whom may be here um but and that's a that's a very difficult problem so even if we do not succeed with that problem I think we we like we are confident at this point that we will succeed at many uh it's it's solving many brain injury uh issues spine injury issues along the way so um yeah so anyways so uh actually we have uh Justin Roiland in the audience uh says the hi Justin so it's a little Rick and Morty reference here um the uh this great Rick and Morty episode about intelligence enhancement of your dog and uh what's the worst that can happen so anyway Rick and Morty I recommend it um so for so you want to be able to read the signals from the brain you want to be able to to write the signals uh that you want to be able to ultimately do that for the entire brain and then also extend that to communicating to the rest of your nervous system if there's a if you have a sort of a severed spinal cord or neck so uh now this is a this video is now 18 months old so this is um pager uh who is playing uh monkey mind pong so this is a pager has a neural link implant in this video um and the thing that's interesting is that you you can't you can't even see the the neural implant so it's the it's we've monitorized the neural implant to the point where it matches the the thickness of the skull that is removed so it's essentially that it's sort of like having an Apple Watch or a Fitbit uh replacing a piece of skull with like a you know a smart watch for lack of a better analogy um so uh so you can see you really can't he looks pretty easy normal um and I think that's pretty important if you have a neuralink device like I could have a neuralink device uh implanted right now and you wouldn't you wouldn't even know I mean hypothetically yeah I may be one of these Demos in fact one of these demos I will so uh yeah anyway so so here's here's uh first of all it's kind of wild hey monkeys can play Pong they're like uh they can't actually pay pong if you give them a joystick uh so Pedro first learned to play Pong with a joystick so I'm like that was a novel it's like I didn't know monkeys could play Punk but they can um and then uh so we first trained pager to play Pong with a joystick then we took the joystick away and have the neural link and now this is he's playing telepath it's a telepathic video games essentially um so what we've been doing since then is uh we've been on the very difficult Journey from prototype to product uh and I've often said that prototypes are easy production is hard it's really I'd say a hundred to a thousand times harder to go from go from a prototype to a device that is safe reliable Works under a wide range of circumstances is Affordable and down at scale it's it's insanely difficult um I mean there's an old saying that you know that it's one percent inspiration 99 perspiration but I think it might be 99 or 99.9 perspiration um the best example I could give of an idea of being easy but the execution being hard is going to the Moon it's uh the idea of going to the Moon easy going to the Moon very hard so um and we've been working hard to uh be ready for our first human and obviously we want to be extremely careful and certain that that it will work well before putting a device in a human but we're we're submitted I think most of our paperwork to the FDA and we're we think probably in about six months we should be able to have opposed neural Link in a human so [Applause] but as I said we we do everything we possibly can to test the devices before uh not even not even going into a human before even going into uh an animal so we do Advantage top testing we do accelerator accelerated life testing we have a fake brain simulator that has the the texture and uh it's like emulating a brain but it's sort of rubber and uh so any we before we would even think of putting a device in an animal we we do everything we possibly can with rigorous bench up benchtop testing so we're not Cavalier and putting devices into animals uh we're extremely careful and uh we always want the device whenever we do the implant if it's in a sheep or a pig or a monkey to be confirmatory um not exploratory so that we like we've we've done everything we possibly can with benchtop testing and only then would we consider putting a device in an animal um and uh yeah we'll actually show you a demo later today of a few hours really of uh of implanting in a brain proxy and if anyone in the audience wants to volunteer uh with a robot right there so let's see since the page of demo we've expanded to work with a troop of six monkeys we've actually upgraded pager they do varied tasks and we do everything possible to ensure that things are stable and replicable and the things like that the device lasts for a long time without degradation so and uh what you're seeing there is it looks like the Matrix but that's uh actually though that's a real output of of neural signals so that that's that's not a simulation or just a screensaver or something that those are actual neurons firing that is one of the what one of the readouts looks like and um here you can see sake it's one of other monkeys typing on a keyboard now this is telepathic typing so to be clear this is the he's not actually using a keyboard he's moving the cursor with his mind uh to the highlighted key now technically um uh we can't can't actually spell and so I don't want to oversell this thing because that's uh that's the next version um so the but what's really cool here is is um sake the monkey is moving the mouse cursor using just his mind moving the cursor around to the highlighted key and then spelling out what we here what we want everyone just felt but um and then so this this is a something that could be used for somebody who's who's say uh uh quadriplegic or tetraplegic human even before we make the the spinal cord stuff work is being able to con uh control a mouse cursor control a phone um and we're we're confident that that someone who is has basically no other interface the outside world would be able to uh control their phone better than someone who has Working Hands so foreign upgradability upgradability is very important because our first production device will be much like an iPhone one and um I'm pretty sure you would not want an iPhone one stuck in your head if the iPhone 14 is available um so it's gonna be it's be able to demonstrate full reversibility and upgrade ability so you can remove a device and replace it with the latest version or if it stopped working for any reason replace it it's that's that that's a fundamental requirement for the device at your link and I should say both sucky and Paige were upgraded to our latest and greatest implants so that that's been really over a year and a half now that that pagers had for the first implant and then the upgraded implant so this is a very good sign that it lasts for a long time with no uh observed ill effects I think it's also important to show that um sake actually likes doing the demo um and it's not like strapped to the chair or anything so uh it's yeah so um the monkeys actually enjoy doing the demos because they and they get the banana smoothie and it's kind of a fun game so um I I guess smart termic is like We Care a great deal about animal welfare and um and uh I'm pretty sure like our monkeys are pretty happy you know so as you can see there's a quick decision maker on the fruit front so so for uh the the first two applications we're going to aim for in humans are restoring vision and uh I think this is like notable in that even if someone has never had Vision ever like they were born blind we believe they could they can we can still restore vision so uh because the visual part of the the visual part of the cortex is still still there so uh yeah even if they've never seen before we're confident that they could they could see um and then the uh the other application being in the motor cortex uh where we would initially enable someone who uh has no ability to almost no ability to operate their their muscles you know sort of like a sort of Stephen Hawking type situation and enable them to operate their phone faster than someone who has Working Hands um but then even obviously even better than that would be to bridge the connection um so uh take take the out the signals from the motor cortex and let's say somebody's got a broken neck then bridging those signals to neural link devices located in the spinal cord so I think we're confident there are no there are no physical limitations to enabling full body functionality so I mean as miraculous as it may sound we're confident that it is possible to restore full body functionality to someone who has a severed spinal cord so yeah [Applause] so yeah all right um and then I went to emphasize game that the primary purpose of this update is recruiting um a lot of times people think that they you know they couldn't really work at neurolink because they don't know anything about biology or how brains work and the thing that we really want to emphasize here is that you don't need to because when you break down the the skills that are needed to make neurolink work it's actually many of the same skills that are required to make a smart watch or uh modern phone work so it's sort of you know software batteries radios inductive charging um and uh you know as well as things that are specific to to us like animal care and clinical and Regulatory matters um obviously machine learning that phrase is used a lot but we also need to interpret the signals from the brain which is a biological neural net and the best thing to interpret a biological neural neural net is a digital neural net um so this is if there's one message I want to convey it is that if you have expertise in creating Advanced uh devices like watches and phones computers then your your capabilities would be of great use in solving these important problems that's that's that's one thing the message I want to convey uh so um let's see uh yeah so with that I guess DJ uh so so DJ's uh was on the founding team of neurolink and just made immense contributions to the company uh as of many of the others who will present but I want just to thank DJ for his immense contribution to uh neurolink and Frank all right cool thank you thanks Elon when I moved from South Korea at age 13 and needed to learn a new language to communicate I wonder whether there are better and more effective means of communicating my thoughts to the outside world and watching Neo learn Kung Fu and The Matrix I remember thinking wow I want to work on that work on making that possible and today I believe that this is attractable engineering challenge since everything about your intentions your thoughts and your experiences are all in your brain encoded as binary statistics of action potentials if you're able to put electrodes in the right places with the right sensing and stimulation capabilities this and many other applications that Elon talked about are possible and we can help a lot of people I'm incredibly excited to be working on this ambitious yet important mission to make that future a reality here at neurolink and I'm also incredibly honored to be working with some of the brilliant colleagues scientists and Engineers across many engineering disciplines to work on this intersection of biology and Technology you'll hear from several of them today to learn about the breadth of technical challenges challenges we face and our progress in the last year and I think you'll find that for most of these challenges as Elon mentioned you don't need a prior understanding of how the brain works and that a lot of what we do is applying engineering first principles to biology so how do you create a high bandwidth generalized interface to the brain from day one we focus on a set of foundational technologies that are safe scalable and capable of accessing all areas of the brain these three axes safety scalability and access to brain regions really form the basis for how we engineer products here at neurolink safety because we want to make our devices as well as the installation as safe as possible so that we can drive the adoption of this technology and scalability because as we make our devices safer and more useful more people will want it and with scale we also want to make it more affordable and access to brain regions so that we can expand the functionalities of our Technologies so our first steps along these dimensions for our device is what we call the N1 implant it's a size of about a quarter and it has over 1 000 channels that are capable of recording and stimulating it's uh microfabricated on a flexible thin film arrays that we call threads it's fully implantable and wireless so no wires and after the surgery uh the implant is under the skin and it is invisible it also has a battery that you can charge wirelessly and you can use it at home so similarly for implanting our device safely into the brain we built a surgical robot that we call the R1 robot it's capable of maneuvering these tiny threads they're only on the order of few red blood cells wide and inserting them reliably into a moving brain while avoiding vascular germs it's it's quite good at doing this reliably and in fact because we've never shown an end-to-end insertion of a robot in action we're going to do a live demo of the robot doing surgery in our brain proxy so who wants to see some insertions so here it is that's our R1 robot with our patient Alpha who is lying comfortably on the patient bed uh this is what we call the targeting view so what you're seeing is this is a picture of our uh brain proxy and the pink represents the cortical surface that we want to insert our electrodes into and the black represents the vascular Shores that we want to avoid and what you're seeing is these hash mark with numbers that represents where we intend to put each of our threads so should we see some insertions so this is another view real quick on the left is the view of the insertion area and on the right uh what the robot's going to do is it's going to peel the array uh the threads one by one from a silicon backing and insert it into the targets that we predetermined in the targeting View so there you go that's the first insertion [Applause] so we're going to see a couple more insertions the whole process of inserting uh about 64 threads in our first product is going to be around 15 minutes for this robot so yeah there's a second one that went in and we're going to do a third one there you go and then that's going to go in the background and we'll come back to it in the later part of the presentation [Music] [Applause] and as Elon mentioned we've been working very hard to go from prototype to Building Product as part of this one of the things that we did is to move our device manufacturing to a dedicated facility in Austin for scale-up manufacturing and what's important to highlight and is evident in this clip is that it's very typical for us to have our Engineers who design also work on the physical manufacturing line to build and debug and this has been extremely extremely critical in reducing our iteration cycle time and we've also scaled up our surgery so we now have a dedicated our own or in fact a double or in Austin and this is just a stepping stone before we um eventually build our own neuraling Clinic so with this product N1 and R1 our initial goal is to help people with paralysis from complete spinal cord injury regain their digital Freedom by enabling them to use their devices as good as if not better than they could before the injury and as Elon mentioned over the last year this has been the central focus of the company and we've been working very closely with the FDA to get approval and to launch our first and human clinical trial in the U.S hopefully in the six uh in next six months so hopefully this gives you a good overview of our product for the next hour we're going to go through a deep technical Dives on these topics to tell you about our technical challenges share some of our progress and preview what's coming next so with that over to near from my team who's going to talk to you about neural decoding thank you and everyone my name is brand interfaces applications our goal is to enable someone with policies control a computer as well as me or even better would like to provide fast and accurate control with all the functionality of computers that works anytime anywhere so I'm very excited to show you how we are using the N1 device with our software and algorithms to achieve this last year we shared with you a video of page of the monkey controlling computer cursor with his brain so how do we do that just a brief reminder first we record is a neural activity from the motor cortex using the N1 device we have we can record from over thousands of channels while he's playing with the joystick then we can train a neural net that predicts the cursor velocity from the patterns of his neural activity with this decoder he can then control a cursor just by thinking about it without even moving the joystick you can play with this decoder a variety of games also a grid task whereas moving the white dot towards the yellow targets every time he gets one he receive a drop of his favorite smoothie and he chooses to play this game every day here you can see his performance from early 2021 around the time we released the previous demo it's quite accurate but it's a bit slower than what we would like and cursing control is the foundation for interacting with most Computer Applications so since then we've been working to improve cursor speed and accuracy as you can see it's much much faster almost twice as fast [Music] however it it's still still a bit slower than what I can do so we are working on creative ways to improve that now speed is not enough you want the full set of functionalities and for decades most software was built for mouse and keyboard control and it doesn't make sense to reinvent this entire ecosystem for brain control at least for now so we are working and we are designing a mouse and keyboard interfaces for the brain the way we do that is by training Pages Pedro and his friends on a variety of computer tasks and then designing algorithm to predict the behavior here we can see a few example of tasks in different phases of monkey training for example left and right click click and drag cursor typing swipe typing handwriting and even hand gestures now interacting with computer is bi-directional and feedback is very important I like when I click on a button and I can physically fill the button being pressed when a potential N1 user will attempt to click they won't be able to fill it an example of how we are addressing that is by providing a real-time visual feedback that represents the strength of the mural Click by changing the color of the cursor just by typing on a physical keyboard is much faster and easier than typing on an iPad keyboard this will make the band control much faster and easier to use typing one of the most important functionalities so you already sent this message and I want to show you the behind the scene of how this message was created and here you can see again sake using the virtual keyboard tapping this message this virtual keyboard is similar to the one I use on my phone and with the speed and accuracy that we achieved so far typing on a virtual keyboard is already fast and easy however I never use a virtual keyboard when I type on my keyboard on my computer because it covers my screen and it's also much slower than what I can do with my 10 fingers we can do better for example a group from Stanford ask a person to imagine handwriting I had an imagine headlighting letters then they decoded the letters from Israel activity using this approach they were able to speed up the typing the typing rates we start this project with our monkeys but of course they don't know how to write so to mimic writing we train Angela one of our favorite monkeys to trace digits on an iPad here you can see him tracing the digit 5 and the digit 2. then we recorded his neural activity with the N1 device but now instead of decoding the cursor velocity we decode in real time the digit that he's tracing on the screen we had two main takeaways from this project one that monkeys are awesome and can learn very very complex tasks the second one that although it can increase the typing rate it requires hundreds of examples and samples of each of the digits and the characters we wanted to classify this would not scale the way we are solving that is by interaction instead of decoding directly the digits we first decode the M trajectory of this on the screen and then when we decoded the head trajectory we can use any of the Shelf handwriting classifier to predict the digits and the characters for example classifiers that are trained on an MS data set why it's so important it's important because now we can potentially decode any character in any language with only one neural decoder for hand trajectory it means that you can write in English Hebrew Mandarin or even monkey language and we can understand you wanted a banana so there are many challenges ahead of us to improve functionality and speed and I want to hand it off to Bliss to talk about the third Parts how we are making our brain interfaces work anytime anywhere laughs hello everyone my name is bliss and I'm a software engineer here at neurolink when I use my computer my mouse and keyboard where can I intend them to at least like 99.9999 of the time my goal is to enable a user with paralysis to control their computer as reliably as I can here's what we want that experience to feel like in this video you can see saki walking over to its MacBook and choosing to work on his typing task the entire decoding system works out of the box and it feels totally Plug and Play the first step to achieving this kind of high reliability is to test extensively offline a typical flow for using the N1 link is to connect over Bluetooth stream out neural activity from the brain and then use that neural activity to train decoders and do real-time inference we've built a simulation for exactly the sequence but instead of using a monkey with an implant we use a simulated brain that injects synthetic neural activity into an implant sitting in a server rack from the point of view of that implant it's in a real brain this stimulation runs on every code commit to validate that from the hardware all the way up through to the neural decoders our entire stack can achieve state-of-the-art performance however while this kind of simulation is great for integration testing of software and Hardware it's not yet detailed enough to guarantee High reliability in the real world in the real world the underlying signals we're trying to decode actually change day to day in this plot you can see the average firing rate detected on a representative channel of sake's implant each bar represents one day and you can see that each day has a different average firing rate than the previous this presents us with a very interesting problem for how to make our decoders robust day to day it can actually happen that if you train a neural decoder on one day of data and then try to use it on the next the average fire rates can actually shift enough to cause a bias in the output of the model here on the right you can see that this bias is making it hard for the cursor to move to the upper right corner you see it struggling here to make it up to the upper right and then it moves much more effortlessly down to the bottom left we're trying many approaches to mitigate this problem some examples include building models on large data sets of many days of data to try to find patterns of neural activity that are stable across days another approach we're trying is to continuously sample statistics of neural activity on the implant and use the latest estimates to pre-process the data before feeding into the model this is really an active area of research for the team and it's a critical problem to solve if we want to enable someone with paralysis to control their computer as well as I can another big problem we have is to minimize the time it takes for a spike in the brain to impact the movement of the cursor on the screen if you have lag or Jitter in this control Loop the cursor becomes hard to control leading to the kinds of overshoots that you can see here on the right I don't know one big Improvement we've made towards uh in this direction is called phase lock phase lock aligns the edge of each packet that we sent off the implant to the exact moment that the Bluetooth radio is going to wake up this minimizes the time it takes for a spike in the brain to be incorporated into the prediction of our neural network here you can see the latency distribution after phase lock not only has the mean been greatly reduced but the variance has been reduced as well this makes it easier for the user to predict the behavior of their cursor over the last year we've made tremendous improvements to the stability and reliability of our system and we've been able to demonstrate consistent high performance across many sessions and many months however there's still a long road ahead of us before the system will truly feel Plug and Play so if solving the hard problems required to ship this technology is exciting to you you should consider applying to join the team now I'm going to hand it over to Avinash to talk about how our custom low power Asic detects spikes in the brain [Applause] hi I'm Avinash one of the engineers on the Asic team we designed the custom neural sensors which include both analog and digital circuitry to record and stimulate across 1024 independent channels we Face challenges across all three major metrics performance power and area not only do we have to fit all 1024 channels into a single quarter sized implant but we also have to measure spiking activity less than 20 microvolts in amplitude and today I'd like to focus on the last challenge I mentioned power consumption is important to us because we want to give future users a full day of use of their implant without any Interruption for charging back in 2018 we were sending every sample from every channel off the device for processing which burned a ton of power in 2020 we brought Spike detection onto the chip as you may know neurons transmit information by firing so simply monitoring for these spikes and only sending these Spike Events off the implant acts as a very efficient form of compression and over the past two years we've continued to make optimizations within the Asic dropping the total system power consumption down to just 32 milliwatts and doubling battery life let's take a look at our on-chip Spike detection algorithm which makes our battery-powered implants possible we first start by applying a 500 Hertz to 5 kilohertz bandpass filter to remove noise that's out of band next we use an estimate of the noise floor to generate an Adaptive threshold per Channel and finally our Spike detector module identifies three key points of a spike identifying three points allows us to detect not just the presence of a spike but the shape of a Spike as well this can be extremely important for distinguishing between multiple neurons adjacent to a single Channel today I'd like to focus on one of the many optimizations that we've made in our latest chip this one specifically cutting system Power by 15 percent note that neurons Spike relatively infrequently which means that our Spike detector spends a lot of time searching for the first point of a spike and very little time searching for the other two points of a spike that only occur after the threshold is crossed we can use this characteristic of the input waveform to reduce memory accesses within the chip by 30 percent let's take a look at how that works our Spike detector is implemented as a single functional unit that's shared across all channels with an SRAM to buffer the state of each Channel as a sample comes in its Channel state is read from SRAM an incremental Spike detection step is run and then the updated state is written back to SRAM since this is happening 20 million times per second across the implant each of these accesses add up quite quickly in our latest chip we split the state into two parts a hot State and a cold state the hot state is accessed on every cycle while the cold state is only accessed once the threshold is crossed reducing the average axis width and saving power we're also working on a Next Generation stimulation focused chip with 4096 channels still within the footprint of our current chips in addition to increasing the channel count we're also increasing the drive voltage so we can get better activation per Channel and to support this higher Channel count as well as a broad range of future applications that you'll soon hear about we're adding an arm core onto the chip and finally since these chips are the same size as our current chips we can still put four of them together into a single implant for a total of 16 000 channels still within the size of a quarter [Applause] very hard to improve the power consumption within the implant but we've also been working very hard to improve the charging experience of the implant which Matt will talk about but first the robot has just completed inserting all 64 threads so let's take a look thank you this is a view of the insertion site similar to the one that DJ showed you earlier but instead of the targeting reticles if you look closely you can see that all 64 threads each carrying 16 electrodes have been inserted into the brain proxy while avoiding vasculature and all just within the past 20 minutes let's hand it over to Matt now to continue the technical deep dive foreign head of brain interfaces electrical engineering our fully implantable N1 device depends on a battery for continuous operation when that battery is running low charging is accomplished through Wireless power transfer however unlike many consumer electronic devices which can simply offer a physical connector charging a fully implantable device poses several unique challenges first the system must operate over a wide charging volume without relying on magnets for perfect alignment the system must be robust to disturbance and complete quickly so as not to be overly burdensome however most important is safety in contact with brain tissue the outer surface of the implant must not rise more than two degrees C in pursuit of these goals our charging system has gone through several engineering iterations the first if you watched our Pig demo in August of 2020 Gertrude was implanted with a version of the N1 charged with our first generation charger this device was implemented in a small Puck package and later separated into a remote coil and battery base this charger was challenging to use however we learned a lot through its implementation our current production charger which charges our current generation of implants is implemented in an aluminum battery base which also includes the drive circuitry a remote coil four times the size of our original device also disconnectable this uh this remote coil has increased switching frequency driving improved coil coupling this charger is in use today including several applications within our engineering and animal Test Facilities I'd like to show you one of these applications here with a device we call our simple charger and the coil has been embedded into the habitat with the addition of one new outer control Loop plus a banana smoothie pump The Troop has been trained to charge themselves so let's see how pager charges his implant on the right we're streaming real-time diagnostics from pagers N1 when he climbs up and sits below the coil you can see the charger automatically detects his presence and transition from searching to charging we see the regulated power output on a scale of zero to one and the current driven into his battery I mentioned earlier that we improved the coil coupling however the high quality Factor coils exhibit good charging performance over relatively larger distances but as they're brought uh closer to the implant what you see is a peak splitting effect where the the best highest efficiency power transfer is pushed up into higher frequencies outside of the ism band required for compliance with regulated radiated emissions in our next Generation charger we address this problem by the introduction of dynamic tuning shown on the right this allows us to in real time adjust the resonant frequency of the transmit and receive coils so that we can change their properties just ahead of degraded performance the electrical engineering team is currently engaged in developing a third generation charger notable improvements include bi-directional near field communication this has allowed us to reduce the control latency and improve the thermal regulation improve thermal regulation results in Faster charge times and now Julian will tell us about how we test the N1 thank you very much Matt my name is Julian and I lead the embedded software group on the brain interfaces team so when we started building implants we had a small manufacturing line and to collect data from an implant you would manually walk over with your laptop you would connect and collect the data of Interest but our goal is to make an ultra safe and Ultra reliable implant and so to do this we scaled up the manufacturing line now testing throughput and data collection capabilities so firstly we added a large suite of acceptance tests to the manufacturing line these test the functionality of each component and the final assembly implants coming off the line are then subjected to bench top testing accelerated Lifetime and animal models we then collect data from these implants around the clock this data is processed by a series of cloud workers and displayed in an aggregate manner and then finally all of this information feeds back into our design process and empowers our Engineers to answer any question about any implant at any time I'm now going to walk you through different parts of this infrastructure starting off with firmware testing so the implant contains a small microprocessor running firmware to manage a whole bunch of its operations and before we release a firmware update we want to rigorously test it with both unit and hot around the loop tests also known as hilltests so to do a hill test what you do is you instrument the battery you instrument the power rails the microprocessor and then we connect to each device with a Bluetooth client and then we walk the devices through various scenarios to test things like power consumption real-time performance security systems fault recovery mechanisms a lot of different things in our original implementation of these systems we used off-the-shelf components to start automating tests quickly however these systems were constructed in a relatively autism fashion and were very difficult to maintain and this meant that testing quickly became the bottleneck for development so to alleviate this the hardware and software teams developed a new system which integrates all the required components onto a single baseboard we can then put the charger and implant Hardware on individual modules that plug into this baseboard including one board with opposing coils so that we can test charging performance this architecture allows us to rapidly iterate different Hardware prototypes because we can simply drop them into this system and reuse all the testing infrastructure additionally we can host the current and next generation of our mural Asics onto fpgas and plug those into this board as well and that allows us to test a whole extra layer altogether so that's how we generated this rather inceptive image here on the right what you're looking at is spiking activity emitted from some of our simulated neural sensors streamed through the entire system over Bluetooth and then displayed on a phone this allows us to test everything in one system from Chip to Cloud this system is one-fifth the cost one-fifth the volume and is very easy to manufacture this allows every developer to have a personal unit on their desk and it also allows us to test to shot the entire test Suite over a large number of these units mounted into a rack all of this has greatly accelerated our rate of development let's look next at how we monitor the implants Electronics the battery and the enclosure so the implant will periodically capture all of its Vital Signs and commit those to flash and then upon next connection with one of our recording stations it will stream that data off so for instance if we look at humidity we can get an understanding of the Integrity of the implant's enclosure and by looking at battery voltage and power measurements we can gauge battery health all of this is done automatically without any intervention giving us 24 7 visibility into the quality of every single device additionally we can use this infrastructure to request High Fidelity information on demand so that we can investigate different anomalous situations so for instance in this particular scenario we were trying to track down the source of some spurious spikes that we were observing on different channels and so we requested roll wave samples directly from those channels capturing good quality neural signals requires intact low impedance electrodes and so this is also something we monitor very closely with dedicated circuitry on the neural sensor so how do we do this we do this by first using an onboard DAC to play a test tone on a single Channel and then we record using our adcs simultaneously we record the response signal on both that that channel and physically adjacent channels not only can we measure the impedance of every channel with this but we can also map different physical phenomena to different characteristic signatures so for instance an open channel will appear as a very large response on the channel and shorter channels will appear as a large response on neighboring channels by looking at the purity of the signal coming back we can also validate that the analog front end of the neural sensor itself is operational in our original implementation of doing these impedance scans it took four hours to get through all 1000 channels but by paralyzing the tests down sampling filtering and then reducing the amount of information we have to stream off the device by moving a lot of the calculation to the firmware side we're now able to scan all 1000 channels in just 20 seconds this means that we can run impedance on every implant every day and then our internal dashboards can play back a history of this impedance so that we can get a really good quantitative insight into that interface between biology and electronics now that you have an idea about how we test and monitor our implants I'm going to hand it off to Josh who's going to tell you about how we get feedback even faster by accelerating our implants to failure thank you hello my name is Joshua Hess and I'm an engineer on the brain interfaces team we are responsible for the implant system design as well as many of the manufacturing and testing tools Julian just talked to you a little bit about some of the ways in which we test our implant Electronics hardware and software but what about the entire system as it relates to longevity in tissue one of the ways we've addressed this is with the development of our in-house accelerated lifetime testing system the system allows us to expedite and capture long duration implant failure modes at scale to rapidly increase our pace of iteration even better the system also significantly reduces the amount of tests which require animal models both for Implant prototypes and of course longevity testing so how does the system work on a very basic level it comes down to three things first we want to mimic the internal chemistry of tissue next we want to accelerate these chemical interactions as well as diffusion with our implant materials and finally we want to aggressively cycle the internal Electronics of our implant with these things primarily the first two we have achieved a conservative 4X acceleration Factor by the erroneous relationship in other words every day our implants spend in our accelerated system is equivalent to at least four days spent in Vivo historically one of our greatest challenges has been the battle against moisture Ingress into our implants so we continuously monitor the internal humidity to watch for abnormal rise here in white you can see some internal humidity data from implants in some of our animals for the duration of over one year as you can see our internal humidity sensing is so sensitive it can even attack to the very small and slow humidity rise just from diffusion through our implant materials now in blue you can see that same internal humidity data but from devices in our accelerated system now if we adjust this data for our acceleration Factor you can begin to see not only the agreement in this data but also just how far into the future the data extends now in red you can see a device which has failed in our accelerated system this device showed an abnormal increase in humidity over the duration of many months before implant electronic failures occurred so how do we build the system well we started building the first system prototype just after the kova shutdown had begun in early 2020. so we had to get a little creative as you can see our first system prototype was a little Scrappy and operated out of one of our Apartments as indicated by the carpeting although Scrappy the system allowed us the fastest path to start testing our devices tuning Our working fluid chemistry and checking our constraints we also immediately started root causing observed failures in early implant prototypes fed that information into the next prototype designs and literally rinsed and repeated over the duration of just a few months the system was built out totally custom and highly iterated with two system versions and countless minder iterations leading us to our currently operated third generation system which achieves high density testing with automatic in vessel charging as well as automatic data collection the system also features an implant sled assembly which accepts brain proxy material such that the implant can be installed and inserted by the surgical robot just like you saw a few minutes ago we also integrated the system into a high density rack mount form factor along with the centralized fluid management system both for chemical uniformity across vessels and also reduced operational maintenance the system has been in operation for the last year and a half and has had its fair share of challenges since the system itself is undergoing the same accelerated abuse as the implants within it it has been extremely challenging to design build and maintain a system of the scale while keeping it robust even against itself so what comes next well we've started work on our fourth generation system and have totally redesigned it from the ground up to be a hot swappable single implant per vessel design partly inspired by high density compute servers with this new system we will achieve a whole new level of density robustness and scale we also intend to have many of these systems operational in the pursuit of capturing even the lowest frequency Edge case failure modes with this we will have thousands of implants testing in pursuit of these goals we've already started work building out the system but there is still a lot left to do there are also many exciting challenges ahead of us such as introducing mechanical stressing brain proxy micro motion and Ethan replicating tissue growth around the threads for more complete and representative accelerated testing so now that you've heard some of the ways in which we rigorously test our implant designs before production for surgery Christine is now going to take you through a detailed look at our surgical process thanks Josh hi everyone I'm Christine leader of the surgery engineering team to get an N1 device it's essentially these steps targeting and the incision drill the craniectomy remove the tough outer meningeal layer called the dura then insert the thin flexible threads of electrodes place the implant into the hole we created and then that's it you've got an implant Under the Skin look ma no wires just kidding uh I mean seriously no wires but I don't actually have one the surgical robot does the thread insertion part of the surgery this is because it would be very difficult to do manually imagine taking a hair from your head and trying to stick it into a Jello covered by Saran Wrap and doing this at a precise depth and position and doing this 64 times within a reasonable amount of time and a neurosurgeon would probably not like it very much if we asked them to do this for the surgery so we have the robot that you saw doing its tiny dance I sort of wanted to call it Tiny Dancer but it's called R1 which is also great the rest of the surgery is done by the neurosurgeon in order for us to make a accessible and affordable procedure we need to revisit this I'll tell you why when I was in school my dad lost the ability to walk and to use his arms and even to speak he was diagnosed with ALS we would look on the internet and you could see maybe one person here or there who had some cool custom robotic assistive device but it was deeply frustrating how limited were the options available to him and there's hundreds of thousands of people with paresis not even counting people with other conditions that our device might be able to help meanwhile there's not that many neurosurgeons maybe about 10 per million people and it takes about a decade or more to train a neurosurgeon and they're already generally very busy and as you can imagine the time is very expensive so in order for us to do the most good and have an affordable and accessible procedure we need to figure out how one neurosurgeon could oversee many procedures at the same time this might sound sort of crazy but probably so did laser eye surgery before Lasik made it normal lasik's been around for about 30 years and counting in the beginning the laser robot did just the most fundamental core part that it had to do and the surgeon did the rest and over the iterations the surgeon has to do less and less and the laser robot does most of it and it's a highly compelling procedure it takes just a handful of minutes and often gives life-changing results since I joined in 2017 we've also done a handful of iterations to optimize the threat insertions of the robot one of the challenges that we've had to face has to do with the optim mechanical Packaging so as you can see here there's about three primary Optical paths that are really valuable for a staff reliable threat insertions one is the visible Imaging of the needle inserting a thread and then another is the laser interferometry system called OCT Optical coherence tomography that gives us the precise position of the brain while it's moving in real time and then also we have to provide lighting and illumination to see what's going on in the visible visible light camera and doing all this where the needle is at the bottom of the craniectomy especially when it's close to the skull wall can be pretty difficult to fit everything and be able to see it so the way that the team solved this is by putting all three of these Optical paths into one Optical stack using Photon magic or polarization whatever you want to call it and that enables us to do vessel avoidance in real time so as I mentioned the brain is moving and where we place Targets in the beginning may not be where you want to insert at the moment the needle is going down there so the robot can actually detect the vessels and then determine if we're going to insert onto a vessel or not if it's safe to insert and then that way we can avoid inserting onto major vessels and that brings us to the robot that we have here today there's still a lot for us to do to get to that procedure where we reduce the role of the neurosurgeon and make it affordable and accessible the primary the two elements of the surgery that demand the most skills from the neurosurgeon are the craniectomy and the directomy Alex and Sam are going to tell you a bit more about how we think we can get rid of the directomy step so that leaves the craniectomy in neurosurgery if your craniectomy is small enough you can use a standard tool called a perforator which makes quick work of this job but for a larger craniectomy the surgeon has to rely on their skill in order to accommodate the variability Patient to Patient in skull thickness skull hardness even within the same patient in the same craniectomy you can have different skull thicknesses for example in addition if we can make something that has a very high Precision craniectomy we can open the design space for future ways of mounting the implant to the skull so I'll show you a few of our prototypes ultrasonic Cutters like what's on the screen and oscillating Cutters have the benefit of not cutting soft tissue you can cut the bone and not the brain but however as you can see here our ultrasonic cutter prototype created quite a bit of heat to cut at the rate that we wanted so onto the oscillating saw here we designed a blade to minimize cut time and also conducted sound and also Heating and as you can see you can cut through hard things like bone but not soft things like skin it's simple and it's it works however if you wanted to cut an arbitrary depth or arbitrary shape the oscillating saw just won't cut it I was afraid no one would get it if you guys are smart so there's a time-tested solution for uh drilling arbitrary shapes which is a CNC drill the challenge with us doing this on a person is that we need to make sure it cuts reliably every single time doesn't cut too deep and a few ways that we're using feedback to you know make sure we don't cut through the brain or force feedback and also impedance and if I could get a volunteer just kidding maybe next time um but yeah so this is a some insight into some of the things we're working on to make an accessible and affordable procedure and now Alex is going to tell you a bit about our next Generation developments [Applause] thanks Christine I'm Alex I'm a mechanical engineer here on the robotics team now that we've covered the technology and surgical process for a current device we'd like to cover some of our next Generation development projects I and the next couple speakers would like to talk about one of those projects which is enabling device upgradeability you've gotten to hear about the advancements we've made over the past year we've improved implant robustness battery and charging performance Bluetooth usability realistically every new device version is going to be significantly better it'll be more functional it'll last longer we need to keep this new technology accessible for our early adopters this means that we need a solution to make device upgrade or replacement just as easy as it is to initially install as many medical device companies have found this is a challenging problem the body's healing response doesn't make this easy so this isn't solved yet but we've made significant progress towards enabling this that we'd like to cover today now we'll have to start with some background as to what makes device upgrade challenging and we'll start with the anatomy Under the Skin you have the skull below that the dura a tough membrane that separates the bone from the brain and between the dura and the Brain you have the pier arachnoid complex a fluid-filled suspension for the brain to install the device the surgeon removes a disc of skull and Dura to expose the brain surface the device then replaces the removed material the challenge is here at this interface over months all empty volume is filled by tissue encapsulating the device and the threads the device would be trivially easy to remove because of the thread's small size they would slip right out of the brain it's the tissue layer that forms above the surface that makes a removal challenging we built tools in-house to study this response and characterize it such as histology and micro CT in these images you can see that layer of tissue that has formed above the surface encapsulating the threads and adhering to the surrounding tissue we've explored many different avenues for Designing around the ceiling process and finding a solution to make device upgrade seamless our best successes have come from making the procedure less invasive instead of directly exposing the brain's surface we instead keep the dura in place maintaining the body's natural protective barrier this prevents encapsulation of the brain's surface and really this is actually a huge win for making the surgery simpler and safer as Christine alluded to however this doesn't come for free the dura is a very tough opaque membrane as you can see in these sem images it's composed of a dense network of collagen fibers these offer an array of technical challenges for inserting our electrodes one of those challenges is Imaging through the dura as you can see on the left our current custom Optical systems offer pretty incredible capabilities for Imaging the exposed brain surface however as you can see on the right once the dirt is in place you can't see the dense vasculature at the brain surface the dirt is in the way there's simply too much attenuation to solve this problem we're developing a new Optical system that uses medical standard fluorescent dye to image vessels underneath the tissue here you can see that die perfusing through the vessels highlighting them there's still a lot of work engineering work to go to prove accuracy and repeatability of this system but once that's done this will allow us to Target and avoid blood vessels underneath the dura we're also exploring applying our laser Imaging system to deeper tissue structures in the bottom left you can see a section of the tissue layers underneath the dura this image is compiled from multiple volumes from our Optical coherence demography system you can see the collage of those volumes above in the future these new systems when combined with correlation to pre-op Imaging such as MRI will enable precise targeting without directly exposing the brain surface now Imaging isn't the only challenge that comes with the tough Dural Anatomy now I'd like to hand it over to Sam to talk about some of the challenges of inserting our electrodes through this membrane thanks Alex hey I'm Sam and I lead the needle manufacturing and design team so as Alex mentioned the same properties of the dura that make it a good protector of the brain also make it really difficult for us to insert the threads into in humans the dirt can be over a millimeter in thickness which doesn't sound like a lot but compared to our 40 Micron needles it actually is a lot for example if you've scaled up the needles to the size of a pencil the dura would scale to over four inches in thickness take a look at how far you have to zoom in to even see it by the time the features of the needle come into frame you can see individual red blood cells in the same frame this is this is just wait this is a real-life sem image of our latest design on the left there you can see the end of the thread in the middle is the needle and on the light is actually a piece of my hair so yeah it's extremely small and besides being really small there's a lot of other challenges associated with designing this um One Challenge is that we have to use the needle and the protective cannula that it sits in to grab onto the thread and to hold it while we peel it from this protective silicon backing and then we have to keep holding it while we bring it over to the surface and then release it from the cannula during insertions another challenge is that the brain is really soft beneath the tough Dura and so if the needle isn't sharp enough it'll just keep dimpling the surface without puncturing and if this free length gets too long it can actually just Buckle the needle like this is that we don't just have to get the needle through we have to get the thread through as well so we really have to focus on optimizing the combined profile of the needle and thread together these are just some of the challenges associated with designing something like this before we found that the key s problem has been improving on our speed of iteration but let's look at how we make these things in the first place so we start with a length of 40 Micron wire made out of tungsten and alloyed with a little bit of rhenium for added ductility we designed this femto second laser Malin house to cut the features of the needle and cannula and it can do this with sub Micron precision we spent a lot of time this year turning this thing from a science project into an industrial system just a couple months ago it took a skilled operator 22 minutes to make a needle and even a skilled operator could only get about 58 yield today that same process takes just six minutes and anyone can get 91 yield with just a few minutes of training with only one click the mill cuts and measures the needle in cannula and uploads the measurements to our limb system so that the robots can use the exact dimensions for each needle that it uses now this is all for a current design though and we've had a couple years to optimize the manufacturing process of it the current design has served us well so far but it doesn't quite protect the thread well enough to get through the tough Dura so like I said we had to come up with something new and we needed to be able to iterate on designs quickly unsurprisingly there's no page in machinery's handbook for this kind of thing so we dug into the science of femtosecond laser ablation and figured out a workflow that allows us to use our laser Mount much like a CNC mill this allows us to iterate several times this allows us to iterate in under an hour for new designs allowing several iterations per day when we're really on a roll as a result the latest design seen on the right can actually insert through nine layers of direct a totaling uh three millimeters on the bench top this is far more than we could ever expect in a human with significant margin [Applause] the needle isn't the only part of the puzzle though as you can imagine all these designs here work with different threads so we need a way to iterate on that as well and we do this by having our microfabrication process here in-house this summer we completely rebuilt our clean room in about nine weeks which among other things greatly reduced particulate counts which allows yield and throughput to greatly increase this combined with all the other great improvements the microfib team has made allows us to iterate on new designs in just a matter of days the last piece of the puzzle though is testing we can come up with as many new designs as we want but unless we have a way to actually test them in the right conditions we won't know what to tweak or even worse we'll spend time optimizing for the wrong things take this failure mode for example um a few months ago we got to the point where we could pretty reliably insert through the dura but when we took the proxies and put them in our micro CT Imaging we realized that our hold on the end of the threads was actually too strong and we were pulling them out just a little bit underneath the surface by the time we solved the problem we realized that this issue was very sensitive to the properties of the surrounding material or tissue we could make a proxy where this never happens and we can make another proxy where this happened every single time and this highlights why it's crucial that we spend time making our benchtop tests Mash tissue as accurately as possible I'm going to pass it off to Leslie now who's going to talk about how we've been doing that thanks hi I'm Leslie and I lead microfabrication r d and part of what we're interested in is understanding the biological environment our implant and threads experience once they're fully installed in the body learning directly from biology though is inherently slow so in order to move fast we're developing synthetic materials that mimic the biological environment this allows us to learn as much as we can on benchtop and start taking steps away from the industry standard of animal testing developing accurate proxies though is challenging the implant environment is made up of many anatomical layers that all have unique properties and as time goes on and the implant site heals new tissue forms filling any available space in addition to that motion related to cardiovascular activity and head movement introduce added complexity so to start addressing some of these challenges we're engineering materials using feedback from biology this may involve mechanical characterization of tissue or analysis of interactions at thread tissue interfaces much of this characterization is even done during surgery itself by using custom hardware and software that modifies our surgical robot to double up as a sensitive characterization tool we then use the data collected and feed it back into optimizing our materials so that they behave mechanically chemically and as shown here structurally just like biology we've come a long way from our humble first brain proxy shown here sitting on a plate and consisting of agar and a pyrofoam sheet and while simple it allowed us to perfect robot insertions through countless bench shop tests today our proxy is slightly more complex where we've upgraded to a composite hydrogel based brain proxy that better mimics the modulus of real human brain we've also Incorporated a duraproxy and developed developed an injectable soft tissue proxy that so far has allowed us to perform benchtop mock explant testing we have a super long wish list for our proxy of the future but some of those items include a surgery proxy with integrated soft tissue brain bone skin or even a whole body a brain proxy that simulates motion vasculature and electrophysiological activity and a biological proxy to test biocompatibility and electrical stimulation there's a ton of ongoing work getting us closer to our proxy of the future including work on lab-grown cerebral organoids as shown here and all of this will get us closer to a future where we learn more and iterate faster on benchtop and reduce our Reliance on animal models or even when they replace them completely and with that I'll hand it over to Dan who will be presenting a very exciting Next Generation application thank you thank you Leslie my name's Dan and I came to work at neurolink after following a career in visual Neuroscience research I was inspired to join this company because I saw in our device the potential to restore Vision to people rendered Blind by eye injury or disease there are a number of particular characteristics of our device that make it uniquely suited to this application firstly as well as being able to record from every channel we can stimulate neural activity in the brain by injecting current through every channel this is important because it allows us to bypass the eye and generate a visual image in the brain directly secondly our device can have an enormous number of electrodes for a visual prosthesis this is important because the more electrodes you can have the higher density of an image you can create in the brain thirdly thanks to our robot we can insert these electrodes deeply into the brain now this is an important thing for a visual prosthesis because the human visual cortex is buried deeply in a fold in the medial face of the brain called the calcarine sulcus in this image I've highlighted the calcarine Cyrus in red in an MRI it contains a map of the visual World visual field it's about the surface area equal to a credit card on each side and if you unfold it and flatten it you see that the image is inverted it's upside down but more interestingly it's mag it's distorted so that the central part of the visual field the fixation point is greatly magnified so for example if you look at this image of Lincoln if you look directly into his right eye everything to the left of that fixation point is directed to your right visual cortex and everything to the to the right goes to your left visual cortex his eye even though it's very small in the image is magnified in the brain to occupy nearly a quarter of the surface area of the visual cortex over the last half century visual neuroscientists have developed a profound understanding of visual processing in the brain what's Driven most of this research is recording from single cells in the cortex usually of macaque monkeys one of the seminal discoveries was that every cell in the visual cortex represents only a tiny part of the visual field your perception is made up of a mosaic of tiny receptive Fields each belonging to a single cell in your visual cortex so if you record from one of these cells in a monkey say in this location you can find a very tiny region of the screen where a light stimulus will cause modulation of that neuron another location in visual cortex will have a location Elsewhere on the screen in this case in the lower visual field these regions are called receptive fields we've inserted our device into the visual cortex of two rhesus monkeys whose names are code and dash that means we can record activity from their visual cortex generated by their not their normal home environment as they roam around but as we all know monkeys love banana smoothie that means we can easily teach them to fixate points on a screen and reward them we can reward them very precisely because we can track the location of their eye using an infrared camera one of the things this allows us to do is to plot the receptive fields for every neuron that we can record with a single device now we do this by showing the animal a movie of random checkerboards whilst you fixate steadily on the screen then we take only the frames of the movie that generated a response in the cell and averaged them all together this is a technique known as reverse correlation it's generally used quite widely in visual Neuroscience for this purpose and this is an example of a receptive field plotted with this technique the central cross is the fixation point and you can see the little red and blue regions of excitatory and inhibitory receptive field these regions give cortical cells some of their characteristic properties and record all receptive fields from all the electrodes at the same time and if we take all these receptive fields and accumulate them together overlap them and place them on a on a computer monitor for scale at a typical viewing distance you begin to get an idea of how much the visual field we can cover with this preliminary device many of the receptive fields are close to the phobia so close to the fixation point and that's partly due to the magnification I talked about with the phobia but there's also a scattering of fields in the periphery these are from recording sites deeper in the brain in the calcarine sulcus so far I've only talked about recording information from the cortex but to produce a visual prosthesis we need to stimulate so if we stimulated the cells whose receptive fields are in this location we would produce a perception of a flash in that location that only the monkey can see how do we know that the monkey sees it how do we know what it looks like well unfortunately we can't ask them what they see but we can train them to tell us something about that phosphine we start by training the monkey to fixate a central point on the screen like this white dot and we start by presenting real visual stimuli on the screen and rewarding the monkey for making eye movements toward those stimuli so here we flash a white dot and the monkey makes an eye movement towards it symbolized by the Green Arrow we then choose another random location and reward the monkey for making an eye movement towards it once he's got good at this task we can begin to interleave these real stimuli with electrical stimulation of electrodes and produce a phosphine the monkey sees the Flash and naturally makes a card towards it this tells us not only where in the visual field The Flash occurred but we can also change the current that we inject in that electrode to see how often he makes that's the card and noticeable or how big perhaps the stimulation phosphine is that we're producing look at code performing this task I want to show you first at one quarter speed uh there's a visual Flash and he makes an eye movement towards it we the monkey can only see what is white on this screen he can't see his own eye movement and you can't certainly can't see when we stimulate but here we stimulate and he makes the same circad to the same location because we stimulated the same electrode nothing appears on the screen at that time and he has no other cue to make that eye movement let me show you this in real time you can see monkey monkeys like to work very quickly and when we stimulate he makes that's the card in real time and looks like he's had enough so what I've shown you is a way to produce a phosphine in the visual field this is not something new in visual Neuroscience but if you think about that phosphine as a single Pixel in a visual image all we need to do is scale up and produce a great many more pixels and have them covering the visual field this is a schematic of what a visual prosthesis using our end device might N1 device might look like a camera the output from a camera would be processed by an iPhone for example which would then stream the data to the device and the image would be converted into a pattern of stimulation of the electrodes into visual cortex with a thousand electrodes we might be able to produce an image resembling something that you see there on the right but as Avinash told you our next generation of the device will have 16 000 electrodes if you put a device on both sides of your visual cortex that would give you 32 000 points of light to make an image in someone who's blind our goal will be to turn the lights on for someone who's spent decades living in the dark thank you thanks very much I'll pass you over to Joey who's now going to talk about another very exciting application of our device thank you Dan so my name is Joey I'm an aero engineer and I'm the head for the next gen team at erlink so for persons with spinal cord injury the connection between the brain and the body is severed the brain continues functioning normally but it's unable to communicate with the outside world you've already heard about how we can use the N1 link as a communication prosthesis to help someone with spinal cord injury control a computer or a phone but it can also be used to reanimate the body let me show you how first a little neuroanatomy movement intentions arise in motor cortex and are sent down long nerve fibers through the spinal cord these are upper motor neurons in the spinal cord they synapse that is make a connection with another motor neuron a lower motor neuron which sends these movement intentions to the muscles which contract and in turn you have movement while of course there are many other circuits involved in voluntary movement you can think about the spinal cord as many pairs of these two connections and in spinal cord injury one of these connections is severed unable to make the muscles contract let's Zoom a little bit further so here you can see on the left across a cross section of the spinal cord with a fiber coming down schematically this travels through the white matter tracks this is the upper motor neuron and then it synapses within this butterfly shaped region of gray matter in what's known as a motor pool in the motor pool the lower motor neuron descends out the ventral roots to the muscles which contract and then the sensory consequences of those movements for example the touch of your hand against an object returns of the spinal cord through the dorsal roots and Ascend the spinal cord up into the sensory regions of the brain again in spinal cord injury this connection is severed if we could place electrodes into the spinal cord say in a motor pool adjacent to lower motor neurons we could stimulate those neurons activating them and in turn causing the muscle to contract and movement to occur but this is very hard to do the spinal cord is quite delicate and it moves significantly within the Bony spinal canal this could cause damage to the electrode it could cause damage to tissue or both but our electrodes are small and flexible and our robot is able to insert them deep into tissue perhaps all the way down into the ventral horn spinal cord and so we have done just that here you can see A View From the R1 robot it's a targeting View and we've placed electrodes across many millimeters of the spinal cord and the R1 robot is able to insert those electrodes deep into the ventral horn into motor pools in very close proximity to lower motor neurons this is important because it allows them to have a localized connection to those neurons and activate very precise movements now to track movement it's very common to use motion capture markers like you might see in the production of a movie these can be placed with a light adhesive and you can see me placing these on my hand we're going to use these markers to let us zoom in on movement in the next couple of slides okay so here's a pig walking on a treadmill and you may have seen something like this before in a previous knurling presentation but unlike before this pig has more than one neuralink device there's a device in the brain but there's also one in the spinal cord and we can stream neural data from this device these devices in real time and use them to do things like decode the movement of the joints of the pig so here you can see on the left a Time series of the hip knee and ankle and we're decoding those those movements so this is super cool but that's actually not what we want to do we want to go in the other direction we would like to stimulate the spinal cord and cause movement to occur okay so let's do that so here's a pig a happy and healthy Pig doing what pigs like to do which is root around for food and snacks and as you'll see on the floor there's a blue square this is a voluntary engagement Zone where the pig places itself indicating that it's comfortable to receive stimulation when it's in the zone we stimulate and if the pig leaves the stone we'll stop stimulating uh and as before you can see we're able to track the position of the joints and also stream neural data as well okay so let's stimulate an electrode so here's one electrode on one thread that when we stimulate causes a flexion movement of the leg so on the left you can see the movement of the joints and you can also see the time series of the stimulation pattern in yellow so the leg is moving up here's another electrode which when we stimulate causes an extensor movement this is actually a little harder to see because the leg is straightening and the hips are shifting but if you look carefully you can see how this is the leg is moving we can stimulate on a great variety of threads and produce different movements and actually sequence them spatial temporarily to provide patterns so on the left you can see a Time series of different stimulation on different electrodes you can see the movements of the joints and on the right we're zooming in on muscle activity that gives us an idea of the kind of strength and power and specificity of those movements as well so in addition to doing sequences we can also achieve sustained movement these are powerful muscle contractions of the sort that you might need for standing or other load-bearing activities and are really crucial for interacting through the world okay so stimulating the spinal cord is only one piece of the story you also have to get like command signals for the stimulation of the spinal cord unfortunately we have a way to do that we have the N1 link that you've already heard about placed in motor cortex how would that work so we place threads in motor cortex and record spikes these spikes would be wirelessly transmitted in real time and decoded into patterns of stimulation stimulation would then be delivered to the ventral Horn of the spinal cord to the appropriate motor pool for the muscles that we like to activate we then stimulate activate those lower motor neurons which causes the muscles to contract and movement to occur now of course movement without sensation is actually kind of difficult just think about what it would be like to try to move your limbs if they're numb but we can also get sensory information as well so the sensory consequences of your movement can be recorded in the dorsal Horn of the spinal cord in the form of spikes for example here a feather touching the hand these spikes can in turn be decoded in real time sent to patterns of stimulation to either the same and one device in the brain or perhaps a different one in a sensory area stimulation of that part of the brain would cause percepts of touch and proprioception closing the loop so putting those two Loops together we have motor intentions decoded from the brain used to stimulate the spinal cord causing movement and then the sensory consequences of those actions being recorded in the spinal cord to stimulate the brain causing perception now we have a lot of work to do to achieve this full vision but I hope you can see how the pieces are all there to achieve this and if you find this Prospect as exciting to you as it is to me I hope you'll consider joining us here at neurolink [Applause] so foreign [Music] it would also be great for the scientific Neuroscience Community to access some of these tools do you have any plans to make these available to neuroscientists yes yes we do um so um that's a great question is uh I think there's probably a lot that could be figured out if we provide the uh surgical robot and devices to Neuroscience uh research departments at your universities and hospitals so I think at the point of which we have we need to be in production with the machines and obviously have the FDA approvals but I think uh it would make a lot of sense to provide this to research universities and hospitals follow up the question is uh of the data that we have the data sets that you've collected are there any that you plan to open source for the scientific community yeah I think that would be that would be fine I think uh yeah sure absolutely because I think it could be really interesting for people uh uh to build upon that um and mail and build Foundation models for the brain yeah that's it's a good point yeah like actually no problem with uh just publishing it on our website you can use it if you want looking forward great thank you for the very wonderful presentation so I have one question so as we will know for implantable electrode either for stimulation or recording after we implant the electrode Scar Tissue will grow around the electrode and especially for uh recording the signal we get will become smaller and smaller after long-term uh implant uh how do you solve this issue uh so uh for context I'm Zach I leave the microfabrication team on brain interfaces uh I don't think we can solve it specifically but uh one thing one advantage we have is both the flexibility and the small size of our threads to try to limit that scar tissue and that damage and uh some future work that we have started working on that we'll continue working on is pushing the size of the threads down um just to try to limit the immune response and really limit that scar tissue growth uh I actually have I want to follow up so do you think it will be helpful to actually load some drug on on the surface of your electrode or some other way well I think like maybe the just the question is like what what sort of signal degradation have we seen over time um and uh you know basically just does it does it work a year later does it work two years later um it does so yeah yeah so uh that's a good point so in terms of thread longevity specifically um really the gold standard that we can use to assess is the data we have from our animal participants uh and so for that uh I'm not sure if it was mentioned before but uh the longest data we have right now is for an animal participant who has 600 600 days with useful functioning channels where we were doing uh something useful with the the signals for BCI uh and then with the newest version of our device we have uh sort of a collection of participants who are at or near one year of data and uh completely useful functioning BCI from that as well thank you if I may add one more thing so you mentioned uh potentially having drugs to kind of reduce inflammation so one of the things that we are actually actively working on is having some sort of biological coding to either reduce inflammation or make them slippery so you know you mentioned uh you heard from the presentation that one of the challenges that we have is removing the threats from these neon membrane tissues that are formed after implantation so there are programs like that where we're really looking at kind of incorporating some of the learnings from biology and these Coatings into our thread so that we can hopefully reduce inflammation as well as make it easier to extract we're also continuing to reduce the size of the the electrode so as when the electrode gets really small there's um the sort of inflammation response are Scar Tissue becomes minuscule so it's like a very very tiny electrode the body basically ignores this is really impressive congrats to the whole team um so as as you of course know one of the problems with current electrodes is they're rigid and they move around so you have these neural nonstationarities and I think many of us had hoped that with these very thin threads they would maybe move more with the brain and you wouldn't see that but from the data you showed over many hundreds of days there was a lot of variability so can you speak to how much do they move and do you have any idea of like why does it move can you stop it from moving is it how stable are the signals hour to hour and day to day hi I'm Bliss I'm one of the leads of the software groups in the brand interfaces team in the particular plot you were mentioning before what we were showing was the average firing rate recorded per day on a particular Channel it's as you well know pretty complicated to understand if you're recording from the exact same neuron day after day after day it could be for example that you're actually picking up a different neuron day to day and that's why you get the change in firing rate we don't think this is at least the majority cause of the situation here the reason is that if you look at sort of the spike shapes day to day even when the average firing rate is shifting a lot you still see sort of stable Spike shapes that's obviously not a fully bulletproof story but at least gives some confidence that it's not actually different neurons you're picking up however there's still is very much a chance that that could be the case in at least some part of the of the robustness now stationary story yeah cool thanks yep thanks question yeah to be clear that like the Electoral position is actually fairly stable um because you've got these very tiny basically very tiny wires with with uh that and then there's some play in the like you've got you've got the advice attached to the skull ritually but then you've got this this long so tiny wire with kind of a a coiled section so it's a it does tend to basically stay in the same place couldn't erling help realize well I mean it's uh um once you're in there you know there's a lot you could do um so um you can obviously measure temperature so you could do very early detection of a fever you could not measure uh pressure I think you probably detect that um at the very early the very beginnings of a stroke because you can see sort of like electrical signals starting to go sort of hair wire so there's actually probably a lot of um just General Health monitoring that you could do once you're in there you know and and with with very simple sensors hi um you guys all did a great job of distilling a lot of complex engineering and Science and making it wonderfully clear so great job um I wanted to ask a little bit about the stimulation I guess for the phosphines and for the the evoked movement um how are you are you thinking is this more like local stimulation is it is it juxta cellular are you steering current around how many cells are you activating how much current are you using I'm just curious what the scale of this is and whether you have a lot of precision or a lot of you know pretty profound behavioral effects too hi yeah I'm Dan and um how many cells you stimulate with a single electrode is dependent on the impedance of the electrode size of the conductive pad how much current you deliver frequency all these factors so there's a great deal of variability that we can use to customize the the shape of a phosphine or the or the shape necessarily but maybe the the intensity of the phosphine we think with our current electrodes at least in code back of the envelope calculation would be something like about a 50 to 100 Micron diameter sphere of cells are being stimulated um in a visual system the smaller that sphere the smaller and more specific you can make a particular phosphate basically the smaller the pixel in in the image you can produce so there's plenty of scope for customization of that there's actually also it's possible to get to a much higher like effective pixel count by um controlling the the the field electric field between the electrodes so uh it's not necessarily it's not a one-to-one relationship you get actually dynamically adjust the the field and simulate Farm have a have a very high neuron to electrode ratio so try to like could you get like um you know maybe 10 to 100 to 1 potentially so it's a megapixel type time basically can you see it normally but I think people would want to know that and I think that is one of the one of the possible outcomes pylon this is amazing oh can you talk about the longevity of the implant itself also how would the material of the implant would react with the brain tissue or density of the bone or bone structure thank you happy to talk about this I'm Jeremy an engineer on the brand interfaces team and I think it's good to start with data so like Zach mentioned we have an implant that was you know a monkey was performing BCI for 617 days and that was pager before being upgraded to the latest device uh for our current version of the device it's lasted for almost a year and then for Accelerated lifetime tester that Josh kind of talked about we have data from our implants from the previous version eight years of accelerated time and from the current version four years of accelerated time and Counting so that's kind of starting with the data those devices are still lasting and still going um theoretically there are kind of three fundamental factors that contribute to the longevity of the device one is going to be the seal the Hermetic enclosure of the device two is going to be the battery and internal electronics and then three is going to be the threads that Zach talked about a little bit and the channels being able to functionally record signals from the brain the seal we think will far Outlast the other two in terms of the bottlenecks so the seal just theoretically I think Josh mentioned that it is a thermoplastic polymer material so there's going to be a very small amount of moisture that diffuses through it over time and we think that that will last you know 20 plus years easily in terms of just that property and like I said we have not seen our seals fail with our current version of the device yet so we haven't really pushed the limits here for the battery and internal Electronics that's really based on usage and how much runtime you want and we are working currently on getting data to project out even farther but right now we believe that we can you know achieve 80 run time at the three year time point which would be about you know three and a half hours for a four hour run time but we're like Avinash mentioned we're we're doubling that very soon and quadrupling is what we have plans to do that so the internal Electronics really aren't the bottleneck either and so really we're attacking the threads themselves and longevity of those channels that Zach Zach can kind of talk about some of the improvements that we're doing to increase that longevity cool thanks yeah so uh as sort of mentioned before we don't necessarily have an end point as Jeremy said for uh the testing of the threads that being said we are focusing on longevity because we think this is an important uh issue to solve uh so one thing that we're doing in parallel with the current device is aggressively pursuing uh silica amorphous silicon carbide insulation of the threads which we believe will take us well beyond five years uh of longevity but of course still to be tested and in parallel with that we're just starting to look at Atomic layer deposition which we think could even push longevity of the threads much further and deposit very thin layers to keep the flexibility of the threads and that Advantage there so along with that we're also of course having to design and validate very robust benchtop testing to model uh really in Vivo conditions and look at Channel degradation so that's what we're looking at for longevity of the threads and then I think you asked about biocom pump and I think for biocomp uh essentially all the materials we're using right now I can say or at least biostable and we send out testing for biocompatibility very often and essentially what we're doing is we're using in many cases known materials from literature that academic Labs have already started to look at and sort of jumping on that and using that as a starting point thanks for answering that cool so we have another question from Twitter this is from David and he asked the team what are the biggest lessons you learned since the previous presentation it's been about two years I'm sure there was a lot of engineering done so yeah anyone want to answer what we learned in the last two years so uh one thing that we've learned in the last couple years is just how much the brain moves um on the human scale compared to you know when you start small when you make brain proxies and a lot of research starts with rodents the brain does not move that much then you get a human and the Brain can move like hundreds of microns or more and when our threads and needles are so small that motion when you zoom in looks like a mile I think to add to that um one thing is how I guess Dynamic the implant environment actually is so we've talked about like when those implant site heals SCAR or new tissue might grow and fill in the space and that'll affect like how our threads might interact in that space so that's why we've emphasized so heavily the importance of Designing accurate proxies so instead of having to wait months for an implant site to heal you can hopefully learn that information in hours I'm Alex on the robotics team I think one of the things we've definitely learned within the engineering teams is the importance of really continuous validation and testing where we're building say motion systems that are precise to single digit microns we need validation and test systems that we trust even more than that to prove that they work reliably and putting just as much focus into those validation and test systems and designing those alongside our products I think is one thing we've definitely learned thing and I don't think that we learn I think as part of BCI or the pain control and algorithm is that again building a prototype and making it work with only one monkey one pager was a great maybe a success but also relatively easy to making it work every day for all the other monkeys so actually making it a product is something that it's not easy but we are learning how to do it I mean I've learned that the brain is really squishy like way squishier than you think it's not like uh you know cauliflower or broccoli or something like that it's more like water balloon uh and then it's moving in your skull like a lot so get a squishy water balloon in a coconut is maybe a good way to think of it hello um given Bluetooth bandwidth limitations have you considered other Technologies for wireless communication foreign hey yeah I can take the first part of this question and then I'll let Matt to the second part of it um it's a great question especially as you think about how to increase and scale the number of channels that we want to record from this becomes increasingly a bottleneck for the kind of work that we want to do uh we're thinking about this in a couple ways one is just directly improving the underlying radio interfaces and I'll let Matt talk about that in a second the other way we're thinking about this is how can you be more efficient with the data you send off the implant and I think the first version that is compression so just taking your data looking at the characteristics of it find out a way to represent it more efficiently and just send off that compressed Stream So for reference right now our Bluetooth bandwidth is around 150 kilobytes per second the compressed stream of data that we send off the implant is around 50 kilobytes per second so we're doing fairly well there so far but when you start thinking about 16 000 Channel devices that won't get you all the way there so some other things that can help on the compression slider to actually just send out the output of the machine learning model rather than the input required to actually run it uh so one thing we've been trying in the background here is called decodon head which is essentially taking the machine learning models right now we're running on MacBooks that our monkeys are gaming on and moving those to actually run on the implant and this is like a super cool engineering problem if you want to talk about how to make uh complex neural networks run on what is the equivalent of a garage door opener come talk to me it's fun um yeah so that's another way to solve this problem is basically do the computationally Intensive work to just get the raw signal that you actually care to use to control something and then send that thing out of the of the implant on the radio side I'll hand it over to Matt yes so to answer your question we are looking at other radio Technologies um one in particular is uh 500 megahertz pan with ultra wideband at a couple different frequencies so this has an advantage in terms of the bit rate that you can achieve it's um on the order of six to eight to ten megabit uh there's also a latency Improvement that's quite substantial and there's also a another wireless technology that we're looking at at w band hi uh thank you all for really clear and compelling presentations um something that struck me in one of the earliest talks I think it might have been DJs was this vision for the ability to acquire new complex skills via these bcis um like the ability to perform Kung Fu and that reflects the fact that the brain is fundamentally a learning machine and yet the um many of the Technical Solutions presented later framed were framed in such a way as to try to correct for the way the brain changes over time over longer time skills drift over the course of days or um the way that the tissue might heal over time I was curious what your vision collectively was for developing out this technology that interfaces with a fundamentally plastic system that changes in complex ways over a variety of time skills days months years yeah it's a tough question yeah I think it will be kind of maybe bi-directional learning in some way in the sometimes skills that we will might fix our algorithms and we prefer to have like more stable kind of performance but of course if the over time the person in the brain will learn how to use better the bciable need to update our models so they will be kind of in the interactive kind of relationship in some way um to learn even new tasks this probably yeah will be something will over time will need to learn what the person kind of learn how to interact with the computer and then build the appropriate interface the ux and also the UI and build algorithms that will help him to control what we want just one thing I'd like to add on to what near said um yeah it actually is an advantage in some ways that the brain is plastic and learns and that can help us because we actually have to do less work in the the human in the loop we'll actually learn how to use our device better but one of the advantages of our particular approach in device is that we are trying to do an extremely high Channel count device so we can you know uniformly distribute electrodes over a functional region and then it doesn't matter so much whether things move or shift over time we can offload that to software and so we can build algorithms that change over time as well and so both those things are actually I think advantages to our particular approach we have another question from Twitter Juan wants to know what career path do you suggest for somebody that is just getting out of high school if they want to work at neuralink in the future well it's really any of the skills that we described uh so I mean we're developing uh new Chips uh you uh this material science uh you know as uh software obviously uh animal care because it's a really all the things that were listed in the in the neuralink slash careers okay that'd be a good guide I'm actually very fond of saying um you know when you flip through any college uh like booklets and look through all the majors I think you can point to every single one of those majors and there's someone at this company who either is an expert or you know have majored in that so it really is um truly truly multi-disciplinary Endeavor and I think you know just focus on whatever your um you know passionate about or whatever you're talented at and then just you know pursue that as deeply as you can and then there's definitely going to be a place for you in uh you'll end up building neural interfaces hey um we got to see the monkeys doing telepathy but could you say a little bit more about the animal behavioral training kind of their lives and day-to-day processes sure I'm Autumn I am head of Research Services which includes our Animal Care Program and as an animal welfare scientist this is a topic that I'm deeply interested in so um our training program is outfitted mostly with behavior analysts who help us think about how to remove any of the potential aversives or frustrations from our training um we think about uh conditioning as the primary which includes positive reinforcement as the primary way to train um let's see what else can I share with you yeah yeah I mean that may not be part of the behavioral training itself but we think of Animal Welfare assessment in the framework of the three R's which is refinement replacement and reduction and so when we think about refinement behavioral training does apply in in that way and where we want to remove specifically in research restraint is one of the things we make a very top goal to remove so um you saw a lot of videos today where animals were walking up to their stations because we worked really hard to remove any requirement to restrain the animal um anything else yeah I can just yeah well just on top of the last point you said um just as an engineer here one of the things that is really inspiring and really cool about this place is that we do get to work on a lot of technological innovations that directly translate to Greater Independence for the animals when they're engaging in these tasks so as you saw you know monkeys charge just by voluntarily walking up to a branch they play games in their home habitat with a laptop computer voluntarily and the fully implantable fully wireless device the inductive charger all these things enable that kind of experience and so this is one of the very cool Parts about working here is we do get to innovate on things like that definitely helps to work with a group of Engineers who can like really make cool stuff for for monkeys to be able to do easier behavioral training yeah so I guess to answer the previous question about what you can study to be part of neural link I guess monkey engineering you can add to add to that Monkey Business hello my question is on uh upgrade upgradability which you guys mentioned quite a bit so in that procedure I imagine there's some kind of expand procedure and then you're going to putting a new set of implants so could you talk about the damage possible if any tissue damage from the X-Men procedure how long you have to wait do you imply in the same areas and uh what's your like brain scanning for the implant procedure in terms of upgrading it um I don't know how many questions I can ask um so I can start to speak to some of those so I I work a lot on upgradability in those x-plant processes and uh designing those to be better um the the the goal that we're working towards is that as I mentioned in the presentation it's really just as easy to upgrade an implant as it is to initially install um we didn't we didn't show many of those explant uh examples today um but we've come pretty close to just popping out an implant and reinstalling another one in the exact same location definitely definitely the goal is we are installing the implant in primary motor cortex which is a valuable area for interacting with a device like this and so we the goal is to implant in the same location maybe if you expand out to other applications then you'd be interested in moving somewhere else but we definitely want to be able to insert into the same area um in terms of damage the I think that the damage that we care most about is uh damage within the brain and what we've found and we we talked about that that challenge of the tissue layer on top of the brain and where I think we're well on our way towards figuring that out um but because of the thread small size the the sort of Scar capsule within the brain is so minimal that they are actually removed quite easily and so we see useful signals even on the second or third time that you've placed an implant and I think some of our BCI folks can probably speak to that we we do have uh monkey participants working with their second devices and uh really making use of those so one two questions one was somebody that asked the question about the brain Bean plastic have you noticed any plasticity from a behavior perspective from any of the monkeys or is it too soon to tell or there haven't been any observations from the monkey Behavior we see that it takes them a while to learn how to of course to turn on the task but also when they are implanted and it's relatively quickly for them to ramp up and get to a high performance of brain control with pager for example after a few days he was able to like three days already able to learn very quickly to use the device it was trained on the task force from this previous implant but with the new one he was after three four days he was able to control to a close to Performance of he he held with the previous implant but have you noticed anything on the adverse side which means the brain has outpaced the neural network that you're running it's hard to say um no not really okay so I have a another question uh which is more about the electrical side so you talked about 1024 uh channels being recording are you transmitting the raw signal or was it only the three Spike events that you were talking about the low mid and the high or is it the raw entire raw firm waveform that you transmit yeah hi I'm Julian I can speak a bit about this and maybe Avinash wants to contribute but um our chips see they're all signals but then when we transmit out uh typically spikes and we detect those spikes in real time on the chip this massively compresses the data um I guess yeah we're making improvements to that but we can request we can request rule samples sometimes we also process uh particular statistics or other data directly on the chip and then send out the calculated values um so there are many ways to sort of play with the data yeah so at least with the current and one system that we have which relies on uh ble radio there is a bandwidth limitations so you can't actually stream raw data from all 1024 channels but just kind of to give you a little bit of a history of how our compression algorithm the spike detection algorithm was developed we did have sort of a wire system I there was a paper that we published with the USB C connector that you know streams all those signals through a hype bandwidth wire connection so we did have kind of those development platforms to be able to see the raw signals and know which set of information that we want to extract that are you know going to fit within the bandwidth of the radio as well as is useful for BCI control and you know also just sending data wirelessly does cause a lot of energy so there's any opportunities we have to reduce that burden uh you know we try to do basically have all that compression closer to where the electrodes are as possible maybe one thing that isn't obvious is that the the actual uh bit rate that you need to control a phone or a computer is actually very very low so I think we might have the record for bitrate is that correct we think we do uh maybe so on the order of 10 bits per second so that's super slow um but if you think like if when you're inputting data into a phone like how fast your thumb is moving how many thumbs what's your thumb Taps per second it's pretty pretty low and um I mean like basically our thumbs are like two slow-moving meat sticks that we you know do this and it's like this is really a load it's like a low bar that's what I'm saying um so for at least for output it's it's a you get 10 best per second you're you're holding ass uh so and that's you don't need your Bluetooth anything for you so you could practically send it out with beeps and Buffs you know so it's not if you go if you're going like a high bandwidth visual now you're you know maybe going to megabit plus but it's it's so it's really well within Bluetooth uh or but anyway it's just we're that is what I'm saying is that's not that's not a constraint that the data rate um one of the sort of like maybe a notable item which we talked about in the presentation but uh we we think we can probably solve for doing the implant without cutting the the dura we can just do basically a bunch of holes through the door which is like the dura is like the big thick iron dry D thing that contains the that's up against the skull if you don't pierce the dura you know if you don't cut the door away and instead you have a bunch of tiny holes and insert the electrodes through the tiny holes into the rain um and then the recovery time is ridiculously fast um you know you're not really losing much in the way of cerebral spinal fluid it's it's you couldn't Theory we I mean this could be like a the whole thing could be a 10 minute operation like LASIK like it's fast it's not like a big laborious thing it's super fast just going back to the long-term use I'm wondering if you have any pathology looking at scar tissue from many animals that have had long-term implants and along that lines it seems like there might be a little bit of a gap between use in medical conditions and helping individuals from a safety perspective so I didn't quite catch the last question but um I Heard the first one I'll ask you to repeat the the second one so the first one is do we have pathology from from long-term use animals we absolutely do um we don't have any pathology from our monkeys which we upgrade and you know are still going we have uh other studies that are primarily to determine safety and so we do have histopathologic endpoints that we we determine the scar tissue formation around the threads themselves in the brain is typically negligible like it barely reacts to the threats at all um so that that's very promising in terms of the scar tissue formation over the the cortex so this neo-membrane growth that fills in the areas that Elon and Alex were mentioning that we remove with our current operation um those we we do you know evaluate that scar tissue but it isn't uh it doesn't pose a problem in any way it's not a continuous reaction to a foreign body it's just filling in tissue that was removed and if you could repeat the uh the second question I didn't hear that yeah the second question really following up on that seems like there might be a little bit of a gap in use and healthy and individuals from a safety perspective you know I think people mentioned that they might be interested in trying prototypes but just wondering what your perspective is on trying to lower the safety risks yeah it's a great question so in terms of really it's about the long-term use of the device so you know we have devices that have been implanted like I said in monkeys where you know for many years where we see no behavioral deficit at all so this first is a question of how you evaluate safety so you have histopathologic endpoints you can evaluate but we're also looking for cognitive deficits or behavioral deficits as well and we don't see any of those in our animals which is you know an important point in terms of the histopathologic endpoints they look really really great the challenge is one of explanating the device which is why we're putting so much effort into the reversibility efforts and our through Dura insertions so when removing the device that's when you potentially could cause damage and so we are doing we have a lot of ongoing studies right now to really minimize the risk of that but we don't think it's a substantial risk with our current approach and like I said pager was upgraded with the previous surgical approach and is doing great so clearly it is you know can be perfectly safe but proving that beyond a shadow of a doubt for humans is something that we're still working to do rigorously to answer your question yeah thank you so uh thank you for a very deep dive on many of the different aspects of the device and the system it's very impressive to see all the engineering work that's gone into it um you just mentioned about bitrate as the prior bit rate holder um I can confirm you have indeed shattered my record so congratulations on I think I saw a peak of 7.4 bits per second well done um my question is actually around the clinical trials and the FDA to the extent that you can share I gather that that device removal or maybe electrode removal is one of the concerns that the FDA highlighted is there anything else you can tell us about what the FDA was concerned about or had questions about with respect to their ID submission yeah I mean we can probably talk a little bit I mean it's these are really challenges you know that we have broadly so uh explanation safety proving that right rigorously for humans is something that we it definitely is one Challenge and was something that that the FDA commented on um other things that they do ask some really great questions so other things involve uh you know things like the thermal bench shop testing of our implants so obviously it's important that our implant doesn't damage the tissue by overheating so having really rigorous and valid bench shop testing for that is very important it's actually Something That We're redesigned to be even more uh accurate now um it's also the case that you know they ask a lot of very hard questions on biocompatibility chemical characterization so we've done very rigorous testing for that but you know they they do ask a lot of questions about getting into the weeds of the data and making sure that there really is no chance for any toxic chemicals or bio incompatible materials to be in the brain so these are all things we're working with you know to uh to just prove again above and beyond uh Beyond a shot of a doubt one thing that's maybe worth mentioning here is that it can be difficult to appreciate the novelty of our product so the surgical robot and the thin film array in particular are quite new and unlike existing devices and this means that we can't rely heavily on literature to support the safety and efficacy of the device so we do spend a ton of effort in designing and Performing testing on our devices so that we can rigorously prove the safety of them and we can't rely just on another product or on some paper and that's something that we're not willing to compromise for our first human participant working very hard to do I think if you ask a question like um like in my opinion like what would I be comfortable in planning this in someone one of my kids or something like that if at this point like if if they're in a serious like let's say they um if they broke their neck would I feel comfortable right now doing it I would I would say we're at the point where I at least in my opinion it would not be dangerous um hey thank you for the presentation so I have a non-technical question um are you collaborating with people with motor disabilities and if so like have they shared any ideas of applications that they would be excited about production I can take the first part of this I'm not the best person to speak to this to be honest but there is a consumer Advisory Board we have made up of a number of people that have uh various conditions including tetraplegia and they give advice to us on a number of topics there's uh just as an antidote someone came to the office maybe six months ago and they were telling me what they most wanted to do with their neuralink device and there were two things that they said one was they wanted to be able to trade stocks day to day to be able to beat their brother and the second one was they want to be able to play shooter games so I think what was most shocking to me about that encounter was the normalcy of that and I found that conversation truly inspiring so you know who you are the person who came and talked with me um have a great day yeah yeah what something that's we've talked about but it's maybe I should be re-emphasized uh we are doing uh we're building up a production system for the devices so we're we're building up breaking out the production line making large numbers of devices we want to make thousands ultimately tens of thousands then millions of devices so the the progress at first particularly as it applies to humans will seem perhaps agonizingly slow but we're doing all of the things necessary to bring it to scale in parallel so Theory it should uh progress should be exponential okay so thank you that was a very cool presentation um so one of the stated goals was recording from everywhere in the brain being able to record from and perturb any location so it seems like currently it's all cortical um and I'm curious with the current device is it is there any sort of long-term goal or idea as to extending it into going deeper in the brain I mean for Neuropsychiatric disorders for memory all these things are much deeper several centimeters so I'm wondering what's the time scale if you were to give a very rough estimate of when I can expect to see a knurling product that goes that deep yeah so I mean the the fundamentals of the device in the skull will stay essentially the same because the I said earlier the the device in this call is very much like a like a smart watch essentially it's got It's a battery radio inductor charger uh computer and um and then you've got the the little wires and so you need to make the wires longer and you'd have to have a deeper insertion needle for the robot but it's this really is intended to be a generalized i o device so apart from the tiny wires being longer and the surgical robot needing a longer needle in theory you should be able to go anywhere because uh it seems to me that part of the robot is trying to detect where the blood vessels are and then avoid them correct would that be possible at that scale I mean certainly not just visually but maybe there's some other way of detecting it is that is that like is that a current goal and do you expect that within I suppose the next decade uh definitely yes I'm Ian I run the Robotics and surgery engineering team here um like of the three axes that DJ mentioned one of them is you know hack system Warriors of the brain so the robot team thinks about this a ton in terms of uh what sensors do you need to essentially go past the surface um and so in this case you're right that right now we can really only see down uh the maximum about a millimeter I think within the team there's questions of what's best to use next but like ultrasound and photo acoustic tomography are two that come to mind as things that can get centimeters deep essentially but it's a super interesting problem you sort of need Deep Imaging and some ability to steer to at least avoid large vasculature deep down yeah or if our if we can make our needles and threads small enough in a way that we can still be precise and accurate at a deep depth then maybe you don't cause a bleed if you hit a vessel yeah I think that's that's really the ideal situation if the threads are really tiny they can actually go through a blood vessel um and it's and it's okay if they're tiny enough so so we wouldn't need the blood vessel Imaging in that case I I'm I actually am slightly optimistic that that is achievable and Matt could probably speak more into this but what DBS currently is kind of just like send it yeah the current approach involves a wire that you blindly pass in that's massive compared to our threads orders of magnitude bigger um and so that's a low bar for us to clear as well because people don't realize like for the Deep bright simulation just how big the hole is it's uh I mean what is it like I mean how basically in current deep brain stimulation how much of a borehole is growled in the brain yeah you're drilling a 14 millimeter Burr hole and then passing a two millimeter wire um you know six centimeters eight centimeters deep into the brain so all blindly hoping that you don't hit a blood vessel telling the patient up front this might be good for you and there's a one percent chance you're gonna your brain is gonna bleed in a way we can't control so that is current technology that is happening right now so doing better than that is we can we can definitely do way better than that there's no problem our needle is 40 microns yeah thanks again for the phenomenal presentation I thought it was fascinating how rapidly you could test all of these electrodes but it begs the question about like what your fault tolerance is if you run these Diagnostics and it comes back that you have something that's either shorted or high Z how many of those before you get degraded performance the second question is uh when you're actually inserting this device we saw examples of the electrode going in and then like looping back on itself but it looked like that was something that was assessed basically by slicing the synthetic material I'm curious what you're doing to validate the insertion of all these electrodes sort of in Vivo how do we know that that's not happening on an actual patient yeah I can enter that SEC the second one um so like I mentioned we can um so we weren't actually sectioning in that case we have a really cool micro CT um so I mean it's essentially like a CT scanner so that's just an intact proxy that we put in this machine and we can you know take a picture all the way through it um and like I mentioned before like we can make a proxy where it happens you know that that looping back happens every single time and then we can make one where it never happens um and we've pinpointed roughly now where actual tissue Falls in there and so our current plan for you know validating and confirming that is making proxies where it you know happens really easily much worse case than any you know any tissue could possibly be and then designing it such that it never happens in that scenario and that'll give us the doing that enough times and with a weak enough proxy um that'll give us the confidence that this isn't actually happening yeah and yeah and this is the next geneal we don't see this problem at all with the the current generation and I'll take a stab at your first question so to clarify you asking what happens if there's a fault on a particular Channel or something yeah that's correct yeah so um the nominal scenario is that basically the impedance will stabilize pretty quickly within the brain and even at that level we can record great signals we see lots of spikes and we can use that for BCI um because we have so many channels like a thousand now 16 000 later uh we can actually run our models with far less channels than we actually have so it doesn't matter if like one channel dies here or there like we can still do really good decode I'm not sure if we have official numbers on how many channels we need but it's like we have an order of magnitude more and the more we have about it but we can already do a lot with what we have all right maybe just one or two more questions yeah I have a question about your um very very long-term uh inspiration to have this high bandwidth communication with Advanced AIS so it seems like you know the advanced AI would need to understand the humans most complex thoughts and emotions and that's what neuroscientists are trying to do so do you have any Ambitions to tackle Neuroscience Beyond neuro engineering well I mean I think we're going to make the input output device and the software interface with it and I think probably um you know post suggestion earlier we'll try to open source as much as possible so people can take a look at it and I think there will be a lot of others that that build upon the work that we're doing um you know the same way that if you make a if you make a microprocessor or CPU or computer that people will write lots of software that runs on that computer um so but if you don't have the computer this is the software's mood so we're making the input output device with the computer and um and then I think probably there there'll probably be a lot of other organizations companies that that built um that build upon that Foundation so yeah um I mean one of the things that's uh I would sometimes wonder is that if you do have a whole brain interface and you can record memories um and really getting into Black Mirror stuff here but um this could be one of them I I also think it's worth mentioning an important point which is that neuraling didn't come out of nothing uh there's decades and Decades of research in the medical academic field that has really set the foundation for what is possible by putting these electrodes in parts of the brain and being able to read those signals decode it for mapping it to some application and um you know being being in Academia before before coming to neurolink um you know the I do think that there's a lot of opportunities for kind of the field to advance at a much rapid rate by having just better tools for observing the Dynamics that are happening and then engaging with it in a seamless way and I think it was Ian who sort of mentioned that you know it's almost as if like we're kind of building an oscilloscope for the brain which I think is like kind of a beautiful analogy of just giving us a bit more abilities into peering into the Dynamics and using those information learned that to I don't know hopefully understand like what makes us and how the brain works and you know of the whole champagne hello the presentation covered keyboard and handwriting based input methods how do you plan to develop an input model that will achieve much higher bandwidths for complex tasks in humans yeah this is a tough question and we start to explore this with monkeys as you saw we have like a multiple we train many monkeys on very different tasks it's still an open question that we're after I think hopefully once we get to our first participant it will be easier to investigate one of the options we are exploring as we showed is to the code handwriting directly this is a one a work that started Stanford and we are exploring here and trying to expand there's also a different uh in addition to just decoding different things from the brain we also try to provide the user different uh maybe user like interfaces for example we show different type of keyboards maybe also swipe and other things that can help increase the communication rate so we are kind of tackling those in two dimensions yeah just one other thing to add in that direction uh as pointed out by many people here so far this is a general i o system that you can sort of plug and play in different places in the brain there's other areas of the brain that can help increase bandwidth for example language wrist speech centers that can help you much more seamlessly communicate for example text if that's your main thing that you're trying to do yeah just I think just having this General input output device will just so gigantically improve our understanding of the brain is is hard to the words can barely Express like you know uh right now we're just guessing a lot of what's going on in the brain but if you have direct i o it's not normal guessing what would learn about the what we will learn about the brain with such a device in in wide use is absolutely or many orders of magnitude more than we currently understand so um I guess on that on that note uh thank you for coming and thank you for watching online [Applause] [Music] thank you",
    "status": "success",
    "error": null
  },
  {
    "video_id": "CkUcCcRq_eM",
    "transcript": "hey everyone welcome back we'll now move into our last event of cute guy 2021 a presentation by siobhan zillas now siobhan is the project director at neurolink's office of the ceo and has worked with tesla and open ai she's a fellow at u of t's creative destruction lab and a founder and partner of bloomberg beta she will be speaking on the future of brain machine interfaces to connect humans and computers everyone please welcome siobhan hey guys um i love that intro walk on music i kind of want to take that everywhere i go um thank you guys for having me here today i as uh as max mentioned so i grew up in markham ontario uh you know definitely had not thought about a ai at all growing up and actually it's probably before your guys time but there's a band called our lady piece that did this album called the age of spiritual machines and that led me to read this ray kurzweil book on basically like human ai symbiosis and i think i was 12 or 13 at the time and just like could not get ai out of my head from then onwards um then ended up as as a canadian would you know played a lot of hockey ended up taking the sat ended up in the us and have basically spent pretty much all the last decade focused in and around ai unfolding in the world in the in the best way possible and so what that looked like for six years was investing in startups that were using ai for net good things for the world and also working with a bunch of non-profits um and one of those nonprofits was uh was open ai and that's how i ended up meeting uh elon ended up working spending two years working on just basically the suite of ai projects in the portfolio so that was uh ai trips at tesla self-driving neurolink uh and then open ai and i mean the thing i'm going to talk to you guys about today is neural language which has been like kind of the most fascinating thing i've and complicated but like also fascinating thing i've ever encountered in my life um which does fold into ai uh you know i unfolded in the best possible way in the world but more from an existential perspective so you know back when i was at bloomberg beta i was really focusing on like applications for good and as i thought about it more and more i was like well you know ai's going to be one of the most fundamentally transformative technologies humanity creates if not the most and so we just need to make sure you know from a humanity perspective this this goes well um and so let me hop over into my presentation what i'll do is there's gonna be just a ton of content here i'm gonna try and get through it in 25 30 minutes and then essentially give the rest of the time over to questions because i'm sure there will be some and just stop me at any time if anything goes squirrely or you can't hear me but here we go so just a quick quick primer on on neuralink in general um it's like why are we why are we even doing this in the first place so i think there's a there's both a short term and a long-term goal of gnarling the short-term goal is basically solve important brain and spine problems with a seamlessly implanted device and you know this is definitely easier said than done uh your brain is a very sensitive organ that has a whole bunch of things around it that historically just has never been messed with you know we've been doing invasive things in medicine for you know many hundreds of years but definitely not not dealing in the brain region because it's it's the trickiest task you can you can possibly choose for yourself so that's definitely the short-term goal and you know if you think about it every single component of your reality everything you've ever touched any memory you've ever had anything you've ever felt like all of this is just signals emanating from your brain and so when we talk about creating a platform to directly interface with the brain you have the potential just to modulate literally everything right because everything is just these signals and in terms of our experience in the world and so that's really the short-term goal and a lot of what i'm going to focus on today but the way this i mentioned to you neuralink does also fold into ai unfolding in the world in the best possible way and so our long-term reason for existence is something that's a little bit more science fiction oriented but like honestly if you look at enough enough years you're like yeah this is definitely going to be a reality so you know you know how do we how do we make this good for humanity and that's basically when the age of artificial super intelligence hits right so every single year artificial intelligence gets more and more and more intelligent of course now that's sort of within a constrained controllable fashion but if you look out 10 years if you look out 50 years if you look at 100 years there's no reason this technology doesn't get basically more and more intelligent and more and more powerful year on year and so if you compare contrast you know humans to artificial intelligence or just basically silicon-based intelligence i don't necessarily know that we'll consider it artificial forever um but one of the things that we have uh is you know we've got a lot going on in our own heads but the actual communication bandwidth between humans and between humans and computers is actually relatively low right so i'm i'm showing you some images i'm saying some words the bitrate on that communication is is relatively low compared with what computers can do with each other or even to some degree what uh computers would be able to express to us if if we had some sort of higher bandwidth opportunity so um you know this is one of these things that again it sounds way out there but you know as superintelligence hits the world one of the things we kind of want to exist is just the optionality to have a high bandwidth interface with it so we just keep high communications and you know basically like allows humanity to go along for the ride and so you know honestly the probability of our probability of us being able to do this successfully in the time frame allowed started at zero percent and so you know we literally exist to to raise that probability above zero percent um we've been working really hard for the past four years i think we've gotten the probability up to you know generously one to two percent and we just we just hope to keep um elevating it from there and so just to step back for a hot second you know like what are we talking about in the first place so your brain is an electrochemical system you've got a lot of neurons you've got billions of neurons in your brain they're connected to each other through synapses so um neurons have variable number of connections with other neurons but a lot of what's happening in your brain is just basically a fundamental wiring and rewiring of these synaptic connections through time and because the brain has electrical properties one thing we can do is uh essentially put an electrode in the brain that allows it to hear electrical signals from nearby neurons and so that's essentially what we're trying to do and you know one one question that often comes up is hey you know like why do you have to go inside of the brain for this and there are a lot of things happening in terms of reading brain signals externally but because our end goal we have we have two goals one one is to just really fundamentally help people with debilitating brain disorders and the other is to get to that high bandwidth interface and just basically the physics of how this all works doesn't allow us to get to significant enough resolution on either a read or right dimension in terms of a brain machine interface and so um it would be our preference not to have to go inside the brain but it's just per our current understanding of physics impossible to do it from outside and so we're not the first ones to want to do things with the brain so i'm just going to quickly walk you through a couple of things that that exists today uh in terms of best in class for uh medical research the most popular device used to interface with the brain directly is called a utah array this essentially has a 100 silicon shanks attached to a base plate so if you look at that image on the bottom right side um these are again the you've got two millimeters per scale there so you've got these relatively teeny tiny but still large silicon shanks you kind of you rest that thing on top of the brain you use a pneumatic hammer to like put it inside the brain and end off you wrote but um a few things are are definitely constrained here i mean one is the fact that this has existed for a couple of decades it hasn't really progressed um and again one of the goals of this was to just understand the brain a little bit better the goal was never to take this to humans in a way that they could actually use uh to either enhance themselves or to solve any any particular just brain ailment that that could be flagging them and the other the other tricky things about this which again what we look at ourselves is essentially trying to move beyond the utah array as our first step is if you if you take a look here you've got these these ports that come out of the brain and anytime you have a percutaneous port that gives sort of like a very high risk of infection because you have this constantly open thing and so from the get-go for neural link it was just not an option to have anything percutaneous which from an engineering perspective um inserts a whole bunch of challenges but they're ones that we're pretty excited to solve um and then one other thing that we'll get into a little bit is just the fact that you know if you think about your brain people use different things like kind of like what overcooked wet spaghetti or yogurt but your brain is this like fundamentally soft medium that's kind of pulsing up and down and when we think about brain machine interfaces we want something that's going to last on the order of decades before you have to to upgrade it and one material property of these silicon shanks in the utahrey is that they're extraordinarily stiff and so when you basically have a stiff medium and a pulsating soft medium you're you're basically causing chronic damage to the brain through time because you kind of have a sword in a squishy thing that's that's moving over and over again so you're creating additional tissue damage so that's what's happening on the research side uh in terms of what's happening that's clinically available there are a few devices that again i think will look back in 10 years and consider these relatively medieval which is good it means humanity will be progressing but uh they actually these fundamentally simple and invasive and actually relatively dangerous devices actually do a lot of good already um so there basically is precedent to show that when you are able to safely put something in the brain you can really really help people and so i've got just one example here which is a deep brain stimulator unlike the thing i just showed you which was a fundamentally read-based device so i was trying to read neural signals what this is trying to do is trying to inject current to essentially regulate certain areas of the brain that are just misfiring in certain ways so in this case this is a deep brain stimulator you can either do it unilaterally or bilaterally this image shows a bilateral insertion you put these uh basically these stimulating electrode channels deep in the deep in the brain region that's malfunctioning and you just apply stimulation current to regulate that area one of the downsides of this to date is that again because it is a bit dangerous it's not always effective only people who have very very debilitating conditions that have absolutely no other recourse are eligible to get it so an example would be somebody who has just uncontrolled parkinson's tremors and something like stage four parkinson's would be eligible but again it's there's a very high bar for for putting this stuff to use [Music] and so that takes us to what neural link is building this is a this is a a snapshot of what the device looks like today um i'm going to call attention to a couple of things and just kind of walk you through how we got here and so just as a just as a fundamental backdrop here i've i've worked with probably a couple hundred startups in my life and both the blessing and the curse of gnarling is that i've just never seen so many different dimensions that have to come together to make a single device work right so you know in our case if you take a look at those threads uh i'm going to just hop forward one slide it's a little bit difficult to tell here but what you're seeing is 64 individual threads each of those threads has 16 working electrodes on it and those connect back up into the package and so if you were to pull up pull off one of these individual threads and look at it you'd have to you would barely be able to see it and if if you weren't holding on to it very close it's just this teeny wisp of hair it's about the third the width of a human hair that would just float off and you'd never be able to find it again so we're trying to build something that small something flexible something that just kind of hides inside the brain and moves with the brain brain's material properties and so we've ended up having to lean on a lot of micro fabrication experts to basically build this thin film electrode array technology um again in contrast to those silicon shanks we were just like this was it was completely a non-starter for us to to do anything that would cause chronic injury to the brain and so that's definitely been kind of several years of work of neurolink to be able to get those electrodes thin flexible relatively invisible to the brain um and uh and then obviously be able to integrate that with a broader electronics package so where we've gotten basically the a lot of what we've done in in the last year or so uh so we've been refining everything but one fundamental architecture change that uh you know elon looked at and insisted on is our original instantiation for this the simplest thing we could have possibly thought of to make this work uh it was was the image you see here and so you see basically uh you've got these four little divots that go in the skull there and that was just the thread and um the the chip that did the analog to digital conversion of the signals from the brain um routing down to basically a wireless data transfer and then we had a battery that set on the outside so we would power this with a swappable battery um elon decided he was just like look you know guys we got to simplify and we got to make this completely invisible um because he's like i just you know some somebody who has somebody who's going about life through this it should people should not be able to tell the difference it needs to be completely invisible and so we ended up swapping to this one architecture which is is very clean um but basically if you think about what we have to solve here we basically have the threads that are coming off underneath that that implant we have the implant itself which has to house so we've got 1024 channels per device so you basically have to be able to convert all the signals there get all of the data off the head and provide enough battery life that this is deeply functional and useful to somebody that that needs to use it and so here's kind of the the implant side of it which is kind of half half of what neural link is focused on from it from an engineering perspective so again 10 24 channels what we do is we we needed to shrink it to his size both in terms of width and height that basically um fills the cavity of the skull that that we removed to insert it right so it ends up just being another piece of your skull at the end of the day and then what we did is create an inductive uh charging system so you basically you know right now we have it like this we are working on animal models and we charge just with this little puck uh that's able to charge the implant uh through the through the skin without raising temperature causing any other biological damage and then what we'll do as we move to humans is uh i i'm pretty sure what it's going to end up looking like and i can use a canadian word which is making me really happy you probably have like a a charging toque um i know here we have to call it bini which is just completely infuriating to me so you basically have a charging hat that would allow you to charge it when you're not using it or continue to charge it while you are using it um so that's the one half that's the actual implant if you move over to how do you actually get it there's the surgery and so um you know we're looking to move to our first human patients in the next say six to nine months we're actually going through our initial fda process right now and our eventual goal is to make this as easy and simple as lasik the first surgeries will not be that um but that that is the end goal and in order to do that one of the things so we spend a lot of time and energy on robotics at neurolink link one of the cool things about that is you know i i've had a lot of friends in robotics over the years and robots are just cool like they just absolutely are awesome they're fascinating but a lot of people sometimes build robots because they're they're fun and they're cool but they're not necessary in the case of neuralink the robot is just completely necessary for for two reasons um and reason one is i talked to you guys a little bit about the the threads that we have to insert um the it basically kind of operates like how a sewing machine would work so there's a there's a little loop at the end of the thread and what our robot does is uh and i'll show you a video of it in just a second it grabs each of those 64 threads and inserts them into a very specific area of the brain and you could be the best human neurosurgeon in the world you have absolutely no shot of doing this because you just need micron precision right and a human hand just cannot possibly um can't possibly do that from a motor control perspective in the first place but in the second place there's just no way you'd be able to do it fast enough to be able to grab all of those threads and put them very precisely in the brain um and so we we absolutely just fundamentally need a robot like when we devise this device we're like well we can't even think of this device without thinking of a robot surgeon and so um just as early as we were working on the implant we were working on the the robotic surgeon to actually be able to implant the device and so the rough outline of what happens in the surgery is again you have to pull back a skin flap you have to take out a portion of the skull that's done by a human right now and then you have a surgical robot come in implant all of the different electrodes and then you have the human kind of close up and again nothing percutaneous patient looks completely normal and now let me introduce you to the robot so as you can see the robot has a massive base and one of the reasons there are a few reasons why the robot's so big so one again micron precision we need a bit of weight there just to ensure that we're not wobbling all over the place um thing two is we just have an insane amount of just optical technology again it's it's mostly hidden by that shroud there uh but we have a whole bunch of different wavelengths of just passive optics and then we have something called optical coherence tomography with and we actually have a the highest resolution optical coherence tomography setup in the world and what that allows us to do is compute a 3d map of the brain because not only do we have to find a way to just insert around different blood vessels but the brain is constantly pulsating and if you want to get within a few micron precision with each of these insertions we have to be able to very um very accurately track the brain in 3d space and so i think next slide is slight gruesome alert i'm very normalized to this but for anyone that hasn't seen brains before i'm gonna i'm just gonna let this play a couple of times because it kind of shows one of the reasons why it's just absolutely critical to have a robot um you can see the vasculature on the top of the brain and so if you think of something like a utah array that just had that you know that broad bucket of spikes when they put that in they're just puncturing a bunch of these blood vessels and that's terrible for a number of reasons one you know uh it it creates an acute and chronic immune response when there's just enough um blood that uh let me just play this puppy again there we go uh you don't want to create that immune response what that ends up doing is your brain sends a whole just a whole bunch of um things like astrocytes and glia that just glom onto the electrodes um and basically tag them as a foreign body and they're like no no you're not doing this we're going to shove you out of the brain and we're just going to coat you so you can't hurt us anymore so you obviously can't get brain signals when your electrodes are coated and beyond that sometimes if you do puncture one of these vessels i mean the blood vessels are there to oxygenate the cells and so you're just going to knock out an entire column of the neurons that you're trying to record from um and so for us this surgical robot and again i showed it to you in one quick slide here this has been this has been many years of work to get it to be this um precise to be able to grab uh to grab the thread and insert it in the brain around the vasculature um and so one kind of kind of interesting and funny thing about this this whole situation is you know you've got that robot that's just absolutely massive the only part that interacts with the brain is that little needle which starts its life as a 40 micron wide a tungsten radium piece of wire and then we have a femtosecond later that sorry femtosecond laser that reductively manufactures it um with about eight different cuts into this very precise geometry that basically grabs the thread and then is able to look at the brain insert it pierce through the meninges on top of the brain without causing damage insert that thread and then retract and do that 64 times in a surgery and so that's all of the uh the the implant and robotic side of things to get this in the brain but once we get in the brain the question is does it work and so i'm gonna go over just a couple of things we've done um to date so in august of last year we ended up sharing some of our first large animal results um and so just to give you guys a backdrop of of how this works you know our our preference of course would be if we didn't have to use animal models the reality of the situation is is the fda and just in general you have to test these things on non-humans before you move into humans and the way we end up going about that is uh and our whole company has this ethos anything that can possibly ever be tested outside of an animal animal model has to be tested outside of an animal model and so we've invested a lot of time and energy in doing things like hardware and loop testing a lot of visual inspection on the devices and we basically have a whole bunch of chemical tests as well that we subject the implants to once you've done that it's time to test in animal models and so we are primarily using two models right now for the the human grade implant and so the first one of those is pigs and what we use pigs for is is basically anything we can do that doesn't require extremely rich behavior uh and so here's an example of just actual brain signals coming out of a pig so this is a sound of thought this is the sound of reality um the way our brain communicates is in action potentials and what we basically do i'll let me hop back to that other slide even though it's a little bit loud um because we have to have everything on a head what we've done is is basically so the the models for reading things reading signals from motor cortex and actually being able to map them to useful behavior are not overwhelmingly complex so what we've tried to do is basically shrink the footprint of the chip and make it as strength the analog pixel and make it as less power-hungry as we possibly can and then what we're basically doing on head is we're template matching to the shape that you see on the right in in the blue which is the shape of an action potential so every time your your neuron wants to fire it depolarizes and repolarizes and and gives you kind of a signature shape which we then identify on a template that we have on head and we're essentially shipping just all of the spike data that we get and the temporal signatures associated with that so that's that's what we're shipping off head to then run our models uh and here's just a raster plot of what this ends up looking like across 1024 channels in this particular case we had so if you think about a pig and it's like what does a pig spend most of its life doing it spends most of its life kind of rooting around with its snout both from a tactile and scent perspective and so what you're basically seeing here is every time you get one of these hills we're stroking the pig's nose um and for anyone who has seen our august event from last year we uh we had this kind of rigged up to a musical setup so you basically were playing the big nose like a musical instrument just to just show the efficacy of the brain signal uh we also did just a basic uh motor fitting model so we had piggies are very food motivated and they're huge sweethearts so you can see this this piggy on a on a treadmill getting some snacks um she's walking and essentially we're basically fitting a model to uh the emotions that she's making as you can see predicts quite close to reality again this is with a single 24 channel device in one hemisphere of the brain tracking one half of the pig's body and so that's kind of as far as you can push on sort of like pig behavior you can't really really really train a pig to do richer behaviors um and so that's what we've done with pig model so using that both to just prove that we are reading useful signals from the brain and also to just show the pig is our main model to show that there are no chronic health effects uh that happen as a result of of our implant and so that's the animal model we're using in our fda study everything from the get-go that neurolink has created is also read write compatible i think some of our first applications and humans which i'll get into in just a moment are going to be read heavy just because that's the thing that's going to be more useful in cortex-based applications that we're focusing on right now but we're we've also just run the set of experiments we need to to ensure that we are effective right effectively writing information to the brain and so the way we do this is uh this is done in primarily mouse models just for for debugging what you do is you end up inserting a fluorescent virus into a brain uh and stimulating in a way that when the neuron fires uh you basically can see just light around it so we've got two threads in here which you can see in red we're stimulating each of those threads and you can see the neurons associated with or just basically proximate to that thread end up end up firing so we're writing information into the brain when you zoom out this is just essentially what you see these are just different vertical slices of the brain but you're seeing that when we stimulate there is just gross neuronal activity in that region and so one of the things we can do over time is see basically how little current we we can inject to enact the change that we want to see but just like a computer these interfaces are are going to be read write compatible so where are we right now um we're focusing on a couple of things the thing that is most important to us is safely getting a useful device to humans in need as quickly as possible uh what we're focusing on out of the gate is helping quadriplegics get digital freedoms back right so anybody who just has completely lost movement um south of their neck uh what they'll be able to do is essentially use a computer in the same way with the same fidelity that you or i could um and again i think that the first patient will be able to do with some fidelity but there's no reason that we couldn't get to essentially again the same bandwidth that you are you or i get and i think the thing that i'm excited about is you know getting it to the level where you know somebody who you know used to love love video games for example uh unfortunately you know became quadriplegic can just like whoop my ass at their favorite video game um and that is of course you know a little bit a little bit of a more fun application than others but uh for somebody to be able to actually contribute back to society by doing meaningful work on their laptops being able to stay socially connected to loved ones that's the place that we're starting and so in order to ensure that all of our systems are up to speed by the time we get to first humans this is where we transition over to monkey research which we again very low numbers of monkeys these monkeys are treated super well they're just like huge sweethearts and what we're doing uh primarily right now is we're inserting our devices into both hemispheres focusing on the motor cortex and we're having the monkeys do specific tasks and so elon had mentioned in a clubhouse inter i guess club has interview presentation whatever you call it the cool kids are calling it these days but he had um shared just at a high level one thing that we have been working on which is monkeys playing pong with their brains um i won't share in too much detail right now because we're going to have something coming out in the in the next couple of weeks but the i mean it's pretty cool monkeys are playing pong with their brains um and so primarily we worked on motor cortex one thing we were also putting a bit of effort into and is also inserting these devices into visual cortex with the thought that our second or maybe third application in humans will be just visually reconstructing um images for people who are blind and so what that looks like right now is essentially using the devices to read the information that's coming in uh through through the monkey's eyes and uh basically recomputing that on on a screen so you can at least see kind of course imagery uh and our goal is to improve the resolution of that over time and so yeah i just talked a little bit about you know this this final monkey demo is kind of functionally the last step on the path to humans um we have them working on tasks like pong but we also have them working on tasks that are extremely similar to what a human is going to want to use out of the gate and so this is just a render of what the neuralink app will look like so there's a calibration phase where you know you essentially ask the patient to think move to the right and as it moves to the right it trains the model uh and then once you figure out right left up down what you can do is is find our granularity and basically move in a circle like you saw here so monkeys are currently performing that task as well and they're doing quite well at it and then once you get beyond that you can see you could very get a very rudimentary keyboard out there that would basically allow a human just basically type out whatever they're they're typing out in this case you are using just basically sort of like 2d control with a click as we move further down the line uh their existence proof to show that we might be able to shortcut a whole bunch of different things like in the same way that you have that little swipe function in your phone instead of just like tapping buttons and you have autocomplete you could do things like that or you could even tap into language areas of the brain and transcribe just basically full words or full thoughts based on neural patterns so it's it's the future's gonna get super super interesting um but so that's that's really what we've been working on right now we're just super heads down trying to see how far we can push the models in advance of getting to first human and as i mentioned we're going through our first series of fda studies um just to essentially prove that this is going to be safe and efficacious for first human and one thing i'll close on just before questions is just that like the vast array of talent types that we need at the company um one of the things that's been super exciting for me is just one i get really excited by working with brilliant and good-hearted people who are excellent at things that are adjacent to to what i know right and so getting to work with the best material scientists roboticists neurosurgeons um and we have a whole bunch of just like fundamental scientists too that are dealing with the more biological side of things um working with the best animal animal care specialists um and then again the best folks in terms of like actually building models that understand like neural networks for our neural networks like right now it's mostly linear models just because we're working in motor cortex but um it's going to get meta pretty soon neural networks on neural networks and it's just been anyway it's been it's been a super fun experience i'm i'm stoked to be able to share a little bit with you guys and happy to answer any questions you might have thanks so much siobhan that was that was such an amazing presentation so i'll throw up some questions here now and then okay so just to start off with a more general one so by your estimate approximately how long do you think it'll be until healthy humans can start getting the neural implant it's a good question i think part of this is predicated on the what so a lot of what i showed you guys is um what we've been focusing on out of the gate is cortex-based application so your cortex is obviously like at the top is quite accessible once you make that craniotomy you're just basically inserting directly into the brain region that that you want to read and write to um for cortex-based applications again i mentioned quadriplegics were the were the first set of patients that we were we were going to serve in part because there's just huge need there and in part also because there's no loss like the i mean if you're quadriplegic you're not using your motor cortex to move your body right so we're inserting into unused brain just to give upside um where we move beyond that i think there are a bunch of different paths we could take so that could get so high bandwidth that you or i may eventually want it right just in terms of a direct interface with our computers i think that's less probable um for first applications for healthy humans another thing that we've been we've been thinking about and talking about a lot is a lot of a lot of the most paralyzing ailments um for humans tend to be in deeper brain regions so you know ptsd anxiety obesity um there are a lot of things that that just go get again like extreme depression to the point where you're suicidal like some of that stuff just doesn't have kind of chemical remedies um and things like insomnia for example so if you're able to access deep brain regions you can treat a whole bunch of other ailments but that also may be the shorter path to translating into something that would uh help a healthy human um like as an example for me i'm like well i don't have insomnia but like you know sometimes getting to sleep at night is a little bit tricky if you had something that was truly at the invasiveness of lasik that could help you get to bed and help you get like a full night of rest i mean you'd be kind of crazy not to get it right and so that happens to be the thing that i'm i'm particularly interested in just in terms of the first thing i would want as a relatively healthy human a lot of people have different answers to that question so if you were looking for something that again just unequivocally is going to going to take you to better control like of your computers and your digital devices and be your default way of interacting you're probably looking at 12 to 15 years um again pending fda cycles and various other things like that but if you're looking for something i have a i have a sneaking hypothesis that some of the deeper brain implants will be desired by relatively healthy people on a shorter time scale um but again on the earliest you're looking at seven to ten years plus whatever additional regulatory cycle was required to make it just like universally available but i think the existence proof will exist in seven to ten years yeah wow that's that's pretty soon honestly in the grand scheme of things but yeah it's kind of wild i like neurolink we think in months and people are like oh what's six months out we're like it's two weeks like i don't know yeah well the ambition that's definitely getting the job done um but just to follow up to that so have you guys actually kind of delved into some of the emotion kind of like the deeper brain stuff that you were talking about or is that something that's just not too high of a priority a little bit farther off um so one thing i think i didn't probably explain clearly enough is there there are a lot of folks within the neuroscience community in research that are targeting very specific things one of the things that we've been focused on is just there's literally a chicken and egg problem right where like there is no fundamental brain platform that exists and so both our understanding and our ability to treat are just are just hamstrung right now because humanity just does not get how the brain works on mass um and so for us a lot of people be like hey you know you should should we partner up on things or we do this and we're like guys we can't wait like as soon as we have this like stable brain platform that we can you know allow others to use or use ourselves for a fundamental understanding of the brain like that would be sweet we're like focused on the engineering and the biology of it right now um and again we are learning things about the brain but it's just like it's so early days compared to what things will look like in five to ten years so understanding that underpinnings of like what fundamentally governs more complex emotions like we're just not there yet like i don't we're like a little bit there is humanity by virtue of some fmri and related experiments but like it's honestly it's been very humbling because the brain is the thing i think humanity understands least out of like i think for example like the ai we've created is way way in excess of how much we understand how much our own brains work right it's just it's just super early yeah for sure um okay so in terms of like actually implementing the the neuralink device like how do you you sort of touch on this a bit but if you go into like a little bit more detail about mediating the immune response from the electrodes yeah and so we talked about two things and the two things we talked about were just as much as possible matching the material property of the brain and doing everything we can do to prevent um an immune response that would be caused by bleeding right because that is one of the major signals to the brain responses like is is blood in a place where it's not supposed to be so those are two of the things that we focus on another thing that we worked on and again i mentioned we have some more fundamental um uh biological talent at neuralink uh you can do things like co coat the electrodes um in in different proteins and just basically different compounds that do also mitigate immune response um and you can also just do the very simple things like uh administer antibiotics and various other things uh at time of surgery but it's basically those those are the four mechanisms that we have to to reduce overall immune response over time um the histology on in our animals right now looks looks quite good and we wouldn't be working to take it to humans if the histology did not look good um and honestly i don't think we have pushed as hard as we can yet on the third thing i mentioned which is just again biological coatings that mitigate immune response but both the teeny tiny size the material property and the fact that we avoid all vasculature and move with the brain once it's in has actually we've we've been really pleasant i don't say pleasantly surprised like this was the design goal from the beginning um but the immune response is relatively manual minimal okay great um so i'm just picking we have we got tons of great great questions for you so um it's gonna go with the curveball yeah so um this one definitely touches on our theme a little bit more about what protective measures are being put in place to avoid hacking this device yeah this is one of those things we've got a couple guys at the company who are um just excellent within this domain so one one thing that's been funny going through neurolink has just been we have so many brilliant people um and sometimes it's not their time yet right like as an example um we've got a guy named joey who runs neural signals who's been at the company for three and a half years um but a lot of the early life of the company was just you know getting to a stable device that was high channel count and now we're actually implanting that device in monkeys and he's like you know the the greek god of monkey signals kind of situation and so his time has now come at neural language like yes we're doing awesome stuff with like complex brain signals like great great it's my time and so we actually do have a few folks at neurolink that joined um you know they're great across a whole bunch of things within like ee network spectrum but um they're really interested in working on the problem of like how how do you ensure this device does not get hacked and like you're thinking about it this is kind of the most um of an iot device this is the one that like if you hack it it's closest to home um we are there there are definitely a whole bunch of things on on the uh the firmware software side that uh we're doing to ensure it doesn't get hacked the reality of the situation is our attack surface doesn't open for at least another few years um and so it's just one of those problems that's going to be very very important but it's kind of like again like putting the cart before the horse like we need a useful device that's going to go to humans um kind of it's just a problem that we will put an extreme amount of energy in starting six to 12 months but we have a minimum viable solution now that again we've we've pre-cleared with uh you know how other medical device companies have dealt with similar types of implants um but again like if there are millions of people that millions of people that now have neural links uh you just need to ensure this thing is like iron clad but it's just it's just been too early to expend extreme resources on it since it's just it's like it's not in a person yet yeah um okay so here's another one so we have uh what are the concerns about needing to detach the technology to prevent over-stimulation [Music] god i got it got it okay yeah sorry the second half of the question um is super important too so um in terms of detaching to prevent overstimulation i mean this this device just obviously can be shut off at any time um a lot of the applications that we're talking about out of the gate are primarily read only and so you're not really you're not really talking about injecting current and if you're talking about injecting current it's very small amounts of current um and i think this is something that we're going to have to learn slowly through time if i look at the second half of this question so i'm talking increasing the bandwidth there are just a lot of things that we don't know yet right um like if if you could write more information to the brain what form does that take um basically like how much of it is actually usable i think one one hypothesis for how this ends up working is there are going to be things that are richer and more information dense than traditional language so right now like i have these complex concepts that i'm trying to communicate to you the most efficient thing humans have is like we created the technology that is language right and language works in written format language works in verbal trans and uh transference format um but i think one one strong hypothesis is that there's going to be an eclipsing technology to language once you have brain to brain communication it's unclear exactly how that's going to work um and obviously if you were just like hardcore stimming a bunch of portions of the brain what does that lead to that leads to seizure right and so you can't have that um so i i think one of the things that's going to happen in addition to the ability to again electrically right to the brain is to just have more core fundamental technologies for how information is transferred but again you can shut it off at any point and again it's just so easy to do things like one one beautiful example of an invasive brain device today is epilepsy monitoring systems right and they're both read write i think it's the only example of a functional read write device but the thing that they're trying to read is just gross activity in the brain which is a seizure right um and so one thing you can do with these devices is you are reading all the time right if you get any inkling that there might be some semblance of like runaway over simulation you just stop or you apply corrective force and so um preventing things like seizures is i think relatively trivial um figuring out how to like cram more information into you know every signal is again just an unsolved problem okay um so i think this may be one of our last questions but it's a little bit more technical but so how useful or transferable is research that's been done on non-invasive vcis those that use eeg to the work being done with neuralink um honestly almost not at all and that's not to say that those things aren't useful like they they are useful for understanding certain fundamental things about about the brain on mass but it's just like i think the best analogy that i've heard for explaining what the difference is is um like imagine a football stadium and like what kind of information you would get out of a football stadium depending on where you are and so if you are in a goodyear blimp outside you can hear when the crowd is cheering so like you can tell did something massive happen in the game or not right so you get that level of granularity now let's say you're way closer and you're basically in the nosebleeds and you're standing in the back you can hear a little bit more you can hear what some of the people in the crowd are saying but you you can't hear like what the coach is saying what the players are saying on the field right and so i think the difference here is that you know an eeg ends up being closer to that goodyear blimp because you can you can end up hearing what's happened like you get certain gross waves um but again that's like an aggregation of you know many millions of neurons right and then you can get closer you can do something called um eco which is you put like a sticker on the surface of the brain and so there you can get to find a grain resolution like a million neurons right um and then once you go inside the brain you can actually just get to single neuron resolution which is you can hear what the coach is saying you can hear what the player is saying and so i i would kind of just use that analogy to show you know what we've basically learned eegs and honestly things like fmri fall into kind of the same category you get a finer grain resolution of what's happening within certain brain regions on a course level but is that going to help you actually create a decoding model for doing 2d cursor movement just like it's just impossible okay great um so you've touched on a lot of these questions now but and i'm trying to understand what this one is um here i'll throw this up so can the device tell the difference between an abstract notion of like a left and that means left thought versus more concrete thought i mean you could you could do it both ways right so the way this is going to work um is there's going to be account so there's going to be just like a course calibration phase like say say i got the implant and i'm trying to use it for the first time the way this is going to work is i'm going to see that little cursor and it's going to be like the most efficient way is to say think of moving your hand to the right right and so if you think about moving your hand to the right or sorry we're going to the left in this particular case um what it's going to do is when i successfully do that like i might it might start doing this at first and then as i kind of like figure out what's going on his model figures out what's going on it's going to go to the left um and so in in that particular case especially since people understand motor left right that method is usually used for the calibration phase right which is just like think about moving that way you could also train it on literally anything like you could say move right when you think of a canary right and you could train the model to do that um obviously or the abstract concept of left you could do that um obviously that's a little bit trickier it's a little less linear for actually like getting to your result um because you know what happens when i want to go on a 45 degree angle right if i'm thinking of half canary and half hawk that's a little bit weird but if i have like trained myself to move right or to move up um i just end up getting a better result but like the point the broader point being you can basically like you can you can match whatever two concepts you want if you do that in a calibration phase okay so i think this will be our last one but um yeah sorry this has been a super engaging q a session and you've really provided some amazing responses so um what would you say your goal is looking for very far into the future as unrealistic as a theme i mean honestly the thing i care most about i just want ai to unfold in the world in the best way possible i mentioned shorter term that ends up being you know we're going to create this technology anyway how do we double down on using it for net good one reality and this is something i couldn't even talk about three four five years ago is is the notion of super intelligence it's like some people think this could be incredibly dystopian they think it could be the end of humanity some people think this could be incredibly utopian right and so i think one of the things that's really really hard for us to see and predict is what set of actions can we take to just like maximize the probability that we get that good future right we get i mean utopia is always tricky but like a humanity like basically a future where humanity's actually flourishing right um and so one of the things and i think for whatever reason naturally growing up i was always looking many years into the future is what can we do now and it's actually really tricky to understand what we can do now i mean neurolink is one of i think you know maybe 50 actions we could take right now but just basically maximize the probability that um agi and humanity kind of can coexist in in good possible ways i i mentioned i also work with open ai um and one of the goals there is to ensure that uh as this technology is developed it's developed in a way that includes many parties it's not just going to be one person who has a runaway advantage with this kind of like really really big and important technology that gets to decide what to do with it and so i think you know fundamentally i have a few goals one is that you know humanity as a species gets to participate and decisions made about ai agi as it unfolds in the world and we have a bunch of democratizing forces um that allow for that participation i think two is again like one of the things we're working on at neurolink is basically ensuring we get to go along for the ride like we've got we're literally creating a species um i think even if we didn't want to create it the the fundamental nature of this is that what is ai it's compute it's algorithms this data right like someone's going to make it from a game theory perspective um and so having mechanisms like uh you know a high band with brain machine interface that allow us to stay very close to this thing i think is kind of just super amazing and important and i also think just from like a humanity perspective you always want just diversification of um continued existence um and essentially enough inspiration to get you through the day so it's like you know some of the stuff i work on is like how do we prevent bad things from happening but then some of the other stuff is just like hey like it's so much cooler to wake up when humanity's trying to solve a whole bunch of inspiring problems and so whether or not you're working on climate change making like multi-planetary um or you know just like future of like ai created or human created art like we just need good reasons to to wake up every day even even in a scenario where you know kind of technology has taken just a different direction so um i think my goals as i as you've kind of seen from this answer are like somewhat diffuse but there's just like a whole array of things where you're just like oh man we can like increase the probability the future's gonna be like awesome um and so i just basically try and divide my time and hours on as many of those things as i can like productively help [Music] yeah great for sure i mean ai definitely has the power to increase the quality of life across all avenues for sure so uh i'd love for just another huge thing and i can say that it's a crazy revolutionary innovative company it's just monitoring how far ai's come to actually be able to implement such a precise interface with complex of our bodies it's just amazing [Music] so a great way to cap off the conference",
    "status": "success",
    "error": null
  },
  {
    "video_id": "Kbk9BiPhm7o",
    "transcript": "",
    "status": "error",
    "error": "No transcript available"
  },
  {
    "video_id": "r-vbh3t7WVI",
    "transcript": "it's called neurotransmitters neurotransmitters are released from the end of an axon in response to an electrical spike called an action potential when a cell receives enough of the right kind of neurotransmitter input a chain reaction is triggered that causes an action potential to fire and the neuron to in turn relay messages to its own downstream synapses action potentials produce an electric field that spreads from the neuron and can be detected by placing electrodes nearby allowing recording of the information represented by a neuron [Applause] [Music] [Applause] [Music] [Music] [Music] hello everybody so that that video was not too Shutterstock that was actually your link so that that's actual video from the company so if you want to get a sense for what it's like to work in your link that video is indicative of the atmosphere of your link it's an incredibly talented team and you're gonna hear a lot from from them tonight so we're gonna actually go quite into depth on what we're doing why we're doing how we're doing it and I'm just incredibly impressed with the caliber of talent at your link and the in fact the main reason for during this presentation is recruiting so we really want to have the the best talent in the world come and work at near link anyone that's interested in trying to solve this problem and that's actually the primary purpose for this this presentation so okay so the why of neuro-link just to go over it he is I think it's important for us to address brain related diseases the everyone if they if you survive cancer and heart disease odds are that you will have some brain related disorder so be like Alzheimer's or dementia and it if you don't friends and family will for sure and I think unless we have some sort of brain machine interface that can solve brain ailments of all kinds whether it's an accident or congenital or any kind of brain related disorder or or a spineless order if you know somebody who's broken their neck or broken their spine we can solve that with a chip and and this is something that I think most people don't quite understand yet and we're going to go over in detail how this is possible but I think there's there's an incredible amount we can do to solve brain disorders acted damage and all this will occur actually and it quite slowly so do what emphasize that it's not going to be like suddenly neuro-link will have this incredible neuro lace and start taking over people's brains okay it will take a long time so and you'll see it coming so getting getting FDA approval for implantable or devices of any kind is quite quite difficult and this will be a slow process where we will gradually increase the issues that we solve until ultimately we can do a full brain machine interface meaning that we can in Eltham Utley yeah this is gonna sound pretty weird but achieve a sort of symbiosis with artificial intelligence so this is this is not a mandatory thing this is the thing that you can choose to have if you want and and this is something I think it's gonna be really important at a civilization level scale so and I've said a lot about AI over the years but I think even in a benign AI scenario we will be left behind and so hopefully it is a benign scenario but I think with a high bandwidth brain-machine interface I think we can actually go along for the ride and we can effectively have the option of merging with AI this is extremely important and if you think about your limbic system and your cortex your your limbic system is kind of your primal needs and one since it's like where you're a lot of your emotions are coming from and then the cortex is like the the thinking planning part of your brain and I haven't met anyone who yes who wants to get rid of either the cortex or the Olympic system so clearly they worked were together well even though your cortex is in principle far smarter than your limbic system everybody wants to keep the limbic system and their cortex so hopefully we can have a tertiary layer which is the kind of a digital super intelligence layer and in fact you already have this layer so it's your phone and your laptop and the constraint is just how well you interface that the input and output speed so the Alpha speed is especially slow since most people typing with thumbs these days so you have a very slow output speed your input speed is much faster due to vision but the thing that will ultimately constrain our ability to be somatic with AI is bandwidth so in the limit after after solving a bunch of brain related diseases there is the existential it's mitigation of the existential threat of AI or yeah this is the pointed it so creating a well line future this is that that's the idea of nearly 100 billion cells called neurons neurons come in many complex shapes but generally they have a dendritic Arbor a cell body called a soma and an axon the neurons of your brain connect to form a large network through axon dendrite junctions called synapses at these connection points neurons communicate with each other using chemical signals called neurotransmitters neurotransmitters are released from the end of an axon in response to an electrical spike called an action potential [Music] when a cell receives enough of the right kind of neurotransmitter input a chain reaction is triggered that causes an action potential to fire and the neuron to in turn relay messages to its own downstream synapses action potentials produce an electric field that spreads from the neuron and can be detected by placing electrodes nearby allowing recording of the information represented by a neuron [Music] I rethought we'd play that twice it's so good you have to play it twice well I think it like a lot of people in the audience you know there's a wide range of knowledge about neurons I mean some people view the brain is like this incredibly mystical thing that cannot you cannot interface the brain but and and then some people are aware of the brain simulation such as occurs for Parkinson's patients so try to try to address the broad range of understanding so I mean you're on essentially but like you know there's that whole idea what if we were just a brain-in-a-vat this is often posed by philosophers except we are a brain-in-a-vat and that's it that VAT is us golf everything that you perceive feel here think it's it's all action potentials it's all just its neural spikes and it feels so real it feels very real but but it's it's this that these are all impulses from neurons what's called a a spike and a goal is to record from and stimulate spikes in neurons and and do so in a way that is orders of magnitude more than anything has been done to date and safe and good enough that you can it's not like a major operation it's sort of equivalent to just sort of a lasik type of thing so wait where you can sort of sit down machine does this thing and here you can walk away with within a few hours that's it and you know this you're not even in a hospital so so like this was basically it was a key points that worth taking away the system that we were designed in version one is capable of on the order of 10,000 electrodes so each each chip which is four by four millimeters is capable of a thousand electrodes or has thousand electrode and we think during after 10 is feasible so this is in contrast to the the best fda-approved system which is like a Parkinson's deep brain stimulation a thing which would have on the order of 10 electrodes so the system even in version one that we're going to unveil today is capable of a thousand times more electrodes than the the best system out there and they're all read and write so this is this is really quite I think I mean for something to be a thousand times more than what is public approved is quite a big difference and and this will this will get better with subsequent yes subsequent versions the slide may seem a little generic it was like everything's got robots electronics and algorithms at this point but no threads so the the feel like human transcendence there's actually I wasn't transcendence [Laughter] so that there's there's very tiny threads that are about about a tenth roughly of the cross-sectional area of a human hair so there are extremely tiny threads in fact the threads that we have it likes it even in version one are about the same size as a neuron so if you're gonna go stick something in your brain you wanted to not be giant you want to be tiny and to be approximately on par with the things that are already there but the neurons so this is about the size of of a neuron the each thread and then you really need this to be done with the robot because it's very tiny and it needs to be very precise so you don't and you don't want to pierce a blood vessel so when you sew each thread that the robot looks looks sort of basically through a microscope and puts a inserts each electrode specifically bypassing any vasculature you know any kind of blood vessel and and making sure it's like we inserted without causing trauma or minimal trauma yeah it's not zero but you won't notice it that's the important part you won't like you know yeah what feel thing so and yeah as the algorithms so just give you a sense of scale this is how tiny the threads are that is not even a big finger that is a small finger so the the these threads are just like like I said where smaller than hair and there's a thousand of them and this is what what the robot looks like it's sort of quite quite a complex device but it I it all comes down to a very tiny tiny point so just like you see the robot the robots on the left and and then the what looks like the needles for insertion next to a penny but in fact that the the actual needle that gets inserted is way way tinier it's that little tiny thing at the where the arrow is pointing that's actually the size of the the needle it's about 24 microns in diameter extremely extremely small it's so small you can't really even see it within the picture with the penny and then this is a your reign on uranium not really that's a car so you can get a sense for the robot doing the electrode insertion but that's a very zoomed in view so they're all very very tiny and the robot is very selectively applying them very delicately and and then this is what the jib looks like action potentials so the each one of those represents one electrode so there would be up to 10,000 of these about these lines yeah so I guess like it's always difficult sake there's gonna be a list there's a lot more in this presentation so in terms of things I think are important to bear in mind this I think has a very good purpose which is to cure important diseases and ultimately to help secure humanity's future as a civilization relative to AI the threads are very tiny and there's a lot of them and they're very carefully placed and the the operation on a per chip basis it involves just a a to me a two millimeter incision which is dilated to eight millimeters and then the the Chavez place placed through that and then we add it goes back to being two millimeters and you can basically good shot you don't need a stitch so and then the the interface to the to the to the chip is what is wireless so you have no wires poking out of your head very very important so you it's it's basically bluetooth to your phone because we'll have to watch the App Store updates for that one make sure we don't have a driver issue updating so but the key is like this this is something that is it's gonna be not not stress for our girls not stressful to to put in should work well hopefully it would check it out very carefully before it becomes obviously FDA approved and I and it's wireless so you the this this I think has tremendous potential and we hope to have this aspirationally in in a human patient before the end of next year so this is not not far and then as I mentioned earlier this is the main purpose of this presentation is recruiting and we need very talent people in very tell people in all these areas so it's a lot of very challenged people are needed to make this ultimately successful and then speaking of talented people let me hand it over to max yeah thank you thank you I'm max Hodak I'm the president of neural link I remember a couple years ago when we started talking about the idea of neural link and that there might be a company and whether this was even a good idea I mean my first reaction was that I wasn't sure that this actually was a good idea the technology was there yet and I think it's Elon has this incredible like incredible optimism where help Pierce through these imagined constraints and show you that really a lot more escape a lot more is possible then you really think is today and you have to be very careful telling them that something's impossible it better be limited by a law of physics or you're going to end up looking stupid and so I so I've wanted to build a neural interface has really been like a central goal of my life basically as long as I can remember this is I thinking like we talked about AI being potentially the last invention that we have I think that I've been with BMI might be like really the first invention in many ways of like the next chapter of us it's just real like as Elon alluded to earlier everything about your experience or thoughts or memories it's all in your brain and represented in the firing statistics of action potentials so all right so just what is a BMI and we'll go through this really like fairly quickly I think so there's you start with hopefully a brain and a machine but the machine is just a stand-in for the outside world it could be it could be another brain because it software it could be a robotic arm but you want to receive energy from that world and in part through the senses like vision and audition and impart energy back into the world to things like motor control and that that language that they use to communicate are they putting aside the hardware for a second it's very important understand what that is because people ask like oh can I talk to my dog or can I do these things but it's more to understand what that language is in that language in the most general sense is information to a first approximation everything is information but we just consider here the information represented in neurons and so consider two like toy neurons one so these lines are imagine action potentials and so imagine a neuron that fires very regularly like a metronome like this doesn't tell us anything there's no information conveyed in this signal we don't learn anything from it on the opposite end of the spectrum imagine or that fires completely randomly this also doesn't tell us anything that's also doesn't carry any information now we know that this is these two degenerate cases are not what neurons do because if you fit a model from recorded neural activity to behavior of things like a cursor of a patient or a subject that's implanted and you correlate these then you can build a graph that looks like this and this is a figure from a classic paper in this field from like the academic heritage in this field from 2003 I think that actually some of the authors of this paper in the audience today and you can see it's the x-axis is number of neurons and the R is the goodness of fit and you can see that as you add neurons the quality of the model improves this tells us that neurons care and their spike trains carry information about things here to ask them to us fairly quickly that's because what they were fitting here was just 2d cursor control which has simple dynamics and if you have tasks that are more complicated then you need more neurons so the classic definition of information is a difference that makes a difference it's just some piece of information or knowledge that tells you something it's like a very abstract concept but it's such like information theory is such a deep rabbit hole if you haven't seen it before the original paper mathematical theory of communication it's like it's very readable I highly recommend it you'll start seeing information everywhere it will totally change the way you view the world because the world is information as we've talked about before and understanding information also gets this question of well why do you have to have an implantable device why don't you have eg or a wearable or an optical thing and the answer of course is like well what's like these are different information carriers and what information are they carrying and we know that like if you open a back issue of the Journal of Neuroscience and you understand how some species of bird encode sound localization or something you'll find a discussion of spikes and we as far as we know everything that we care about is found in the statistics of spikes so that's what we focus on there are other things like fMRI or EEG these are different information carriers carrying different information which we think is it which we believe is impoverished relative to spike saying that's the scientific consensus and so the question for all these different things is well what information is found in your carrier we focus on spikes that means we have to be inside the brain because the there's no ceiling that we're aware of on that with respect to that like grand vision of your perception your thoughts everything like motor output and you like rien loss limbs and so why does that mean that you have to be inside the brain so you want spikes well people have studied if you take a neuron and you put an electrode on that specific neuron so you have a ground truth electrical potential of the that one neuron and then you place an extracellular electrode nearby which is what our electrodes and the Ute are a and other people are like we're not in the cell we're near the cells and then you measure how far away from a neuron can you be when you know what the ground truth spiking activity is can you no longer see the spikes and it turns out that the answer is about 60 microns which is like 0.06 it's it's very small it's a lot less than a millimeter so you have to be firmly under the skull like you're not there is no wearable that is going to get you spikes this is a physics constraints as far as we're aware and so now I want to I just want to talk about briefly there's like normally didn't come out of nowhere there's a long academic heritage of research here the cochlear implant has reached millions of patients since the 50s for deaf patients over a hundred thousand patients have received deep brain stimulation for Parkinson's and a central tremor and dystonia now other other indications and about twenty patients have received the Utah Ray which is a little hundred electrode rigid metal silicon device and even though it has very few channels they've been able to do some really cool stuff with it there's videos on YouTube of BrainGate patients doing things like controlling tablet computers or even texting each other through through Utah raised just from these the small number of electrodes and so there's many of the people on the team that normally came from this academic like this academic work I got my start working in a lab at Duke University studying the how mappings between brain and and like the screen space change so if you make it so that the joystick goes like the cursor goes sideways and you push forward instead of up like how does the brain change the representation so the point is that there are lots of people that have been looking at this problem from lots of angles for decades and we're in the greatest sense building on the shoulders of giants here and so the question is why not use one of those devices why not use a Utah or a deep brain stimulator implanted pulse generator and there's it's just in the Utah rate case the the rigid sharp metal electrodes produce a fairly strong immune response and this doesn't end up hurting the patient but it does mean that you lose the ability to record single spikes over some period of time usually between one and a couple of years there's also a big percutaneous connector through the scallop so you need to plug in big external electronics and you're never really confident that the rusco infection is is gone for the duration that you have the implant deep brain stimulators solve just solve a very different type of problem they are very effective for some Parkinson's patients but they have only a couple electrodes and they're really geared towards injecting large amounts of current not recording single spikes so they're really very different the DBS is really just a very different type of platform for very different type of problem so we had to go back to the drawing board and start over to build something that met the goals that you on laid out for us we knew as Elin mentioned that whatever we built we wanted it to be wireless ooh you know what any connectors or wires coming through the skin it had to be something that would last for a long period of time not something that you'd have to take out at two three or four years in it had to have practical bandwidth so we talked about high bandwidth or ultra high bandwidth like what matters is that it for the task that you're after there's practical bandwidth that allows you to effectively do that thing whether that's cursor control or typing or robotic arm or maybe in the future vision and it has to be usable at home it can't be something that you go into a clinic at the hospital for two hours a week and under tight supervision of technicians plug you into the amplifiers and turn it on that's me saying that you can live with and so two and half years ago we were nowhere close to any of that this is a photo of some of the prototypes that we've gone through over that over that time so we started on the far left that's the entirely passive board that has 64 electrodes on it and connects two connectors that go to big external amplifiers and then we added integrated electronics with our first custom chip that's also 64 64 channels and then there was a big leap to the the device that Ilan showed a photo of earlier that has 3072 electrodes in a fully implantable package with just a USB C port coming out and then we we took a step back in channel count B's room we have to optimize safety longevity and bandwidth altogether and so in order to optimize some of those other things we moved to an easier to manufacture system as 1536 channels in a USB C port and those last two are the focus of the paper that we released today and so we've we've learned a lot from these record a lot of data through these like these have are actually used every day at der link to record neural data and work with it and they taught us a lot about the architecture that we think is the basis for our first human product that we're calling n1 and the central component of that is the n1 sensor this is it's a little hermetic package it's about it's when it's fully assembled this is missing an outer mold it's into an 8 millimeter diameter force millimeter tall cylinder and each of these has 1024 electrodes and we can stim and record through through every one of those channels exploding it blowing opening it up a little bit you can see there's there the thin film which has the threads that Elin talked about which is the wisp going off to the side there's a hermetic substrate and then that gets welded later to a package that goes over top and that's made into our custom electronics and we'll go into more detail leaders that work on each of these talked about these in more detail over the restless' presentation so yeah I mean this is just to not to belabor the point I know that Elon really hammered this in but these things are very very small they're like they're not you can't you can't manipulate these this is one photo this is not two photos going together and you really can't manipulate these with your hand that that part at the top is just a backing material that's surgical packaging they're they're peeled off the threads were peeled off that one at a time by the robot to place it into the brain and then yeah and we had to build it a surgical robot and the first impetus for this is just you have to place these threads you can't manipulate these threads you need a robot and then that turned out to that grew into understanding where the blood vessels are and imaging into the tissue and the surface of the brain moves because you're breathing and you have a heartbeat and there's lots of complexity of dealing with this incredibly high entropy substrate and it's not all offload to the robot it's the robots under the supervision of a human neurosurgeon who lays out where the threads were placed but it would not be for the surgery is not possible without the robot and so the n1 implant we can place as Elin mentioned many of these possibly up to ten in one hemisphere for our first patients we're looking at four four sensors three in motor areas and one in a somatosensory area which are connected via very small wires tunneled under the scalp to an inductive coil behind the ear and that connect wirelessly through the skin to a wearable device that we call the link which contains a Bluetooth radio and a battery and this is importantly the only battery and radio in the implant so if you take this off the implant shuts off and if there's software upgrades or security issues it's much easier to upgrade the firmware on the pod than it is to try and change the implant it'll be controlled through an iPhone app you won't have to go to a doctor's office and have them have an exotic programmer to to configure it and the first thing that you'll have to do is learn to use it like a mad if you've never had arms and then suddenly you have an arm and you have to pick up a glass on the table it's like not a cognitive task you just like how how do you you can't think your way through that and so it's kind of a trippy experience at the beginning where like patience at first it just kind of wanders around and then they figure out how to break the symmetry and they learn how to control it and and that's like it's a long process it's like learning to touch type or play piano and so the for the first product where we're really focusing on three distinct types of control the first is giving patients the ability to control their mobile device to be as we heard from over and over again from patient groups that if you have to have a caretaker around the pressed buttons for you what's the point you might as well have them do the thing you have to get self-sufficient using using the devices on your own and then redirect the output from from your phone to a keyboard or a mouse on a normal computer it'll just show up as a as a Bluetooth mouse or a Bluetooth keyboard like any keyboard or mouse that you can use on any computer and as Elon mentioned this is now this is a forward-looking statement there's a whole FDA process we have to go through we haven't done that yet this is this is like these are aspirations but we are working as hard as we can towards our first in human clinical study next year and again these are plans but the the primary indication for that will be complete paralysis by spinal by upper cervical spinal cord injury and we're expecting that those patients will get four 1024 channel sensors one each in primary motor cortex some at supplementary motor area and dorsal premotor cortex which or two motor planning areas and closed-loop feedback into primary somatosensory cortex which is like if when you type or or walk or pick up a pen you don't there's aren't visually guided movements you have your body has all these senses of where it is in space and pressure and temperature and lots of other feedback and and we think for really high fluent control you have to provide that back to the brain for the synthetic effectors also and of course fully wireless and able to use it at home we think that there's a huge difference between something that you get to use two hours a week at the hospital versus something that you're living with every day and your brain is adapting to as much as the device is adapting to your brain and so to bring up the other other colleagues this team is like incredibly lucky you get to work with this team go into a little more detail on in that decoupling implantation from the electrodes is incredibly important the reason that you have these issues where things like these electric like a tungsten micro wires get rejected is they're stiff and they have they're stiff and sharp then they tear the brain and they have to because I have to get into the brain and so if you can decouple the process of getting it into the brain from what is left there where it can be much softer and have material properties like the brain and maybe be coated in things that help the brain recognize it as itself that's that's really important and then the thin film polymer leads the threads themselves are really cool material science and and we're going to more detail on that and then we'll also talk more about the chips and then a little bit more on just the neuroscience of how information is represented in in firing statistics in your brain and so with that I'd like to welcome dr. Matthew MacDougall thanks max I'm dr. Matthew MacDougall I'm head neurosurgeon at neural Inc when I'm not at neural Inc I'm a brain and spine surgeon here in San Francisco at CPMC California Pacific Medical Center before that I was at Stanford where I worked in labs that have implanted and designed advanced brain computer interfaces I originally became a neurosurgeon because I wanted to help people live happier healthier longer lives I've been humbled in practice by how powerless we are to treat many of the most debilitating neurologic diseases people afflicted with spinal cord injury schizophrenia autism and a host of other neurologic conditions have far too few options I work with neural Inc because we for the first time in history have the potential to solve some of these problems before we get to how we get the device in we have to talk about the guiding principle in our link safety everything we do it in our link is filtered through the question will this make me more likely to want to get one will it make me more like to recommend this to my family and friends this approach impacts every design decision we make so well for the immediate future knurlings devices will only be intended for patients with serious unmet medical needs our design philosophy is that this should be safe enough that it can be an elective procedure so what have we done to try to make it safe for starters we've created very small threads they displace a lot less tissue than the traditional methods in my regular practice today I routinely implant large deep brain stimulator electrodes into the brains of my patients they're big enough to have about a one and a hundred chance of causing a significant hemorrhage they displace and disrupt enough brain tissue that you can often see neurologic consequences just from placing the wire we can do better than that neural links threads are so thin that they're difficult to see with the naked eye they're much smaller than the width of a human hair they're small enough that a human surgeon can't actually implant them without help so we created help our link developed a tool that we're extremely proud of the robotic inserter inspired by designs conceived of in labs here in the bay at UCSF and Berkeley we developed this robot that can rapidly and precisely insert hundreds of individual threads representing thousands of distinct electrodes into the cortex in less than an hour this tool allows the surgeon to aim between the blood vessels they'll cover the surface of the brain with micron scale precision the region of the brain shown in this video represents only a few millimeters of surface of the brain as you can see the brain surface moves with the heartbeat and breathing the robot tracks and adjusts for this movement using this tool we can greatly reduce the risk of harming cortical vessels and causing bleeding here the robot is selecting individual electrode threads and placing them into the brain in the pre-planned location with remarkable accuracy and repeatability using this system we've been able to rapidly place thousands of electrodes into the cortex without causing noticeable bleeding we also have an in-house histology team that examines brain tissue to help us choose electrode profiles and materials to help us minimize tissue damage when you think of traditional neurosurgery you probably think of something very invasive traditional surgery on the brain isn't something that patients ever look forward to or are excited about except in the most dire circumstances usually a clamp is attached to the skull to keep it rigidly immobilized to the operating table we often shave all or most of the patients hair patients can end up with large visible scars at neuro-link we want to create an entirely different patient experience something more like LASIK no scars no big scars no hospital stays no short procedures sorry no hospital stays very short procedures and of course in the end you get to keep all your hair we even want this to be possible under conscious sedation that means you can get rid of the complexity and the risk of general anesthesia as well as many of the unpleasant side effects nausea sore throat from a breathing tube to be absolutely clear our first clinical trial patients are going to receive an experience much more like traditional neurosurgery but our aim is to simplify the procedure down to the injection of local anesthetic a very small opening in the skin a painless opening in the skull below quick and precise placement of threads into the cortex and then we fill that hole in the skull with the sensor allowing the scalp to be closed up over it behind the ear we'll make a small incision to insert the coil we will tunnel tiny wires under the scalp to connect the sensors to the coil that's the process I believe that neural Inc is going to be able to provide us in the medical community with a platform that can finally enable us to treat some of these very difficult to treat diseases also to understand them better I hope you find this as exciting as I find it if you feel you might be able to help us don't hesitate to contact us to talk more about the technology behind all this I'd like to introduce Vanessa Tolosa director of our neural interfaces group hi I'm Vanessa Llosa I lead the neural interface group at neural Inc our team consists of engineers and material scientists who are responsible for making the probes that get implanted into tissue the packaging for the electronics and integrating these two components together we also do all the testing and characterization of these parts before joining neural Inc I led a neuro tech team at Lawrence Livermore National Lab there we worked on a wide range of neuro first static technologies that were used both in the academic and clinical settings I decided to join neural Inc because I saw an opportunity to take all of this exciting work that we were seeing in neuro tech research and actually make them accessible to patients at a much faster time rate than what medical device companies have traditionally been able to do with that in mind at noir link we set out to create a fully implantable neural interface with thousands of channels that are capable of single spike resolution this device must last a long time in the body to do this it must be small flexible and made of biocompatible materials that will minimize the brain's immune response to protect the electronics from the caustic environment of the body it must have airtight packaging also known as hermetic Packaging the device must also be able to both record from and stimulate neurons this is essential for a highly functioning BMI finally the manufacturing process must be scalable and capable of making micrometer sized features consistently currently there are no research or commercial commercial devices that meet all of our requirements so we built one out of microfabricated thin film polymers just like in semiconductor chip manufacturing we use a layer by layer process that generally consists of three repeating steps we're either always depositing material patterning and materials through photolithography or etching away a material depending on complexity of the design these steps can be repeated over a hundred times and to make things more challenging we are limited to materials that are safe for the body in our current design we have a three metal layer process that results in a five micron thick and tend to my 40 micron wide probe to give you an idea of how small this is red blood cells have a diameter of about eight microns and an average strand of hair is about a hundred microns yet in the small footprint we're able to fit our electrodes our wires and insulation for each of those wires with micro fabrication we can drive features down to the size of an electron beam so this is great because we want to make our probes as small as possible essentially we want it to be invisible to the brain but there are other factors that limit the size of our probes for example as we make the wire smaller it increases the resistance of those wires and as a resistance increases it makes it more difficult for us to separate our signals from our noise similarly there are other technical challenges and trade-offs there related to higher channel counts and manufacturing yield electrode size and material and tissue much safety at neuro-link we have an incredible team that's been tackling these challenges and have been able to make high channel counts polymer probes and this image is a silicon wafer that holds ten of these arrays these polymer arrays in this design each of those arrays has over 3,000 channels so what that means is in this one wafer we've manufactured over 30,000 electrodes and over 30,000 insulated wires this is something that can't be done with the way current medical devices are being made that rainbow effect is caused by the small feature sizes on these devices that are interacting with the nanometer sized wavelengths of light that are reflecting off of them if you were to zoom in on the ends of one of these arrays you'll see these this region where we put all of our electrodes so each of these vertical filaments that end in a loop is what we've referred to as a thread and each of these threads can be placed independently into the brain using our robot during surgery this design is called linear edge it's one of over 20 designs that we've made for our R&D work we progressively been increasing the number of electrodes per thread without significantly increasing the width of each of these threads at the base we've been able to do this by adding layers and reducing the sizes of the of the wires down to as small as 350 nanometers this is less than a wavelength of visible light because we're using a lithographic process essentially if we can draw it we can also make it so in one end of our probes are the electrodes on the other end or where we connect the probes to the electronic package or to the electronics through conductive feeders this substrate is part of the Hermetic electronics package standard methods of connecting the probes to the electronics package usually involves some kind of large plugin type connector or a polymer based glue that bonds the two components together but as we increase density and decrease the the footprint becomes impossible to receive to achieve hermiticity in standard medical device connect connectors this is due to several reasons one of them is how these substrates are currently manufactured hermetic feedthroughs consists of holes that have been packed with conductive materials and are embedded in an insulating substrate as you drill more holes and pack them more tightly together these brittle substrates typically made of ceramics become more susceptible to cracking also as you make the hole smaller it becomes more difficult to fill them with this conductive material without getting non hermetic voids standard processing also requires exposure to high temperatures typically over 700 degrees Celsius at these high temperatures the coefficient of thermal expansion or CTE mismatch between the insulator and the conductor can cause circumferential cracking or interfacial gaps during the cooling phase we're able to get around these problems by developing a new process so rather than making the probes and then the substrates and then connecting them together instead we micro fabricate them together into one monolithic component this provides a tight seal at densities that current methods with standard materials for medical devices can't achieve so far we've used this process to make a hermetic thin film substrate with over a thousand connections over a 2.4 millimeter by two point four millimeter footprint next we assemble the electronics and then also attach a wired lid using a laser welding process these two steps have required a lot of internal development as well the result is the sensor that's ready for final assembly and implants into the body next you'll hear from my colleague DJ our customer about our custom electronics Thank You Vanessa my name is DJ Sol and I'm the director of implant systems and neuro-link my team focuses on building chips and systems to get neural signals recorded from our electrode out of the brain and also to put information into the brain before neural link I was at UC Berkeley where I co invented neural dos which is a technology to power and communicate with small implantable systems using ultrasound waves typical chip life cycle from design to verification to tape out is approximately one to several years a neural link we had the ability to co.design our chip with the rest of the system and the type feedback loop from this organization has enabled our small team of analog and digital chip designers to tape out a new design every three months on average that means over the past 24 months we've done eight papers in total representing 15 different chips that have been designed fabricated tested and used in development the artwork that you see on the top of the slide is of some of the actual chips that we made so far for any custom chips we make the architecture can vary substantially but the basic ideas are the same neural signals recorded from the electrode typically look like the one on the slide and in order for us to extract the information that we care about we need to first amplify filter and digitize those neural signals and use digital logic to process and send out the bits we want for BMI we also need ways to diagnose any issues with our electrodes and be able to drive stem stimulation engine to inject charge to the brain when required our latest trip is called n1 system on trip and it is physically small measuring only 20 millimetre squares or four by five millimeters it is low power highly configurable and it has 1024 simultaneous record and stimulation capable and it has on chip spike detection to dive deeper into n1 SOC I like to highlight three key innovations and they are one analog pixel two on chip spike detection and three stimulation on every channel the first is analog pixel before we can convert analog neural signals into digital bits we need to amplify and filter them and this is where the analytical comes in we want to have one analog pixel per electrode so that we can configure them independently so in the case of N one SOC there are 1,024 an old pixels and all that pixels also take up a significant portion of the physical space on the chip and how well they work determines both the signal quality and the characteristics of the overall neural interface the goal of analog design is analog pixel design is to make it as small as possible so we can fit more as low power as possible so we generate less heat and have longer battery runtimes and as low noise as possible so we get the best signals now the challenge here is that these goals are at odds with each other for example we want to achieve lower noise on the amplifier so that more spikes can be detected but as transistors get smaller it becomes harder to get lower noise while keeping the power the same or less since the start of nor link we've gone through three major revisions to the analog pixel progressively improving both the size and power while maintaining performance over the past 24 months we had seven fold improvements in the size of the analog pixel and our latest pixel on the right is at least five times smaller than the known state of the art of similar architecture with one pixel dedicated per electrode as published in the academic literature second innovation is on shift spike detection once the signals are amplified they're converted and digitized to zeros and ones by our on chip analog to digital converters as you'll hear in a second spikes or action potentials shown in this slide are often critical for certain BMI tasks currently there are several different methods for detecting spikes such as thresholding or more sophisticated methods such as principal component analysis and neural link one of the robust ways that we came up with is by directly characterizing the shape and it's worth noting that this is different than template matching and that it gives us more information in a general way in certain cases we can actually identify different neurons from the same electrode based on their shapes our analog pixel can capture the entire neural signals sampled at 20,000 samples per second with 10 bits of resolution resulting in over 200 megabits per second of neural data for each 1,024 channels that we would that we record in our previous systems that you heard about we were able to stream this entire broadband signals through a single USBC connector and cable and we performed real-time spec detection on an equal machine running our optimized decode now we wanted to completely eliminate connectors and cables for n1 so we had to modify our algorithms to fit into the hardware by scaling both making it both scaleable and also low-power and then we were able to also implement this algorithm in our n1 SOC our algorithms can compress neural data by more than 200 times and it only takes nine hundred nanoseconds to compute which is faster than the time it takes for the brain to realize that happen finally it was important for us to enable stimulation from every channel that we can record from and make a configurable and high resolution to make this work we custom-designed stimulation engine for electrical stimulation that can coexist alongside our analog pixels our stimulation engine has point to micro amp of amplitude resolution and 7.8 microsecond of time resolution there is a 16 to one ratio of electrode to stimulation engine so we can't stimulate every channel simultaneously but we can within each stem pulse usually in milliseconds and we can also stimulate any combination of 64 channels at the same time so in summary looking through our n1 SOC it has 1024 analog pixels that we can record from simultaneously with 7.2 micro volt RMS noise while only consuming 6.6 micro out of power it has on chip analog to digital converters on chip spike detection that can compress neural data more than 200 times and it only takes nine hundred nanoseconds to compute stimulation engine with point two micro amp of amplitude and 7.8 microsecond of time resolution and finally Diagnostics for electrode and impedance measurement all of these functionalities that I outlined are integrated into a single four by five millimeter silicon die next my colleague flip will tell you more about what can be done with these signals thanks DJ my name is Philip sabbaths and I'm the senior scientist at neural ink before neuro-link I was at UCSF where I was a professor of physiology there for 16 years I ran a lab that studied how the brain processes sensory and motor signals we developed new neuro technologies and we studied how to take those tools and use them for neural engineering applications today I'm going to tell you about how it is that we can use those amazing devices that Vanessa and DJ just told you about to communicate with the brain now specifically I want to tell you about two things first I want to show you that the work that we're doing doesn't come out of thin air we're building on over a century of neuroscience research and decades of neural engineering research these provide a solid foundation for the sorts of things that that we're talking about second I I want to show you why we believe that even more advanced applications are possible with more advanced devices now when Ilan contacted me over two-and-a-half years ago now and told me about his vision for the company I knew that I wanted to join for these two reasons because I knew that the technology was at a point where with the right team and the right to right vision and a long term vision we could do the sorts of things that we're talking about and I knew that with that team we could do things that no one had even dreamed about yet okay so the first thing I want to show you is a video many of you who are seeing this have seen videos like this before so you know what it is but if you don't know what this is I have the distinct pleasure of telling you that right now what you're looking at is the brain at work eat this is in fact traces of a bunch of electrodes that came off of one of our devices a bunch of electrodes from a single thread and each trace shows you a voltage waveform in time as it's coming off of one of those threads now if we focus in on one of those traces the first thing you may notice is that there are these big voltage deflections that happen periodically and these are the spikes that max and Ilana and others have talked about these spikes occur again when a neuron has an action potential and this is the fundamental element of communication within the brain and this is the thing that we want to tap into this is what we want to be able to record now as DeeDee just told you we have algorithms that can detect these spikes in real time as they're happening and that allows us to collect data that looks something like this this is what we call a spike raster so each row there represents one channel of recording and time goes from left to right and each of those little tick marks is the time of a single spike in action potential all right so presumably there's some information somewhere in there how do we get at it what are we going to do with it well for the first application which max told you about which is allowing paralyzed individuals to be able to control a computer what we want to do is we want to reach into primary motor cortex and record the activity that's happening their primary motor cortex is the part of the brain that sends signals down the spinal cord and to the muscles to drive movement of course it does that with action potentials and in particular we want to record from the hand and arm portions of primary motor cortex so imagine imagine that you have a person sitting holding a mouse and they're sitting still and then they make an outward movement with their mouse and then they reach back what would you see in the brain well here's a here's a synthetic neuron I made data up but but it gives you the idea here's a synthetic neuron that shows that in the background activity when the person's at rest maybe there's some firing but when that neuron when that person reaches outward that neuron starts to fire a lot and when he reaches back the neuron becomes quiet so this is what we call a neuron that's tuned to a particular direction of movement now maybe we'll record from another neuron and this neuron may have a different pattern it may be tuned to the return movement and not to the outward movement so it fires more on the return what if we ask the person to do that movement again what we would see is a similar pattern of activation so the neuron on top still fires more for outward movement in the burner on the bottom still fires more for the return movement but you'll notice that the patterns are different and that's because neural activity in the brain is random it has stochasticity which means that even though the person may be intending to do the exact same thing from one movement to the next the neural code the neural representation at the level of an individual neuron is noisy and this is just one of the reasons why we need to record from lots of neurons in order to be able to gain a high fidelity readout of what the intention is so okay so let's say we record from a bunch of neurons it might look something like this if you look at that you might think that looks pretty messy and it's not clear what's going on but I'm gonna do a little trick I want to take those neurons I want to rearrange them so that they're in the order of the tuning that they have look just as I told you about those two neurons and if you do that look what happens now suddenly structure emerges and I think you'll agree looking at that but there's information in that stack of neurons that tells you about the movement and that's exactly we want to do we want to do that kind of magic in an automated way to read out and to read out the movement the way we do that is by building something that we call decoding algorithms these are mathematical algorithms that we tune based on data like these to be able to take in just those raster's of spiking activity and output the movement that's that the person wants to make okay so for these little fake data I built a very very simple decoder and sure enough it's able to to capture the intended movement this is what we want to do on skel no you might say to yourself I don't understand your talk about moving but I thought it was about paralyzed people right so how does that work well it turns out we know from a lot of prior research but even if you're not actually making the movement even if you're just thinking about the movement or even if you're watching someone else make movement the cells and motor cortex respond in a similar way so we can build up these decoding algorithms just from from those kind of data and then a paralyzed person can think about moving the mouse and the cursor will move now this this kind of decoding has been done in a fair number of academic labs including my own before I came here and in humans and academic studies well what we want what knurlings goal though is to be able to do this with a clinical device that people can take home and use on their own and there's orders of magnitude more channels or orders of magnitude more neurons that we're recording from with that we think the people will be able to get naturalistic control over the computers not just a mouse but also keyboard game controllers and potentially other devices that's what we're trying to do now I've told you about the arm and hand area of motor cortex but the devices that we're talking about because of their high bandwidth and the ability to tailor the location of each and devote individual electrode to a person's individualized cortical anatomy we should be able to reach anywhere in motor cortex so for example there are areas at the base of motor cortex that are responsible for driving activation of the speech articulate errs there was a recent lovely study from UCSF that showed that from activity like that you can actually decode the speech so you can you can decode the movement of the articulator and from that you can create synthetic speech so potentially with a device like this you could restore speech to a paralyzed person who's no longer able to talk but there's no reason in principle that we can't reach all of motor cortex and that would give us access to any movement that a person thinks about any movement at all a person could imagine running or dancing or even kung-fu and we would be able to decode that signal so that could give a paralyzed person the ability to control say for example a 3d avatar that they could use for online gaming for sports it could allow them to control a wide range of assistive robotic devices and ultimately if and when the technology for spinal cord nerve or muscle stimulation gets far enough ultimately it could be used to restore that individual's control of their own body okay I've talked about readout but we remember we want bi-directional information we don't only want to read information out of the brain we want to be able to put it back into the brain now some of you that may seem a little bit fantastical that you could write information into the brain but actually the the basic building box of that technology are already there this is the same image that you saw before of an electrode next to a cell it turns out if you pass a tiny amount of current through that electrode what happens is that you activate cells nearby you cause them to to fire an action potential one or many and that is the technology that is already being used widely outside the brain say for example for cochlear implants which have been used for decades to restore hearing to the death and more recently in the eye to restore vision to the blind in a fairly rudimentary way as I'll tell you more later but in addition you can use the same technology in the brain for example to restore the sense of touch or to restore vision and I'm going to tell you very briefly about those two applications so let's start with the sense of touch consider this little bit of tissue that of brain that I've just highlighted here that's at the border between motor and somatosensory cortex so if we blow that up what you can see is that somatosensory cortex has a very special property it has what we refer to as spatial spatial map and what I mean by that is that there are regions that encode the palm of the hand and each of the five digits for example so if we were to stimulate at one little location say in the thumb part of the of the cortex the person would feel a sense of touch of pressure on their thumb or if we were to stimulate two sites on the palm in the palm area of cortex you might feel a couple of points or touches on your hand this kind of technology has been demonstrated in in many academic labs and in a recent really nice paper it was shown that with subjects controlling a robot arm through BMI getting tactile feedback of when that arm or when the hand of that arm was grasping an object improved the ability to pick up and place objects with the robot so this is this is the kind of thing that can really help decoding so imagine what we could do if we're able to take our device and cover all of somatosensory cortex we could give rich sensation of objects of haptic feedback when you're manipulating objects we could maybe feel different textures but it's not just about improving the user experience it's also about getting to the level of functionality that we want imagine for a second imagine typing now imagine typing with your fingers anesthetized that's going to be pretty hard so that haptic feedback that sense of sensory feedback during movement is going to be important going forward and and yeah okay so that sensory feedback for the hand we can also potentially provide visual feedback so visual cortex just like somatosensory cortex has maps so there's a spatial map in visual cortex which is here in orange in the back of the brain so for example if we stimulate a particular point in cortex we might see a flash of light in a little punctate spot in front of us and this was demonstrated many years ago in in by neurosurgeons and it's been used in academic labs and that we call that a phosphine and you know if you stimulate another area look at a phosphine in a different location so the idea here is that you could stimulate a bunch of different areas and you could create kind of like a dot matrix image of the visual world which could provide a rudimentary form of vision and there are academic labs and even companies that are working on technology just like this but there isn't just one map in visual cortex actually there are a bunch of different maps was a good example of how the brain works is a spatial map but there are also there also maps telling you about the orientation of edges in the field their maps telling you about color there are maps telling you about the size and speed of objects moving so what we want is a device that has sensors that are small enough electrodes that are small enough and high enough density that we can tap into that rich collection of maps with our stimulation devices so that we can do better than just dot matrix so that we can actually create rich visual feedback for the blind that's that's the long-term goal okay that's just again one more example of the way that these devices can be used so I've talked about recording signals and I've talked about stimulating you can combine those two to treat a variety of neurological disorders max talked earlier about deep brain stimulation to treat say for example Parkinson's disease and many people have have those devices in academic labs have recently shown that you can do better with stimulation you can treat better if you also are able to record the state of the brain say for motor cortex and use that to shape the pattern of stimulation deep brain stimulation or DBS has also been used for dystonia is it already proved for dystonia and obsessive-compulsive disorder and we think again close the therapies can do better and in fact for epilepsy there's already a commercial product that does this kind of closed loop seizure detection and disruption although it does it with only about eight electrodes there are many other sorry many other diseases oh no all right we'll get there sorry there are a number of other neurological disorders where DBS has promise but it's still an investigational stage like depression chronic pain tinnitus now even though these diseases are currently being treated with these big DBS electrodes like you've seen we think that there's a potential here for the kinds of devices were or designing an individualized highly focused treatment that will reach broader patient populations and be able to be more effective in the way that they treat these disorders alright lastly I want to tell you about not just sensory input and motor output but about about thought so there are parts of the brain where we know that there's neural activity that encodes the things that you're thinking about and one great example is an area called the hippocampus the hippocampus is involved in memory formation and it helps store episodic memory things that you remember from your life it also has a particular kind of memory for locations and views that you know for example it'll have cells that represent places in your own home or in a city that you know well so imagine that you had that you could record from a collection of neurons in the hippocampus of somebody who lived in San Francisco and knew it well then it's likely that they would have some neurons there in the hippocampus that represent various locations in Golden Gate Park and so if that person were to take a car ride for example from the ocean through the park you would see those neurons fire in order as they took that ride first a neuron that maybe represents a view of the ocean and then the Bison in their Parekh and so on so I've told you about the way that the brain represents information and that these these sorts of of encoding methods representations in the brain are things that we can learn to decode and I've told you about some of the applications and how they might work as the device technology gets better and better and as we get more and more experience with those devices we and other researchers will be able to bootstrap off of those advances to reach other brain areas and other applications that's that's what we're trying to do nor links goal is neuroscience has shown that a wide variety of information content is readily available in the brain we know for example that there are signals that encode speech and language there are signals that encode your mood there are signals that encode the sense of pain when you're hungry and when you're thirsty there are signals that encode your memories and even esoteric things like mathematical reasoning but knurling wants to do is to give people the ability to tap into those representations to get act better access to that information both to repair broken brain circuits and also to ultimately give us better access to better connections to the world to each other and to ourselves all right thanks so we're gonna take a risk Aaron just do some Q&A so that's hopefully was a good understanding of the brain forever yeah nice nice work guys like it's a very proud of the neuro-link team it's done amazing work and yeah it's a really smart smart group I was a lot more where that if there's a lot more really smart people so what you're saying here is the outcome of a lot of hard work by the new rolling team and yeah I think it's it's there's some pretty impressive stuff so we take some questions from the audience it's the lights are bright so you might have to like yeah just take you later this is this is a really interesting question so it's it's definitely too early to really think about this there's nothing new on I don't I think we'll hold open the custom code immediately yeah I think I mean it's conceivably that could be some kind of App Store thing in the future or some sort of platform with like a very rigorous you know verification of the application but yeah I mean I think that there cuz this is certainly not meant to be sort of a closed system ultimately if we can enable others to contribute whether they're at Munich or not that that would be a good thing you just add one more thing to this we've had some discussions like it might be that if you want to build an app or a business on top of it like a brain enabled API then your business model can't be advertising for example there's that's like it's very important to us but there's like that's that this turns out well for everyone and so there's there's thinking like that going on [Laughter] yeah there's no doubt that that plasticity will make things so the question was whether neuroplasticity will help or hurt our effort and I think there's no doubt that it will help first of all there's just the effect you have to learn how to use these devices but for example in work that we did in my lab earlier we showed that you can write in information that isn't perfect it doesn't get that map perfect or even it can be somewhat quite different from the map but ultimately you can learn to use that through plasticity and you know it's it so it's it's there's going to be a lot of learning required and the ability of the brain to adapt to new information and a particular the ability of the brain to take information that comes from multiple sources and merge it in an effective way is I think the kind of thing that really will will facilitate complex new tasks with these devices it's a sense reason that the the neurons are responding to electrical pulses so it as a the electrode is providing electrical pulses it's it to the neuron and electrical pulse is is a neuron it just thinks it's in your and it's gonna for sure adapt dynamically because it just sees electrical pulses and it's it's going to respond to those [Laughter] we cannot do that that would be so the question is if this will not be advertising driven which I think would be unwise then how will it be paid for essentially is that correct or how will you ensure that it's broadly available yeah well I think that the cost of these you know brain disease or brain injuries is extremely high to society if you have to take care of somebody or that they need if they need comprehensive medical care or hospice that this is actually very very costly to society so I think it the economics of solving for that make a ton of sense and if you enable somebody to you know work and be a productive you know it could you know could Rivard contribute to the economy I think that that will I think that the economics of that will easily pay for itself and and then in the limit of course if you want to be somatic with with a I feel like I think it's safe to say you could repay the loan if with superhuman intelligence I think it's a safe bet so I think the economics this will work out and the first order is really just to make sure that it works and works safely and then and I think it'll really be the option of other person but but it is critical that this be so as we've talked about for like a laser Glock device if if one has to be if this has to be done by a neurosurgeon it is it cannot be scaled there's just aren't enough neurosurgeons so it must be just just as one one wouldn't want sort of like a hand operated laser for you know an ophthalmology situation you really want the robot doing it with precision the same thing goes for the brain interface so sure yeah it says the question is have we implant the chip in animals and if so what are the results so I if it's important to say that we regard you know any any triple implants even if it's an in a rat as a very serious thing like so we care we even care about rats even though they have the Black Plague and everything you know so like the arguably have some karmic payback if it but nonetheless nonetheless we're we care about rats so and then we're extremely sensitive with with with monkeys and we work with University of California at Davis for the the any of the monkey activity so and the results have been been very positive do you want to every talk about I know this is this is a sensitive subject yeah but I think it is yeah we definitely need to address the elephant in the room yeah I think that it's there's we wish that we didn't have to work with animals right that we just wish that wasn't like a step in the process but it but it is it's like it's a very important part in the research and development process to produce better outcomes for human patients and improvements in human health and we're try to be very thoughtful and and we follow the the 3 R's of like reduction replacement and refinement of laboratory animal medicine and and we try to be very careful and thoughtful about it and do it as efficiently as possible because we believe that the benefit to humanity is is in the end like about the the benefits outweigh the negatives but the question are also asked about the results and there is paper available I think now soon that has some of the results in them yeah but we have made a you know monkey has been able to control the computer with its brain just yeah why I didn't really start running that result today but there goes the lung he's gonna come out of the back so much fun a point oh yeah so can we speak about the FDA path that we want to pursue and how we might work with the scientific community I guess yeah sure it's well let me start with you well I was also said like we're under no illusion that we think we can do all of the science required for this ourselves like there's an immense amount of neuroscience to be done with these devices and there's a huge amount that we have to learn about the brain and and that's going to be a much larger thing than just a neural link and we want to get these tools into the hands ventually at the right time I think that we're still very small company just focused on getting our first patients and we have to be laser-focused on that but we want this to be a thing that is much larger than this to be a field right we want this to really fuel advancement of the field because the most important thing is not that like neural link is this like one specific place but that it advances all of us and for the for the FDA there's there's a pathway we're pursuing an early feasibility study IDE and it's and and there's the the FDA actually put out draft guidance in February that's very specific to the type of thing we're doing and it's pretty prescriptive it's it's a checklist of what they want to see and and there's a lot in it you have to show that it's gonna be expected to be safe and biocompatible and and stable but you work through that and you give them the documentation I mean people in academia right now quite constrained in working with the the Ute are a is that that's the most advanced thing in academia and our system is at least a hundred arguably a thousand times well it'd be on the order of a hundred I suppose relative to the potential of the Utah Ray tours of magnitude improvement at the experimental level so I think it probably would make sense for us to make more of the robots and provide the chips to academia to further the science sure hard for me to see so you've obviously been working under the radar for quite some time and you've made significant advances now that you're public and very obviously recruiting today and I would assume looking for additional academic partnerships to sort of accelerate your development what is the best way for to get in touch with you and start to show what is best way for academics get in touch with us to collaborate on furthering the field and and yeah hey guys first if anybody out there has technologies or ideas that they haven't heard us talk about today and if they think could be useful for us they should reach out and tell us about it we're always interested in new technology and new things that are happening in the field so that's a first as far as getting the academic community access to data for example this is something that we are committed to doing the details the pathway on that aren't are fully worked out there a number of options and to be honest if you have ideas about ways that anyone in the academic community thinks that it would be good to engage with us we're willing to listen and we'll we'll pick one or two and we'll we will make it possible for the technology that we're working on to have a very broad impact on the field that's definitely one of our goals yeah I mean the things that really drive the technology are you know advancing that the chip design the the software on chip and for interpretation of the results material science for the especially for the coatings of the electrodes like how do you have a long life electrode is it's quite a difficult thing to get get that coating get the materials right and then the application materials for the Mayo something that you you you want to be because you want these electrodes to last for many decades I in the brain but the is quite a difficult environment it really wants to corrode so hacked getting the right the right coatings is incredibly difficult it's a tough material science problem sure right dude do we consider longevity a solved problem definitely not so I think the longevity is one of the key questions and they supposed to say like until you actually haven't implanted it how long does it last and then if it does if it does start failing does a fail in a benign way or in a bad way so I think the it's there's not enough time yet to actually say whether it is a long time with the network obviously makes sense to have accelerated life testing of the electrodes so in a non brain situation so you figure out something that's actually a worse environment than the brain and actually which is actually quite a difficult environment for a chip so and electrodes II if I find something it's even worse and have accelerated life testing that's that's one of the key things and then they need to confirm that in and in in a actual brain so I mean the the latest results are quite promising but it's it's too early to reach conclusions conclusions I think we've just recently seen what we think is a rock will break through but we're time will tell for sure how will we address the mechanical mismatch of the electrodes and the brain essentially because the cells are like jello and electrodes are really hard and so they have a big stiffness difference between brain cells and metal electrode yeah yeah I mean that's part of the reason one of the reasons why we went towards flexible materials like there are a lot of different kinds of probes out there right now if you look it up and that's that's the main purpose of going flexible and also going smaller so as I said that we're really trying to make it as invisible to the brain as possible and then on top of that as was mentioned coatings so all of those combined we're hoping to significantly decrease that immune response but essentially have if you have something with high modulus and something with low modulus but if you make you for making the hime ologist thing very thin it becomes quite flexible yeah moment ever no sure I said I'll try to answer question with the back there I could talk to me to see behind the camera so I think if somebody's a question like essentially if we look at like higher-order feedback where it's sort of at the maybe a whole limb level or combination of limbs or a whole word or letter as opposed to an individual phosphine or something like that okay I think that of course that's a stimulation result not a recording result and it may have something to do with the kind of low resolution of the stimulus that the psychophysics of that it's hard to perceive it as a complete letter but you get filling in of motion that helps you perceive the object I that's kind of sort of my thinking of what's going on and so you know I think these are the kinds of questions that honestly we we haven't yet addressed when I talked about visual stimulation and the kind of rich visual stimulation we want to just to be clear that was aspirational and so you know come back and ask us in a little bit you know yeah but there are is the you know they are like individual neurons that you can't trace to particular names and concepts and people and you know at a kind of advanced long-term level I think people would have kind of like if you if two people had a neural link you'd be able to effectively have a sort of really high bandwidth telepathy or you know who actually technically going over radio waves but it would you could actually communicate at the sort of complex meme structure level using the Dawkins version meme yeah so then you really can't have like potentially a new kind of communication it's sort of like conceptual telepathy essentially it also be consensual I'll try to answer something in the back yeah I can hardly see you but anywhere anyone in the back there basically Thanks [Music] haha okay what's your answer to what do you think we should do I mean we're open to ideas here the overarching objective is to make future better aspirationally and and you know hopefully not pave the road hell with good intentions I think the word hell is mostly paid with bad intentions though big question yeah I think that's it's it's a it's a big philosophical question what what we'll it's hard to say what the future will be with something like this brain machine interface I I doubt that we would be able to eliminate all suffering and it actually maybe ultimately hardly dystopian if we do it eliminate all all suffering if that actually may not be a true utopia you know there's like generally like stories about utopia tend to turn into just Sophia so I but I think we can definitely make a significant difference and we can address that you know when I say we I mean humanity can't address a lot of the suffering that occurs in the world and make things a lot better and I think a lot a lot of times people are quite so if neg about negative about the present and about the future but really I think if if you're a student of history the you when would you when else would you really want to be alive knows now's the best time pretty much it's like those who think the past is better I'm not right enough history i wreck your way in the back there I see actually you can see a hand yeah yeah yeah I think that this but so so know you often have very tight latency constraints in this yeah they have to run locally yeah so the question is how are we doing the backend computation what are some of the methods that we're doing it yes max just mentioned latency is something that we do I want to minimize as much as possible especially in the context of doing closed loop PMI so being able to record and also stimulate and put information back into the brain so in order to make sure that that latency between the end-to-end system is minimized we need to have all that computation locally so that's you know one instance of the evidence of spite detection that you saw that's slowly progressed from you know computation on to from the computer on to the integrated trip is something that we strive to do as well with close look beyond my algorithms yeah essentially the the vast amount of the computation is actually done locally on an ASIC effectively so the amount of data that needs to be communicated beyond the body or the brain is really distilled down to a small amount and and as these are saying like especially important for if you if you want to say it gives on who is a tetraplegic the ability to type at 40 words minute which is one of our goals and that that requires a very very low latency feedback loop I think okay that last question who do okay go yeah far away okay this is a few questions packed in there so one of the first question was I mean if it actually essentially summarizes what we're saying is like what's the ratio of electrodes to neurons because you wouldn't want a one-to-one ratio because that's a lot of electrodes so you really want to actually have that ratio of years as big as possible and ideally at least a hundred to one ask maybe a thousand to one ideally so because the big of the ratio of neurons to electrodes the fewer implantations it's going to be just way better so for sure you'd want what you'd want to to read a whole bunch of neurons and then be able to stimulate neurons and or a cluster of neurons by varying the field potential so that you don't you don't need to have a one-to-one stem or read ratio what I deal II 100 to one because when I've had more yeah I think it's important to add to that that any answer we give to what's the right number of electrodes at this point is speculation yeah that's that's part of the cutting speculate yeah but but but but the point I'm trying to make though is that this bootstrapping that we anticipate is going to happen we're gonna put in devices they're going to be lower level from that we'll be able to find out the information content and information density in the areas that we're looking at with the electrodes we have and we'll and we'll be able to bootstrap up from that so you know I actually believe that we are the ones that are going to be able to answer that question without speculation and I'll say when you look at the anatomy of the brain the brain is mostly silent like if so it on average a classic electrode can see somewhere between 0 & 4 neurons electrically by by different wave form templates but when you just look at the anatomy and like the distances you'd expect it to see more like a thousand and so this question of why is the brain so silent is an interesting one and one of the hypotheses is that a lot of neurons that are very narrow receptive fields that only fire when they have very high information content updates with respect to something specific and one of the challenges with spike sorting is that you can't tell like you can't tell apart like another spike of a neuron that you think you're recording from with that was the first time I heard from that neuron specifically that I don't know is there right and as you have these long lasting chronic devices you'll be able to get more of that out in the decoding yeah I mean I think it's the kind of very the the electrode in your own dense density is gonna vary quite a bit depending upon what part the brain it is as well if it's sort of sort of somatosensory sensory it's probably like a pretty big ratio and like you can like you can get pretty impressive results like controlling you know uh essentially a cursor with your brain is you don't need to buy me like electrodes for that so and there's a lot of neurons in the sort of motor cortex so I think there's some cases where you you could have all right this is just this is really speculation of course I mean some of some places where you could have maybe 10,000 won and some places where you don't want maybe ten to one it could probably I suspect it will vary quite a bit I just really quickly I think dance questioned businesses or longevity is a really important question we think about all the time I I don't think that we're releasing histology in the paper today but I think that that just to put some pressure on this team I think that we're running that around as a phone probably there's some really cool stuff in progress alright thanks I'm overcoming thank you great questions [Applause]",
    "status": "success",
    "error": null
  },
  {
    "video_id": "rVSb0u9OTtM",
    "transcript": "[Music] hey everyone thank you all so much for coming my name is gam I'm a software engineering lead at neuralink and today we're going to be talking about neuro Lake which is our internal data platform and the recurring theme through this presentation is going to be that we are building really simple systems for really complex data and simple doesn't mean easy and I really want to show what we've built uh over the past couple years so to start out I can introduce myself so I'm Gotham I'm a software engineering lead at neuralink for our full stack internal development team and our data engineering team so our team does a whole bunch of stuff it's all really interesting it's a really fun team to work on lots of unique verticals to drive we do data engineering some of which I'll talk about today we also do manufacturing software we do full stack development for lab Lab management tools we build visualizers for a billion pixel images um we have a lot of different things going on and yeah I can't even cover a tiny percentage of it today so if you have more questions please feel free to come and ask me afterwards so before we talk about neural Lake let's talk about neuralink so I'm sure some of you have heard about neuralink in the news um but for those who haven't so neuralink is a company creating a generalized brain interface and what we mean by that is a generalized IO interface into the human brain that can read and write signals from the brain and the reason we're doing this is to restore autonomy and help those with unmet medical needs there are millions of people in the world that suffer from not having their medical needs met people with spinal cord injury people with blindness people with ALS and more and what we want to do is solve these ailments of the brain and of the body with a generalized brain interface and in the long term we really want to unlock human potential for what it means to be a human and really unlock what the human species are capable of so how are we how are we doing this well we have a stack um and the nuring stack is composed of three main components so on the left here you can see that we have our device is called the link and this is an invisible device that is surgically implanted into the cortex of the brain you can see that there is a chip it's a circular disc about the size of a US Quarter and that chip has long thin electrodes coming from it those threads are extremely thin much smaller than human hair extremely flexible and can be inserted into the brain with minimal um minimal invasiveness and these threads have electrodes on them that can sense brain activity so as neurons activate we have signals going up these threads into the chip and the threads are inserted via a surgical robot these threads the these threads are too thin and too flexible to be inserted by a human hand so we've built our own Neurosurgical robot to insert these threads avoid vasculature and move with Micron precision and once these threads are inserted and reading brain data we send signals over Bluetooth over to neuros Signal decoding software and this software can classify brain signals into something useful in the external world this might be moving a cursor on a screen this might be moving a robotic arm there are many things you can do once you have these signals on a computer and the purpose is to decode neural intent and again the purpose of this is to restore autonomy so we want to help people with unmet medical needs and two cases here that we have are um telepathic control of a computer cursor as you can see on the left for people who have motor impairments and are unable to move their arms in a way to control a computer we want to allow for telepathic control and this year we did start our first clinical trial and we have one person who is actually benefiting from um this therapeutic case and another example of unmet medical needs that we are hoping to address is the ability to cure blind you can imagine if you have a perfectly working visual cortex but have other issues with your uh retina or optic nerve that prevents you from being able to see we can send signals directly to the visual cortex and stimulate the brain and induce Vision so both telepathic control and restoring Vision are two applications of this device and you can imagine because our brains make up a core part of who we are and what we do um there are many many many applications for helping people around the world so let's talk about data at neuralink and you can imagine there's a lot of data involved here right we have data from the brain we have data from our devices and we have a lot of data to manage and the data is incredibly complex so neuralink the neuralink stack I just described is one of the most complex medical devices to have ever existed and we have correspondingly complex data we have of course the brain signals and the signals that come from the device we have Telemetry coming from the device that's that's um safety critical that's indicating um whether the device is functioning properly we also have the brain signals themselves and we also have manufacturing data as we manufacture our devices we have information about the manufacturing history and machines on the manufacturing lines we have histopathology data that is large billion pixel images of tissue that um we use to assess the safety profile of our devices we have data collected during surgeries to make sure that our surgeries are safe and effective and more this data is incredibly multimodal it's incredibly complex we have images we have video we have huge images we have logs that uh that end up being terabytes large so this data is very very very complex it's not just simple tables or simple log stores there's a lot of different modalities we need to address here and we have a really strong design Philosophy for our Data Systems we want to have simple systems and again simp Le doesn't mean easy but simple means that we want systems that can scale down to a single developer machine and up to stateless clusters the development experience for a developer who is building the system should be seamless running something on your local laptop should be the same as running something in production and we want to really prioritize the local development experience instead of having large distributed services that we need to put up like a spark cluster to be able to access our data which runs in a jvm and can run slow on a local laptop we want to make sure that we prioritize the local development experience by using composable libraries and we want to avoid large stateful distributed clusters for our data lakes and warehouses we don't want to have a a database cluster that we need to maintain in order to store and access our data and finally we want to have code that's a catalog in and of itself and what we mean by that is we want to be able to generate a catalog that contains all of our data sources so internal users can access this data and Discover it and we don't want to have a database server and like all this complexity to maintain when we have this catalog application we want to just be able to define a table in code and have that generated catalog so you can see the Simplicity here from the end user experience internally when users access our systems or when developers build the these systems they can replicate uh production environment locally they can build a system easily using composable libraries and end users can access data without us having to maintain a bunch of different services so there's three main prongs that I'm going to cover here today we're going to cover ingestion Discovery and access so ingestion is the ability to ingest data into our systems so we want to have low latency streaming ingestion pipelines and we want to have an elegant way to handle schemo versioning and backfilling of our data lakes and as many of you can guess we we're going to talk about Delta Lake a little bit as a part of this we also want to allow for Discovery now with all of this complex data it it's really hard for people to know where all the data lives no one person knows where all the data lives in their heads right they need to be able to organically have a certain query or certain problem they're trying to solve and then look at our or look at our catalog and find data organically so we want a simple data catalog that's generated from source and we want low code dashboards that people without software experience can build that are generated from our catalog definitions again a single source of Truth for all of our dashboarding data cataloging and so on and then it should be easy to access now a software engineer can of course write a query to a Delta table and get get some information but we want everybody at the company to be able to access our data so our goal is to have a single click to get a data frame you click a button copy some code and you have a data frame on your computer that you can you can play with and query we also want to create autogenerated SQL and rest apis from our data catalog so that our internal dashboards can query these apis and again using the single source of truth of the data cat catalog so this is a 32,000 foot view of what our neural Lake data platform looks like um and this is really just a part of it again I can't cover all of the complexity of our data um today but we have data sources we have offline batch writers and low latency writers for processing that data and putting it into our data data stores and we have our data stores themselves we have Delta Lake we have relational stores uh and so on and something you'll see from this talk is that our our data stores can be spread across multiple different instances so we can have data in Google Sheets if we have data in spreadsheets we can have data in Delta Lake we can have data in postgress tables if we have high transactional needs which we do and all of this data is then easily discoverable and accessible via a data catalog via a python client and Via visualization tools and what you'll see is that the the catalog the python client and the visualization tools all come from the same Source there's a single source of Truth for defining these and then everything else is autogenerated which makes maintaining these systems very easy so first I'm going to focus on the data sources the low latency writers and how that data ends up in our Delta Lake stores and there's a lot more to talk about with offline batch writers and so on so if you're interested in those use cases find me afterwards so before we talk about our data ingestion let me just give you some primer on our Technologies we use so we have Delta Lake and we use the library Delta RS in our realtime inje pipelines so what is Delta Lake I'm sure a lot of people here are familiar but for those who aren't Delta lake is a open source project and is essentially a file protocol or file format that allows you to have acid compliant trans actions on top of blob Stores um a lot of data Lakes before Delta Lake were just parket files on S3 or on um a different blob store and the downside to that is you can't really get any kind of assd compliance across multiple files right you can't have transactionality across multiple files because these systems only allow for atomicity for a single file and you might might have partial rights if clients are reading from multi multifile stores while you are also updating multiple files at once and we don't want that so with Delta Lake we have a transaction log and park files on S3 and as long as the client and the writer both uh follow the protocol which means that they both read from the transaction log before pulling the paret files we can ensure that all of the Delta rights are shown in a transactional way so you'll you'll never have partial rights seen by the client and the Delta l prot is implemented by Delta RS so Delta RS is a rust library that implements the Delta Lake protocol it allows for writing and reading of Delta Table stores so the Delta RS library takes care of both reading and writing the Delta Lake updating the transaction log doing Atomic commits across multiple files and so on um the Delta RS library is written in Rust it's incredibly performant and it has uh it's highly concurrent and it also has python bindings so you can import the Delta Lake Library uh in Python and you can run it locally on your machine you can run it in a stateless cluster and the developer experience is the same so we're going to talk about a use case for Delta Lake we have multiple Delta tables each of them have slightly different use cases at the company we're not going to cover all of them but I do want to cover one interesting use case that we have so this is real time or close to real-time data ingestion into our Delta tables so on the left you can see that we have the and one implant being used um and while it's being used we have data being emitted we have events being emitted from the device these may contain neural signals and we also have events being emitted from the usage of a computer so the individual using the neuralink device is going to be playing a game on their computer they're going to be moving a cursor in some way and information about that cursor and about uh their neural signals will be being sent um into an ingestion API and this ingestion API stages these events onto a message queue then which are then consumed by a writer in batches written to Delta Lake stores and downstream those are consumed by rust based query engines and the purpose of this is to train a model to better calibrate to the neural signals by a user so as we have neural signals leaving the device we also have events on the game itself from the cursor movement and we want to align the time stamps of these and create labeled data that can be trained on so this whole pipeline uses Delta Lake as the main store for the um label data which is then queried via Delta RS or um uh other rust based query engines and that data can then be trained upon and this whole pipeline makes it so that the label data is available in seconds so in a few seconds we have all of the data available for training and this is is updated live and the reason we want such real-time availability is because during a session our Engineers might train multiple models and we want to be respectful of somebody's time if we're we're engaging in a session with them and we want to make sure that we can train a model as quickly as possible to try to converge on an optimal solution so let's really focus on the message cu the writer and the Delta Lake instance so we're going to talk about how the writer actually writes to the Delta Lake instance while consuming from the from the message CU and how the endend flow works for data to arrive in the Delta Lake store so the neural leg writer instance has three primary processes we have a writer process a compaction process and a vacuum process now these three processes each have a different function the writer process as you might guess consumes batches of messages from the message q and writes them in batch to the Delta L table now there's a problem here we have messages arriving very rapidly um in real time and if we write a new right to Delta Lake for every batch of let's say like 10 messages then we'll end up with a lot of small files in our Delta table and if we have a lot of small files in our Delta table then we have a problem because Downstream readers need to read a bunch of small files on S3 and that means that we have high latency for our reads because we need to do individual get requests for multiple files so instead of doing that we have a compaction process that once files are written to S3 at some Cadence right now we have it configured to be once a minute um but this can be tuned we have a compaction process that reads all of the files within a partition and compacts them into a single file now in Delta Lake this is possible because again we have assd compliance across multiple files this wouldn't be possible POS if we had a bunch of just park files on S3 right so in this case we read all of the files that are uh in the Delta table compact them and write back a single file and now when clients read that single file um it's uh it's it's very fast to be able to um get that single file into into memory now there's a problem though so with a compaction process running alongside a writer process we need to make sure that all of the commits themselves are serialized now remember Delta lake has a commit file and it has parquet files and both readers and writers access the um the the commit file the transaction log before they access the parquet files now if two simultaneous processes are trying to write to the same file you could have a conflict and this could lead to data loss right so we need to make sure that we have a serialized commit even if our rights are happening simultaneously so the compaction process takes a long time it takes some time for compaction to run but that can run in parallel while rights are continuing to come in and once the compaction is complete and the compacted file is written to S3 we can then serialize a commit and commit to our transaction log and we do that by we have a there's a custom put if absent semantic in um in the in the Delta RS Library where we use Dynam DB and we take a lock on on the um on on the commit file itself and once we take a lock on that commit file we can then commit and then release the lock and then the other process can can write and there's some more complexity here because there uh when a process has a lock for a critical section um it takes a time to live for that lock and while holding the lock a writer Can it can crash right or it can pause indefinitely for any reason you know you could have other processes on the machine then you could be thrashed by the scheduler on the OS and so on so when this happens we need to have a custom put if absence semantic with a repair mechanism on the Dynam DP lock itself to make sure that if a process crashes or pauses while holding the lock we don't have um we don't have data corruption um I have a link in here for the actual for another talk that describes the put of absent semantic in detail so I recommend you all take a look at that and if you have questions on this please find me after so that's how data gets into our system we have writers we have low latency we have uh High latency batch writers and both of these kinds of writers are writing to our Delta stores and all of this data is available so let's talk about data Discovery and data access so like I said we have a data catalog we have a neural likee python client and we have visualization tools so all of this allows internal users to be able to access our data without needing to ask a bunch of people where data is stored or know all of the different Delta tables all of the different Park files that lie in uh on our blob store and so on we want to make data access really easy internally at the company so I'm going to start off by talking about the Nur Lake python client so the Nur python client um has a very simple structure we have a catalog which can have many databases which Each of which can have many tables so this is a very standard uh data catalog structure and a table is really just a way to get a data frame via the neur like python client so it's just an abstraction to be able to get a data frame and when we use the python client it looks something like this we have a very clean API we have the ability to get a data frame for a specific database for a specific table in this case this table is called normalized band power you can pass in some filters and you can collect the data frame and the return data frame is actually a subass of the Polar lazy frame so polar is another rust based data frame library and polar is very similar to pandas but much much more performant um because it's written in Rust it allows for very very powerful um concurrency and it is able to process data very quickly and the lazy frame allows us to LA execute these data processing queries so queries are only run once collect is called and evaluated lazily and we've subclassed that so every table returns a Polar Polar lazy frame that can then be executed on and something we have planned is to actually do push down queries where we actually filter on the parket files themselves before even pulling them over the network so we can lazily evaluate and op even our Network calls so how do you add tables like so you saw that table it was uh normalized band power right here right so how did somebody add that well it's super easy so all a developer has to do once they've set up their data inje pipelines once they have data being written is they just have to write a single line of code they we have declarative syntax that looks like this where they can declare a paret table or declare a Delta table we support both uh and they can essentially just give a name the URI the partitioning scheme and some documentation information for the documentation that's automatically generated so this is a single line of code it can take a developer maybe a few minutes to write and their table will be available in the catalog we also have custom functions that can be a part of our catalog so you might imagine a view that involves consuming for multiple Delta tables or multiple paret tables um you might have a function that Crees a Google sheet or a relational database and get some data back we want to make that data also available for internal users so in this case we have a function it's called robot insertions the doc string is automatically pulled into our documentation and all they have to do is add the at table annotation at the top and once they add The annotation that is automatically registered as a table in our catalog and available inside of our catalog it really could not be easier and the catalog looks like this so you can see we have an autogenerated catalog we have all of our databases all of our tables and internal users can just go onto our internal site they can access this catalog this catalog does not hit a database to get its um information this catalog is a static site that's generated um in our C and deployed and does not have um any any kind of backing to it it's autogenerated from the code that you saw earlier and in the catalog we have the view of a single table and you can see here that we autogenerate the code for the python client as well so an internal user can literally go to the catalog browse a few tables looking for the data they want once they find the table they can just hit that copy button they can go to jupyter HUB which we host internally and they can just paste that code into a jupyter notebook and sorry and once it's in the Jupiter notebook the um the code automatically can be executed and the jupyter Hub instance is already authenticated to S3 it's already authenticated to the buckets it needs to access so an internal user doesn't need to Wrangle with credentials or logins they don't need to they don't need to worry about where the DAT is stored just copy paste analyze data so in addition to our catalog we have readon apis that are also generated from our table definitions so when developers Define a table in Python we create readon apis and these readon apis accept HTTP SQL queries they also accept the post wire protocol and this means that dashboarding tools like grafana can be configured to point to R readon apis so just to really emphasize the Simplicity here a developer all a developer has to do is add a single function like this or a single line of code a single command like this to Clare atively and that is already available in the catalog automatically and it's also available in the dashboard the developer doesn't have to do anything else and internal users can then set up their own dashboards for this data source so I want to Deep dive into how exactly this works and at the center you can see that python client really is the center of all of this the python client which contains the table definitions generates everything you see here and no additional action is needed from the developer again from the from a developer experience this is simple I just add a function I don't need to worry about anything else so the python client itself uses polers and P arrow in order to query the Delta tables and the parquet files so with a single declarative command a a developer can create a table definition and we've already written the code to automatically configure that to be a pi Arrow query that fetches from uh from S3 or from whatever data source you have in mind and the result is a polar's lazy frame that we have that lazily evaluates the query once the user actually wants to do something useful with it and once that table is defined in the catalog then we generate the catalog website and all we do is we take the table definition that you saw where we have the annotated function or we have the declarative Syntax for that table and we just serialize that to Json in CI so once you merge your code to to the main branch we serialize the result to Json and then we generate the static site and we deploy the static site it's easy no database to manage for the catalog nothing the site is up and the catalog is super responsive and doesn't require any additional maintenance and users can query use the python client directly so internally somebody wants to just pip install neural Lake they can do that on their own local machines they can use Jupiter Hub Jupiter Hub comes pre-loaded with a Nur Lake Library they don't need to install it on their notebook it's just everything is just ready one click to copy one click to paste one click to run very easy and in CI we also autogenerate a row API config so some of you might have heard about row API but for those who don't know row API stands for readon API and uses Apache data Fusion which is a um which is a query library that executes um executes logical plans on data and is written in Rust and what we do here is we generate a row API instance and row API accepts HTTP requests containing SQL it accepts graphql queries and it also accepts the Press wire protocol so you can literally configure a PC cool client connect it to our neural Lake stores which is autogenerated from a single line of developer uh a developer wrote to add a table and you can pretend like you're interacting with a postest database so that is incredibly powerful and Ro API stateless you can scale it up scale it down just by adding more uh more instances and Roy API allows for really fast querying and then grafana the dashboarding tool can simply plug into Roy API because Roy API takes HTTP requests um containing SQL it can be configured as a grafana data source and the grafana dashboard can then just be built by users so if I'm a developer and I just created for example a new way to ingest manufacturing data from our manufacturing machines on our line and all of this data is being streamed into my Delta lake table all I have to do is add a single line and I can tell the manufacturing team hey you guys get dashboards you guys get jupyter notebooks you guys get an HTTP API you can access data from and it's all really easy people who aren't software Engineers but know a little bit of python know a little bit of scripting can use the python a python client they can use Jupiter Hub people who don't know any python at all can use grafana and configure their own dashboards and this just makes it really powerful for us to access all of our data internally so this allows us to build the best product we can because our data is so complex that we really need to be able to make data access at the company incredibly easy otherwise we would not be able to make good decisions and we would not be able to help millions of people so in conclusion the way we do this is we Define tables in code everything is defined in code One Time One command one function and we can generate a catalog API and dashboarding tools and we don't need to maintain a catalog database at all and read and writs both scale down to a laptop so you can run the neural client on your laptop you can if you're developing a writer you can run that on your laptop too the neur lake writers that write data to Del Lake can run as a single process on your laptop very easy there's no overhead of a jvm there's no overhead of trying to run spark locally and this means that we can also scale it up to a stateless cluster and these clusters scale right because they're stateless you can just add more add more instances and this flexible design makes it really easy to extend all of our systems so you can add offline batch processing you're not limited to just Delta Lake and real-time event uh ingestion you going to have B offline batch processes running an airflow which we do you can have any kind of backend for your data you can have a Google sheet and just have the neur lake python client read from that Google sheet if that's useful to you you can read relational data we have relational databases we have highly transactional workloads that we need to have in relational databases we don't want to exclude those from our data Lake and all you have to do is just add another line to your python client very easy and the rust based systems that I talked about so polers Apache data Fusion Delta RS really allow for high performance data access um writing code in Rust means that you can write write code with high concurrency without worrying about race conditions because of the inherent safety of the language and it means that we can build systems that are composable libraries that can just be imported into a developer machine and then run in production and scale up and down without needing to worry about the overhead of larger systems and you know the the real uh Champion here is Delta l which allows for us to do all of this because it allows us to have asset transactionality on blob stores we can have commits across multiple files on our blob stores and we don't need to maintain a database server we don't need to maintain a big cluster that stores our database that we need to maintain so I do want to thank the Nur Lake team so we have some of the most brilliant people that I've ever worked with in my career on this team and they've contributed immensely to this effort and I feel incredibly privileged to be able to work with people like this every single day I think we've built a really unique culture at our company where we have a really high bar for talent we have really talented individuals who are incredibly mission-driven and determined to help millions of people around the world and who are interested in building simple systems to really manage our uh really manage our data and again simple isn't easy and everybody here has worked incredibly hard to build these systems and finally uh before we go to Q&A I I want to leave you all with some quotes um these are from uh participants in our clinical trials we have started clinical trials in the United States as of this year and there are people that benefit from these devices whose lives have been changed by the by the use of our device and this is honestly what keeps me going this is what keeps most people at our company going is knowing that we can help people in this way and knowing that we can change people's lives and plan to change many many more people's lives one more quote and to be honest I get a little emotionally even thinking about it because it really is it really is a privilege for us to be able to work on such amazing engineering work and work on bleeding edge Technologies and also help people in the most fundamental and meaningful of ways possible and to really have that be the way I spend most of my waking hours is an incredible privilege and and I I I personally feel incredibly lucky and most of my colleagues also feel incredibly lucky with with the way um with the way to spend their time so if that kind of an engineering culture interests you if you want to help billions of people around the world if you want to work on these kinds of bleeding gadge Data Systems we are hiring so I encourage you all to go to ning.com careers and I will be available after this talk so please come and find me as well and I think we're out of time thanks everyone for all your questions great thank you all so much [Music]",
    "status": "success",
    "error": null
  }
]