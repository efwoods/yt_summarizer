[
  {
    "video_id": "CJn0WRKwg34",
    "transcript": "Hi, I am Brad Smith. I'm the third person in the world to receive the Neuralink brain implant. I'm also the first person with ALS and the first non-verbal, which means that I rely on it for all communication. I am making this video using the brain computer interface to control the mouse on my MacBook Pro. This is the first video edited with the Neurolink and maybe the first edited with a BCI. This is my old voice narrating this video cloned by AI from recordings before I lost my voice. I want to explain how Neurolink has impacted my life and give you an overview of how it works. I have ALS, a really weird disease that kills the motor neurons that control my muscles but not affecting my mind. My experience has been pretty interesting. Starting with a shoulder injury that would not heal and ending up with my current status. I cannot move anything but my eyes and I am totally reliant on a ventilator to keep me alive and breathing. My wife Tiffany is the best caregiver I could ever imagine. She does everything for me with only our kids and friends and family to help. She is the key to making Neuralink work. I will stop talking about her because she doesn't like the attention. Before Neuralink, I had to use an eye gaze control computer for all communication. It is a miracle of technology, but it is frustrating. It works best in dark rooms. So, I was basically Batman. I was stuck in a dark room. Neuralink lets me go outside and ignore lighting changes. So, how does Neuralink work? The implant is in my motor cortex, which is the part of my brain that controls body movement. The actual implant is the size of five US quarters in a stack. It replaced the hole in my skull. The threads are placed by a robot just a few millimeters into my brain, avoiding the blood vessels, so there is almost no bleeding. It connects to the computer via Bluetooth, and the computer does a lot of processing. This is a feed from the 1,024 electrodes in my brain. in green. It looks like the matrix. This is the raw signal that my brain is giving the computer. The computer has to decide what is important. Finding the signal and ignoring the noise. The Neuralink implant embedded in my brain contains 1,024 electrodes that capture neuron firings every 15 milliseconds, generating a vast amount of data. AI processes this data on a connected MacBook Pro to decode my intended movements in real time to move the cursor on my screen. Neuralink does not read my deepest thoughts or words I think about. It just reads how I want to move and moves the cursor where I want. How do I train the system? This is how I train the system to interpret my intended movements. I move the cursor to the bubble and hold over it until it disappears. The yellow targets are just for holding over and the blue targets are for clicking. We initially tried to move my hand to control the cursor, but it was not doing well. After a lot of research and mapping how body movements match the signals from my brain, Neurolink engineers found that my tongue was the best for moving the cursor and clenching my jaw was best for clicking. So, I trained the system with my tongue and it works much better. I am not actively thinking about my tongue. Just like you don't think about your wrist when you move a mouse. I have done a lot of cursor movements in my life. I think my brain has switched over to subconscious control quickly. So I just think about moving the cursor. This is web grid. It is the way Neuralink and others use to quantify how well they are able to decode intentions. I click boxes and it measures my score. If I click outside the target, my score goes down. I have reached a top score of five. My score was less than one before Neurolink with I gaze. You can try Web Grid on neurolink.com/webgrid and you can see how fast you are. Tools to make it work. This is the mixer which has tools to adjust how the cursor behaves. First is bias correction. Because the brain is constantly changing, the cursor drifts over time. So it is like the cursor moves in a random direction. This tool lets me adjust how the cursor drifts. This is a great reason why Neurolink is doing human trials because the monkeys cannot explain how the cursor drifts. Monkeys just give up when bias gets difficult and want snacks. They are like my children in that way. This is the speed control. It changes the speed that the cursor moves around. Speed also changes across different models and over time. Able-bodied people can adjust the mouse speed dynamically, and I can do that a bit. But the speed control sets the baseline speed for the cursor. I adjust it often. These are friction and smoothing that are more advanced settings that refine cursor movements. They might change as Neurolink develops the system. The last thing is the click stiffness, which adjusts how hard it is to click. Keyboard and parking spot. This is the virtual keyboard developed by Neurolink. I can't move or speak, so I rely on the keyboard for everything. It gives word suggestions for the situation and it pushes apps out of the way. I do a lot of different things with keyboard shortcuts. So, I created a keypad using the Mac accessibility keyboard to make some keystrokes easy to use. Things like select all, copy, paste, undo, and ways to navigate the page are all really useful. and I can use the toolbar and some shortcuts. The other feature I asked for is the parking spot. Sometimes you just want to park the cursor and watch a video. The first two participants could use voice commands to pause the cursor. I could not do that. So, I asked for a way to park it, so it would only come out when I wanted it to. I can go to the bottom right and the cursor jumps into the circle. When I want to get out, I have to hit the dots in a specific order and it jumps out. When it is in the parking spot, I can watch a show or take a nap without worrying about the cursor. I heard that the other two people love this feature. The chat app and communication. The hardest thing about ALS is thinking much faster than I can type. We have created a chat app that uses AI to listen to the conversation and gives me options to say in response. It uses Gro 3 and an AI clone of my old voice to generate options for me to say. It is not perfect, but it keeps me in the conversation and it comes up with some great ideas. My friend asked me for ideas for his girlfriend who loves horses. I chose the option that told him in my voice to get her a bouquet of carrots. What a creative and funny idea. We are also working on a faster way to type with the cursor. The standard keyboard is designed for two hands to alternate sides. We found a keyboard designed for a single finger or mouse, but I know the standard keyboard really well. Is it faster to start over and learn a new keyboard? I used Grock AI to make this app to train me on the new keyboard. I don't know how to code, but Grock walked me through it and wrote the code. So, I am really impressed. What does it mean for me? Neurolink has given me freedom, hope, and faster communication. Overall, the whole Neurolink experience has been fantastic. It has improved my life so much. I am so happy to be involved in something big that will help many people. I have enjoyed working hard with interesting people on important questions. I'm still trying to get faster at communication. I have spent the past few years with ideas and thoughts that I cannot share because it takes too much time to type it out. I can already communicate faster and in more ways than I could before, and we are still working on ways to get even faster. Like Noland, the first Neuralink recipient, I believe that God has put me in this position to serve others. I have not always understood why God afflicted me with ALS, but with time I am learning to trust his plan for me. I'm a better man because of ALS. I'm a better disciple of Jesus Christ because of ALS. I'm closer to my amazing wife, literally and figuratively because of ALS. I get to work with the brilliant people at Neurolink and do really interesting work all because of ALS and because we listened when the Holy Spirit told us to move to Arizona where Neurolink ended up opening their first site. Don't get me wrong, ALS still really sucks. But I am talking about the big picture. That is what I have learned. God loves me and my family. He has answered our prayers in unexpected ways. He has blessed my kids and our family. So, I'm learning to trust that God knows what he is doing. The big picture is I am happy. Tiffany is the greatest person I've ever known and I get to spend eternity with her. My kids are doing well, especially under the circumstances. And I can control the computer with telepathy. Life is good.",
    "status": "success",
    "error": null
  },
  {
    "video_id": "Ux5AiKmra-E",
    "transcript": "right welcome I hope this is working uh welcome to neurolink live update uh we're going to tell you about uh the progress of the first patient with neurolink umand and uh sort of do a recap of of the progress there uh then talk about what uh changes we were're making for the second patient which we're um uh hoping to to do an implant in the next week or so um and this is for our first product which is called tathy which enables you to control a computer or phone just by thinking so let's uh in fact so so we'll start off with just some introductions DJ want to start hi everyone my name is DJ H I'm an electrical engineer and a chip designer by training uh I led the design of first several generations of the neuralink implant um currently I was on the founding team and currently a president I'm Matthew McDougall I'm a practicing neurosurgeon and head of neurosurgery at nurlink uh yeah go ahead yeah head of brain interfaces applications and I'm Bliss I'm a software engineer at nink trying to figure out how to turn brain activity into cool stuff in the world all right thank you well let's see so we'll just get going into the presentation so our first product is something I said we call telepathy which is enables uh the uh person with a neuralink implant to control their phone or computer just by thinking um and once you can control your phone or computer you can essentially control almost anything uh just and literally just by thinking so there's no uh eye tracking or anything it is purely uh purely your thoughts uh so uh this is a like really qu quite a profound uh device that can help a lot of people who have lost uh the connection between their the brain and body so U you imagine people like uh Steph hulking who uh you know imagine if he if he could communicate at the same speed as someone who had still had the connection to the brain and body um so it's it's really uh something that can help millions of people around the world and it's a it's part of our our overall goal of enabling a very high bandwidth connection between the the brain and uh and your and the rest of the world and your computers um the long-term goal which sounds little esoteric is to at the uh the risk of of uh the civiliz civilizational risk of AI uh by having a a sort of closer symbiosis between human intelligence and digital intelligence but uh but that that that'll take many years uh along the way we're we're going to help solve a lot of uh brain injury or spinal injury issues U so U and without first product telepathy that's that's going to be really quite profound uh that there is also potential longterm for uh Bridging the the the Gap so if there are damaged or severed neurons being able to to expand the gap between the brain's motor cortex to the spine to enable someone to uh use their body again I think that would be very exciting um and it's you know that that is something that is possible in long term um and they're not our second product which we've demonstrated to work with the monkeys is uh Blindside which would enable someone who is completely blind or lost both eyes or completely lost uh their optic nerve uh to be able to see so that's uh that's something that we hope to demonstrate in the future so this just gives you a sense of what the device is uh a way to think about the ne device is kind of like a a Fitbit or an Apple Watch with with tiny wires or electrodes those those tiny wires are implanted in the in the brain and they uh read and write electrical signals so uh a lot of people think the brain is this incredibly mysterious thing it's it it is mysterious in a lot of ways but but it is actually U it does operate with electrical signals so if you can read and write those electrical signals uh you can interface with the brain and um the the device is is sized so so that it is the same size as the as the piece of scull that is removed so if it's like a few centimeters diameter of SK skull that's removed we replace that with the device uh after implanting the the tiny wires with the surgical robot and that enables uh read R capability to the neurons completely wirelessly yeah yes exactly um it's completely wirelessly so like I could I could have a neural link right now you wouldn't know um and it it charges inductively so you could just uh basically have a electromagnetic pad that that that you charge the device with so yeah it's like an Apple Watch exactly so uh except that it's actually a much harder technical uh challenge to solve given that there's limit as to how much eat the brain tissue whereas in for phones and you don't actually really care if it's sitting on a table sure uh yeah it's Al it's got to go through skin and stuff as well in our case so it's it is a tougher challenge to to charge and to uh have high band with Communications given that it's got to go through uh skin and hair and stuff we have solved it but we have solved it yeah so yeah yeah so our first step with the telepathy is basically to unlock a digital Independence for people uh with paralyses and to allow them control the computer just with their mind without moving their body and uh our goal is to provide them the same level of control functionality and reliability that I have when I'm using a computer even better than uh the level of control I have it's not a high bar for you actually just to be clear this guy he's controlling this with his brain so he's not like you can't see his hands in this video but he's not using a mouse and keyboard just you know thinking about how to move the cursor and playing civilization no ey tracker right there's no ey tracking from he's liveing like you watch this on Twitter just thinking that's it just thinking this just a couple days cursor move here yeah yeah yeah this like last night or two nights ago something yes yeah I think I think the way he also described it is he's using the yeah he has many more videos on his uh yeah on the platform definitely check them out um yeah so he can he's streaming that live and also can talk and like move his head without problem mul yeah you also like if you join this live stream you can ask him questions he'll he'll tell you all about what it's like to move also I think U I haven't played civilization myself um but I think this is actually not easy mode this is expert this Emperor mode Emperor mode if you have played s Emperor mode is like the highest difficulty level just the point is like this is a ctively demanding task while live streaming playing the hardest mode of the game and uh he's able to do that while moving talking engaging with uh you know the audience while playing one of the other games he likes to play a lot is chess and I think it gets lost sometimes that he's actually playing speed chess against me yeah which requires an incredibly High Fidelity degree of control and and speed of control in order to be able to win so also another cool stuff about about our device is that you can use it anywhere anytime also on a plane during a flight while uh creating really cool memes of c um also our device unlocks things that previously were impossible for our participant for example uh we're able to connect him to his gaming console switch and play Mario Kart with friends and family and it was lovely to see them playing together after years uh that he couldn't do it since he's injured imine if you're sitting one ra over from this guy on the plane look over he's making a cat M No Hands no movement live in a world world yeah it's strange strange time yeah and uh he loves using the device and using independently daily to watch videos uh read uh play games uh using the browser and the key metrics that we care is to make sure our device is actually useful is to is basically the amount of hours we use the device daily and weekly and we track it uh weekly since the since the surgery and on weeks that he's not too busy and not traveling he can even reach 70 hours uh of using the device is a week and this is amazing um he would of course love to use it more but need to run resarch sessions uh he needs to sleep sometimes and also of course to charge the device once in a while hopefully we'll improve that over time I think maybe not obvious to people who are watching this like it's a normal MacBook he's controlling this isn't like some limited edition thing where there's only a few options like he can just do anything that you can do on a MacBook Pro same one I have on my desk actually it's the exact same one and maybe another interesting point is that on the first day he used BCI turn control he was able to break the previous world record for cursor control just by uh uh using the brain and recently he even doubled it and was able to uh outperform about 10% of our engineered neuralink and you can be sure that we are very good in this game and very quick and if you want to check out how well how well you can do it you can do it on our website and it's very uh addictive games yeah it's a very simple game you just have to click on the Square but uh but it's it's it's it's actually even though it sounds silly it's it's quite a yeah it can be quite it sounds like it can be quite addictive and it's especially if you get a low score and you think there's no way I got to so I I mean any anyone who wants to try this I recommend going to the neurol link.com website and seeing seeing if you can beat Nolan's record and it's that you will find that's actually quite difficult to do so um and this is really with version one of the device and with only a small percentage of the uh electrodes uh that that are working so this is uh this is really just the beginning but even the beginning is twice as good as the world record this is important to emphasize um the you know the media has a habit of of uh saying that the glass is 10% empty uh but but actually it's 90% full so uh I think it's really quite an accomplishment of the nuring team to um have achieved with ver with the first patient the first device uh uh twice the world record for the uh brain to computer uh bandwidth that's a really an astonishing an amazingly great outcome um and it's only going to be get better from here so the potential is to ultimately get I think to megabit level um so that's that's part the long-term goal of of improving the the bandwidth of the computer interface if you think about like how low the bandwidth normally is between a human and a device it's the average bandwidth is extremely low it's I say less than one bit per second over the course of a day so if there are 86,400 seconds in a day uh you're outputting less than um that number of bits to any any given device except in perhaps very rare circumstances so uh the this is um actually quite important for um for AI you know basically for for human uh AI symbiosis is just being able to communicate at a at a speed the AI can follow so yeah just to emphasize again he's performing at this extremely high level with about 15% of his channels functional um and so we want to mitigate any of the problems that led to that situation so you know the brain is a fascinating organ uh I'll share with you some of the secrets about the brain during any typical brain surgery a small amount of air is introduced into the skull um that's because neurosurgeons like to have as much room as possible around the brain and so uh there's this little known control mechanism of allowing the CO2 concentration in the blood to rise a bit uh which allows the brain to either expand or contract depending on where you target that CO2 so typically neurosurgeons will have the brain shrink by lowering CO2 what we're going to do in our future surgeries is keep the CO2 concentration actually quite normal maybe even slightly elevated that'll allow the brain to stay its normal size and shape during surgery that should eliminate this air pocket that we saw in the the first participant that air pocket we think may have contributed to eating up some of the thread slack uh as as the air bubble migrated to be under the implant push the brain away from the implant and so that's easy enough to fix another consideration that we want to focus on for our upcoming participants uh is that the brain think of it like a really complex folded onion it's layer upon layer of sheets of neurons all over the surface of the brain folded into this um you know oddl looking shape the folds of the brain travel down deep into the brain and and along with it Go those onion layers of neurons and if we insert very close to one of the folds where there may be very useful information encoded in neurons we may end up traveling with our threads parallel to some of the layers of neurons that were most interested in avoiding them entirely uh to avoid that possibility we're going to insert uh in our future participants more close to the middle of the apex of the folds uh ensuring that we're crossing the layers of Interest layer five of the CeX I also think that it's important to um highlight here those tiny wires that Elon mentioned uh they're they're fraction of a human hair they're very flexible uh intentionally so because you know brains constantly moving and you want the electrodes to be moving with the brain causing less of the scarring and um it's it's actually impossible uh for a human neurosurgeon how however talented Matthew is to actually maneuver them by so we have a surgical robot that we built that can actually precisely Target them in any threedimensional space XY as well as Z with Micron level Precision while avoiding vasculature so that you don't disrupt um the the and and cause immune response from happening so uh we we actually have the technology to be able to place them exactly where we want them in yeah it was truly amazing to see the surface of the brain after the robot had inserted all the electrodes on the first participant without a drop of blood in sight um is really quite an achievement yes so something that probably most people don't realize is that the the brain appears to be sort of somewhat undifferentiated so if you look at the cortex it looks like a whole bunch of folds that were you know maybe like it's it's it's not obvious just looking at a say a picture of the brain that uh that that it's the brain is highly differentiated that there's you you you pretty much know exactly where the part of the brain is that uh controls your right hand and your left hand and your leg and that that kind of thing or or Vision it's it's actually U quite precisely located it's not uh some people like might might think look at the brain like oh could be could be anywhere but but actually we it it's it's your brain is is highly differentiated even though it doesn't look it's yeah do you want to describe how we actually we like how we identify where the drill the yeah so we can we can put a patient that is considering this implant uh into an fmri so a functional magnetic resonance imaging machine and ask them to imagine hand movements that you know because of the spinal cord injury don't happen but just imagining those hand movements causes these areas of the brain to light up in the fmri scanner and so we have a pretty good idea based in in fact for each individual participant which part of their brain is going to um you know respond to imagined movements of the hand and so we can map those imagine movements much as we all do uh when moving a mouse to controlling a cursor on a screen even without the use of a mouse yeah but anyway I think this is kind of an important point that like it's not like your the part of your brain that controls your hand might be anywhere in the cortex it's this is not the case it's going to be in a very specific region and it's going to be um extremely common across people Precision is key too yeah um the left-handed right-handed my mind too like if you're right-handed you want the device on the left side yeah lateral side to the hand that's your dominant yeah the left side of your brain controls right side of your body yeah everything's CR yeah another of the risk mitigations we're looking at in the future yeah is that you know that the implant has a certain size the depth of the bottom of the implant is actually thinner than the average human skull and so what we want to be able to do is control the size of the Gap under the implant give the threads that travel from the implant into the brain as much slack as possible uh we didn't do this in the first uh participant because we didn't want to you know manipulate any of their issue that we didn't absolutely have to in upcoming implants our plan is to of sculpt the surface of the skull uh very intentionally to minimize the Gap under the implant such that the bottom of the implant travels perfectly flush with the normal Contour of the inner side of the skull that will put the implant closer to the brain it will eliminate some of the tension on the threads and we think it will reduce some of the tendency of threads to retract brain and we actually built a tool to do right yeah this this is actually this is a very important detail uh you really want the the inner Contour of the skull to be flush so the implant there's there's no the brain doesn't want to Puck her up into the into the Gap that's really quite a big deal so like like minimizing the air pocket um and the implant being flushed with the the the inside Contour of the skull is are two very important uh improvements the additional benefit here is that uh you know you do see some amount of stick up what we call stick up so you minor bump in the head but this actually eliminates it even further yeah yeah I mean it's like really our goal is that that if you run your hand over the top of the skull you don't feel any any bump you don't feel any any device um and that even if someone was bold you wouldn't really even notice it um and uh and then from the the in inner cont of the skull that the the brain from a physical standpoint doesn't really notice that there's a divot in the skull mhm because there's no divot okay another aspect of uh of the human brain that you know obviously differs from any of the animals that we tested in uh is that the human brain is a lot bigger and so you may not realize that that means the the human brain moves quite a bit more uh than uh any of these other smaller brained creatures and so when we open the skull uh we see the brain travel toward and away from the robot about 3 millimeters in total as the heart beats and and the breathing takes place and so that movement uh you know it it adds a small challenge for the robot uh in precisely choosing a depth to insert each thread it's not an enormous Challenge and we've already upgraded the robot capabilities to be able to even more precisely Target depth in in even a very rapidly moving brain uh with a high amplitude of movement you may think the most obvious mitigation for Threads that pulled out of the brain is to insert them deeper we think so too uh and so we're going to uh broaden the range of depths at which we insert threads so you know for the very first participant we had an enormous amount of data from our animal work and we had very highly optimized our insertion depth to maximize uh the crossing of layers of interest in the cortex with the electrodes that we're recording from now that we know retraction is a possibility we're going to insert um at a variety of depths that even in several cases of differing amounts of retracting threads we're going to have electrodes at the proper depth and with the deepest threads be able to track how much retraction has occurred across the surface of the brain um from from each thread and so we're going to you know both have more threads in the right layer and have better data on how much retraction has occurred if you're a BCI nerd you might know that being able to control individual Z depth per thread is not something that most uh neural interface devices offer most neural interface devices are kind of a static fixed rigid array that you push in and all the electrodes are at one depth right to be able to do this is actually pretty pretty novel part of the robot yeah the historical approach is to actually pound in a sort of bed of nails with an air hammer into the brain it looks crazy that that that is yeah just with a with a Pneumatic Hammer that's the that's this is it sounds somewhat barbaric this is not what we do but this is the what's been done before is literally just hammering in what looks like a better Nails into the brain which actually works it's astonishing that it actually works but I mean some people like manually like DBS probes you're just sticking in by hand search is just guiding them in those are several several orders of magnitude more volume of brain tissue that you're destroying compared to what we're doing but that deep brain simulation stuff does actually work it actually helps people a lot Yeahs of thousand yeah yeah that's a great product yeah but I mean I think we'll we'll be able to do um a much more finessed version of that down the road um so uh I mean it's really difficult like the the the neuralink device is something that really absolutely minimizes damage to the brain absolutely minimizes the load on the patient um and the goal is to allow someone to live a completely normal life um they they you won't even notice that someone even has the device um so like I said restoring the ability to control your computer and phone that's telepathy and then next device being able to allow people to see that could not see before in fact you could you could allow people to see kind of like dordy Le Forge in Star Trek in any what whatever infrared yeah infrared ultraviolet um radar um so so so I think another way of saying it is that we want to give people superpowers so it's it's not just that we're restoring your prior brain functionality but that you actually have functionality far greater than a normal human that's a super big deal MH and and I also think you know often times the questions that we get a lot is why do you have to actually go into the brain what if you place it on the surface or outside the skull basically the long story short the physics of how it works you really need to get the sensors which are these placing in the brain next to the source which neuron as close to it as possible otherwise what you get is you get a population response and not be able to kind of do the level of controls that we believe of yeah I mean May uh a good sort of analogy would be like if you're trying to understand what goes on in a factory you kind of need to go into the factory you can't just put a stethoscope on the wall um and try to figure out what's going like anything on the outside of the the trying to read things from the outside is like put putting a stethoscope on the wall of a factory trying to understand what's going in the factory it's not going to be effective be in you got to be threads are got to be in there um so um but I just want to be emphasize again like the goal is to give people people superpowers um not not just to restore prior functionality so I that's very exciting um and I think that should give hope to a lot of people in the world that the future is going to be exciting and inspiring and uh the technology is going to give them superpowers I mean that's that's amazing so yeah I guess these off yeah could can you multitask with it yeah in fact if you look at Nolan's streaming and you can just uh check out Nolan streams on on the xplatform um he's multitasking all the time so he's playing video games while talking and uh listening to podcasts listening to podcast yeah yeah exactly so uh it's it's really just like if you you're using your hands and you you you can be you know playing a video game while talking so I mean don't take a word for it there just go watch I mean yeah he's out there on the internet doing his thing yeah yeah exactly uh so can you do keyboard shortcuts or is it just the mouse yeah that's actually what we're working on right now oh sure So currently he's walking the mouse but we are also exploring recording more Dimensions out from the new activity multiple clicks uh so to do shortcuts or just able to control more games like control Co games with an Xbox controller uh but also in the future we expect we plan to expand to decode uh text not just the mouse control but also allow our participant to type much faster and yeah yeah actually so maybe uh going back to the discussion of thread retraction you know one of the very exciting parts to me about this story is that we're able to do so much with 15% of channels when you have more channels what that actually offers you is not just faster Mouse control because in the motorex neurons don't all represent the same thing so if you're trying to understand like uh you know what an individual finger trying to do uh you might or might not have an electro next to it and the more channels you have in the brain the higher likelihood you have know representation or decodability of all fingers on the hand and so if you're trying to do something like output text at a fast rate is something that matters a lot for people who are completely locked in who cannot speak at all who are trying to you know just say I love you to them to a loved one in their family or ask for a glass of water or a scratch or whatever you know being able to type it a fast R it's extremely important and the more fingers you have access to higher probability you can do that efficiently and so yeah you know I'm super excited about high how high the ceiling is we can uh that we can get to as we resolve this dat retraction issue yeah I mean we're C we're currently at approximately 10 10 bits per second uh PE great but uh ultimately we want to get to a megabit uh and I think say ultimately whole brain interface I think you know many years from now I think gigabit level is possible um so that's uh that's pretty astonishing um now you know with there's a still version one about device as we mentioned it's version one with only 15% of the threads working U the current device has uh 64 threads with 16 electrodes on each thread um our next device has 128 threads with with eight electrodes per per thread um because as we get more confident about how where exactly to place the uh the electro the thread you you need fewer electrodes per thread so we can essentially with the current Dev without substantial changes I potentially double the bandwidth if if we are accurate with with the placement of of the threads um and then our next Generation device will have maybe even more channels yeah yeah of year yeah so our next device we aiming for yeah uh 3,000 channels uh so this will just keep getting better and better uh really moving up I think in or magnitude in factors of 10 basically and what kind of bandwidth so I think won't be it won't be all that long before uh someone with a neuralink device can communicate faster than someone who is has a fully functional uh body and yeah so uh I think you know faster than the fastest speed typist or auctioner the Esports tournaments are going to be like won't be able to speak faster than someone can communicate with a a neuralink leftly device maybe a very interesting part of this basically we currently um uh connect to standard uh inputs to the computer through Mouse and keyboards but very soon as we will have a much hard bandwidth we need to think about new ways to actually build the interface for the devices this is something that we very excited yeah no that's that's a good point um because the the current um input devices are centered around human hands yeah so it's like we've got these you know little meat sticks that we move and um the this certain rate at which you can move your little move your fingers and and so we've got like The Mouse and the keyboard and or the joystick control you know like Xbox controller or something like that uh but you really don't need that you can actually uh you don't you don't you don't need since you're no long if you're if you're not trying to use your hands you don't you actually don't need those uh conventional uh control mechanisms um and so this is why like ultimately I think you'll be able to do uh conceptu ual uh telepathy like where you can communicate entire Concepts uh uncompressed to someone else with a neuralink or to the computer even today we have some problems here where like you know if you don't feel the mouse clicking under your finger how do you know it actually happened you know you're you're seeing it on the screen but you don't actually feel the mouse click you don't have the appropriate accepted feedback of you know the keys under your fingertips or the trackpad under your so there's all sorts of interesting ux challenges how to actually give the user some sense of what their decoder is actually doing what they what the earing is actually doing when they're trying to use so Wireless yeah it's Bluetooth um it's just a Bluetooth connection just like how your normal Apple mouse or like apple magic keyboard connects to your computer same exact thing in fact in yeah we can basically have this exposed as an HID interface if we want hiid is just the name of the protocol for like sending bits from a mouse into a computer uh yeah I plug into basically anything yeah yeah I I think we we chose that interface because it's ubiquitous basically any devices are are have Bluetooth capabilities our our long-term goal is to actually have our own protocol you know that is safe and secure U but for now you know we've chosen it for interoperability so the question is can a neuralink chip repair the paralysis in the long term you know we can't do that right now we have done sort of preliminary work implanting a second neuralink in the spinal cord and we can restore naturalistic looking hand and leg movements in animal models um but this isn't something that is you know don't don't hold your breath waiting for it it's going to be a while we've got a lot of work to do but yes there's no reason in theory that we can't repair paralysis yeah um I mean essentially to to I mean there's there's no there's no physics barrier to fully solving paralysis that is perhaps a way to say it that you've got signals coming from your motor cortex uh that uh if they are transferred past the point where the the nerves are damaged essentially just it's basically a Communications Bridge um so you bridge the communications from the motor cortex um past the the point uh in the necr spine where the nose are damaged and you should like it just phys it is possible from a physics standpoint to restore full body functionality from a physic standpoint it's a very hard technical problem but it but it is there is nothing that prevents it happening from a physics standpoint so in terms of next phase of roll out well um we really want to make sure that uh we make as much progress as possible between each neuralink patient so this is we're only just moving now to our second neuralink patient um but we we hope to have uh you know if things go well High single digits this year uh and uh I don't know maybe this is somewhat dependent on regulatory approval U and how how much technical progress we make but within a few years hopefully thousands yeah and I think one thing that is important to highlight is that you know it's not that we' built only one device and one surgery we've done hundreds of surgery we' built thousand thousands and thousands of devices even for just the the ability to unearth any sort of lowf frequency failure mode so we have already been investing very heavily in infrastructure to be able to scale this thing on the device manufacturing side as well as on the surgery side of things we want to be able to help as many people as quickly as possible we go through obviously the appropriate hurdles right that are regulatory challenges and proving out the device with yeah and the the device implantation really needs to become um almost entirely if not entirely automatic uh in the same way that say lasic ey surgery is done U you know you don't have an opthalmologist with a a laser cutter by hand that that would be crazy uh but the opthalmologist oversees the uh the lasic machine and make sure that the settings are correct and then the the machine does everything and restores your eyesight uh it's really remarkable how many people have had their eyesight restored uh with with lasic and I think there's another one called smile it's they keep making it better we need to have something similar for a neuralink implantation so that you basically sit down and whatever the the what whatever kind of upgrades or you know brain fixes are needed um that's that's reviewed uh by medical expert OB we want to make sure that that is reviewed correctly but but it really needs to be automatic so you sit down and and within 10 minutes uh you have a neur link device installed very very fast I mean it's very sort of cyber punk you know dosx if you play those games when we new start to interface with other devices like wheelchair is great question we currently focusing on uh controlling computers and unlock Independence in the virtual world of course our plan is as we mentioned earlier robotic arm and wheelchair to unlock Independence in physical world this of course add additional risk if you make M your computer there's some to that but we are working with the FDA to allow us to quite soon well it seems like if if the wheelchair has a an app well the wheelchair just needs to have have an interface it does so if if the wheelchair has a Bluetooth interface uh you you could just Bluetooth interface to the wheelchair yeah and and um but that's probably something we should do we're pretty soon it's really a matter of paperwork of showing that you can do it safely you don't like drive off a cliff well I think we well we can the speed so it doesn't go King off into disaster um but uh you know so just make it go slowly at first uh but yeah so uh being able to sort of really the the nework device just should work generally for anything that's got a Bluetooth interface including potentially an Optimus asking uh yeah you yes you could communicate with Optimus uh yep absolutely Optimus will well we Al also be able to talk to Optimus but like why not just beam it but you could just yeah instead of talking just you could just beam it directly or if if someone has lost the use of speech then then they can still communicate to an Optimus uh they can communicate telepathically to Optimus or by bluetooth um and um and and so even if someone has you know completely less the ability to speak they could still uh control Optimus or with their computer or phone I mean also like if you have an optimist and you have a neuralink you can just directly map the brain signal to control of the physical arm of the robot and that's a very meaningful thing like if you're you know folks that have spin cord injury one of the biggest requests is to be able to scratch yourself this is something that quite annoying actually um and if you have a scratch on your face like you can't fall asleep until you scratch it uh you know it's very convenient to be able to move something physically towards you to be able to scratch similar things like eating food you know if you need somebody to feed you very hard to have know dinner with friends in a way that is you know sort of a normal social experience and so if you can feed yourself pick up a fork and actually eat a piece of chicken on your own uh you know that's a big deal uh it prevents and saves a lot of interactions with caretakers and other people in your life that you rely on to take care of you it really increases your I think an exciting possibility long term also is to say um if you take parts of the optim Optimus humanoid robot and you combine that with a neuralink let's say somebody has lost their arms or legs uh well we we could actually attach an Optimus arm or Optimus legs uh and uh do a neuralink implant so that the the motor commands from your brain that go would go uh to your your biological arms now go to your robot arms or robot legs um and again you you'd have basically cybernetic superpowers actually so the latency from the nurlink to your hand would probably be slightly faster than it is just to go to your physical hand so you can imagine like if you're a piano player or a I don't know anything that requires extremely fast know hand movements that you could actually have a pretty imbalanced right-and robotic arm control versus leftand physical Arm Control because one of them yeah like I said this is kind of a cyberpunk DEX in future where you have cybernetic upgrades that are actually better than your biological uh lims and uh it's certainly the we'll have a much you know as as particularly as we expand to a large number of of um of of customers or patients for neuralink uh the understanding of the brain will improve dramatically uh because the really there isn't a fine very fine grained understanding of the brain today because there just the sensors aren't good enough you got fmri which is pretty good but it's still not as good as actually having um high bandwidth electroids in the brain yeah I think this is underappreciated as a research tool to to move that whole effort forward of really knowing you know what the physical substance of human thought is we don't know uh to the to the degree that we need to so neuralink is actually a a very powerful research tool yeah I mean we I think we can ultimately uh understand and and fix uh quite severe psychosis or like if if somebody's got like the if somebody's got like a a like a delusion that they have a chip in their brain yeah I was wondering if you're going to mention that one um we just want to be clear that there's only one person with a neur link chip in their brain um so for people out there who think we've put their chip in their brain we wouldd like to assure you for what it's worth you probably won't believe us but we did not put each chip in your brain okay um so there actually a remarkable number of people who think we have put a tri in their brain but we have not um but in the future if you would like us to put a chip in your brain which will perhaps help with the issue of thinking that you have a chip in your brain uh then we will be able to do so uh so there there are people that have uh severe schizophrenia they've got basically things that um their brain is malfunctioning in some way and um and this is actually due to really like physical circuitry issues you can think of the brain as like uh really it's a it's a biological computer and if if some of the circuits are crossed it's going to uh you know it's going to crash or it's going to have issues that caus it to not work um but with a neuralink device we can fix those issues and uh you know give someone who I think pro has say severe schizophrenia or or psychosis of some kind uh allow them to live a normal life I think that is one of the likely things in the future so uh yeah I mean yeah you can certainly imagine like uh I'm sure people have like parents grandparents who've uh you know have memory that's uh not working as well as it used to be sometimes they they forget who who their grandchildren are or or what day it is and this is something that a neur link device could help fix I mean that that's actually one of the personal reason um in many way like forms of you're literally losing your and part of your identity yeah which is a just a very very go through yeah and it's really just it's a glitch in the biological computer that is uh a fixable uh glitch it's like like a it's a short circuit essentially how does the device charge and how long does the charge last yeah so the current version that Nolan has it lasts between four to five hours on a sing charge and it takes about 45 minutes to charge one thing we've learned from Nolan is that that's actually one of the main limiters for him using it more uh it's actually pretty hard to use a product more than like 70 hours a week but that's about what he has used it for it in some weeks yeah 70 hours in the week yeah I mean just for context like you sleep roughly eight hours a night so that's you know we're doing better than the bed like the bed is 56 hours a week of use roughly and uh so 70 hours a week of uses I challenge you to think about products that you've actually use for that duration but that's again some of these points are worth like emphasizing again like the that noan our first neuralink recipient has used the rolling device for 70 hours in a week which is incredible you probably won't enjoy that I'm sharing his computer use publicly but well I mean I assure you it's for productive things only no um but actually so one of the things we've learned is that in the next version of the device we really need to like double or you know increase that battery life and so I think uh DJ the next version is going to be double actually double without without increasing the Char correct same charging time double the battery life meaning you should get roughly 8 hours of use and the goal is to actually get to all they use so you can just charge um you know maybe in your sleep sleeping pillow exactly as soon as you got like 16 hours of usage then you basically have 24 hours of usage because it can charge while you're sleeping one other thing that's important I think to call out here is if you're paralyzed you can't you know put the charger over your head yourself and so it's important to think about like it's not just duration of bettery use but also can you recharge it yourself independently so we spend a lot of time thinking about how to make that feasible because then that means that you can this is what no one does you can use the device charge it use the device charge it use the device without needing anybody to come in and sort of help you with that which is a big deal if you're trying to play C until 5:00 a.m. at night when your family's asleep and the way in which he does that is that there is a charger coil that's a bigger you know about this big um and we actually put it in the um sleeve of a of a hatan or a beanie and then he wears it and then says with the voice command charge charger energize that's the one he likes how writing work uh so so uh yeah the current device that Nolan has is is is is reading um so it's trying to read his essentially like wrist movement from from one one hand um that's also you know with with like in the future like would pretty cool to give Noland a a second implant that would allow the other hand to be used and also have higher uh obviously higher active electrode count so then you can play two essentially play games two-handed because that's nor only how you play games um and uh but then with with writing uh it's really just uh it's an electrical impulse instead of like reading electrical impulses from the neurons you you issue an electrical impulse uh which is obviously critical for vision so vision is is writing which is just triggering electrical impulse in the vision part of the brain um and that like activates a a pixel so we we actually do have this working um in monkeys and we had have we've had it working with monkeys for a while now uh where you can sort of flash a pixel and then you watch where the monkey obviously the monkey's like what that monkey a little surprised to see like hey there's a flash here and a flash there but it's gets used to it after a while uh but it just you you can see that that the pixel in the right location because the monkey's eyes will will Dark to that location it's not on on the screen like there's no pixel on the screen there's no pixel on the screen in your brain yeah just like just verify that that the the that you're triggering a pixel in the right part of the brain so um you know the initial resolution for uh Vision will be relatively low you know sort of Atari Graphics type of thing but over time it it could potentially be better than normal vision and then I guess in terms of some additional applications for where writing to the brain can be useful is M uh applications as Bliss mentioned there is feedback there's appropr acceptive feedback there's a ttile feedback especially for robot arm like you're trying to grasp a cup you need to know you got it one one an egg to know it's a very much a delicate balance of not just initiating the movement but getting the feedback and controlling it accordingly so there there there is a subeta sensory cortex that's right adjacent to motor cortex could could be benefit motor movements so any changes in neural growth after the device is inserted we don't see any any signs of neural damage uh but I and I guess we we have seen some rebound on some of the electrodes right correct and then also I mean I guess I guess you know rain is very plastic yeah it's not that plastic well it it does diminish quite a bit after age 20 throughout childhood uh especially when you get to age about 25 brain really is done cooking yeah but there are there is a little bit of damage done with each insertion uh but it's a minuscule amount compared to anything else U there and so um it's an easy amount of damage to recover from and it's really only detectable on cutting pieces of the brain after uh after the animal's no longer alive and looking at them under a microscope you can't really tell during life that there's been any brain another way to interpret this question have the if you find any changes in neural growth after the device is inserted one way to interpret that is like the user learning how to use the device and I think on that side of things there's been tremendous progress he's put in hundreds of hours trying to figure out the best way to use this device cuz he really thinks that you know if he can figure this out he can help share this knowledge with I mean he's like on Friday night at 800m you know he's starting a session of like you know figuring out himself how to how to push his own performance to the next level and uh that's really a unique learning process cuz there's not many people in the world that had the experience of moving something with your bra and so there's a lot of nuance to like okay how exactly should I imagine or attempt to move my wrist to get that thing to uh yeah he's really died that into also just the sheer number of hours that he's used even in the past six months right yeah um in many ways like I me he's using it in his travel in his plane right effectively BCI has left the lab yeah yeah yeah I mean one of the questions is how close are we converting thoughts into text um I mean right right now it's more about Moving Co from the screen on on a virtual keyboard um but um long term you should be able to really transmit entire wordss faster than anyone could possibly type able to type hello world today directly from but we're still in the early days making that a polished experience I mean the other things that we're looking at is sign language right at the end of the day it is a movement of a of and into yeah that's true was the brain trying to naturally push the threads out I mean this is sort of a universal feature of any implant in the body the body tries to reject it uh and the goal of the surgeons and the technology team is to fight that and so with artificial hips and with you know screws in the spine we've done a really good job of finding biocompatible materials and techniques to uh fix those implants in the body I mean past a certain age it's getting hard to find someone without some kind of implant you know a knee hip uh some kind of screws in their spine um and so we've got this problem pretty well solved uh so to answer your question yes the body is trying to get rid of any implant but we can ensure that basically can't it's also worth highlighting that the threads have not actually moved um in the past five months um there's there's some still minor movements in terms of like some maybe maybe getting pushed in a little bit pushed out a little bit but it's it's more or less very stable and been stable for and the reason for that is you know once you once you do um a brain Sur surgy it takes some time for the tissues to come in and then and then you know the start tissue or the neom membrane to actually come in and then anchor the threads in place and once that happens everything has been stable and seen much movement that's where the world record performance starts to come in yeah that was a couple weeks ago yeah yeah the threads like it is important that the threads be extremely tiny if they're extremely tiny then the the brain uh does not the smaller they are the less likely the brain is to react to to them so that's why you want the threads to be extremely tiny and also to minimize any damage to neurons um so by way on that note we we do plan to actually share some of the um you know the tissue response in detail in some of the the later upcoming updates yeah it is quite a challenging um it's challenging on many fronts to do something like this uh because you're you're trying to read read and write electrical signals but you need to have the threads themselves need to be uh uh like electrically isolated um and and not subject to corrosion in the body so like the you know just metal by itself is so much subject to corrosion or or being attacked um so it's it's it's like in terms of the various Coatings and things to actually make this electroid work while not actually eroding its performance over time is is very difficult human bodies are very very harsh it's it's a bag of salt water with B sensors that's elevated temperature that is well regulated mean I'm sure people have experience dropping their electronic devices in a seawater and in an instance yeah yeah so we we better sort of wrap this up soon um I if there's like a few few last questions um yes so a good question so what about uh upgrades um so yeah we we we do think it's going to be important to be able to upgrade the device over time uh just like you wouldn't want um like an iPhone 1 stuck in your brain forever uh you you know if you've got an iPhone 15 you probably want the iPhone 15 not the iPhone 1 um so U I think people over time will uh be able to upgrade their their neural link so we'll take the neural link device out um and put a new one in um and uh we we have done this with um some of our um animals and they've actually in one case we did it with we we upgrade our device three times and and uh with a pig we did with monkey as well able to do BCI yeah so and he's he's doing fine yeah has impant actually hit his I think his record with the last yeah with the with an upgrade no still beat him though no still beat him yes this is true humans are top of the species leaderboard right now pag is like what like eight or something p is like 8.5 BPS okay well it's a very high score I'm not trying to put Pedro down and also to train a monkey to do that is like a whole Challenge on its own we have like the best animal care team world yeah I just do want to emphasize we we we we do our absolute best to take care of the the animals um and uh uh when we had like a USDA inspector come through she said that uh this was the the nicest animal uh facility she's ever seen in her entire life I mean they breakfast on an app like the the monkey orders room service yes I'm not even kid we we have monkey room service which is a rare um in fact we're the only ones who offer monkey room service so we really do everything we can to maximize the welfare of the animals so all right with that uh thank you everyone for tuning in uh hope you found this interesting",
    "status": "success",
    "error": null
  },
  {
    "video_id": "88I7gLR5v_A",
    "transcript": "PRESENTER 1: Welcome\nback, everyone. It's a great pleasure to\nintroduce Mengmi Zhang. She's one of the top post\ndoctorate scholars in my group. And she's going to do a\nvery exciting presentation. I don't want to spoil\nit, but she's the person that has multiple faces. So if you see one of these\nstrange avatars here, that's Mengmi. And she will talk\nabout something that's extremely cool\nand super exciting with a tutorial\npresentation for you to learn about deep\ngenerative models. So without further\nado, Mengmi, please. MENGMI ZHANG: Yeah,\nhello, everyone. Welcome to this tutorial. Yeah, my name is Mengmi. I'm a post-doc\nfrom Gabriel's lab. For those of you who have\nnot seen me before, just to clarify, this is not\nactually how I look like. Since I'm going to talk about\ngenerative models today, I thought it might be fine to\ndo a live demo of how we can use generative models\nin our daily life, for example, in\nthe Zoom meetings. Instead of mimicking fictional\ncharacters in a movie, the generative models could also\ngeneralize to a real person. So I'm going to switch my\nfaces just for now, like this, like Steve Jobs. And also it could, like, extend\nto a person in a painting. And I can also move\nmy head a little bit and make funny\nfacial expressions. What is more interesting is\nthat the generative models has been trained on celebrity\nfaces, and yet they have no problem generalizing\nto, for example, animal faces. Like, now I'm\nmimicking a monkey. Or a cartoon character\nlike SpongeBob. All right, for those\nof you who wonder what is the DeepFake\ntechnology behind the demo, here is a peak\nabout the algorithm. I also attached the link\nhere and the paper for you to check them out. Here's the source image, which\nis the avatar you want to be. And this is the driving\nframe, which is the real me. And then we can calculate\nthe optical flow, which tracks the movement\nof individual pixels between adjacent frames. We can also calculate\nthe occlusion map, which details the part\nof the background that needs to be repainted. Together with the features we\ntracked from the source images, these three components form\na latent representation. And then we could pass\nthis to the generator. And the generator could\nthen synthesize new frames and broadcast to Zoom. And now I'm presenting you\nan overview of deep learning frameworks in machine learning. In the previous tutorial,\n[? Andrei ?] and [? Boris ?] talk about the basics of\nconstructing a convolutional neural net and establish\nconnections between activations of the artificial neurons and\nthe neurons in mouse brains. In this tutorial, I will move\non to unsupervised learning. In particular, we\nwill be focusing on generative\nadversarial networks and how we can leverage\non these generative models to reconstruct images\nfrom brain signals. Here is the outline\nof the tutorial. I will first talk\nabout the basics of generative\nadversarial networks in an advance version\nof the [INAUDIBLE] which is BigBiGAN, followed\nby a review of existing image reconstruction methods\nfrom brain signals. Before I go straight\naway to talk about GANs, I want to introduce you\na very important concept being in deep learning,\nwhich is deconvolution. In the past, we\nhave been talking a lot about convolution. So the blue patch\nhere is the input. The cyan patch is the\noutput feature map. We observe that the 2D\nconvolution typically reduces the output\nfeature map dimension. In this particular example,\nthe feature map size decreases from 5 by 5 to 3 by 3. In contrast, deconvolution\ndoes the opposite. Here again, input is in\nblue and the output is cyan. The deconvolution\noperation upsamples the input a feature map\nfrom 2 by 2 to 4 by 4. In PyTorch, here is a\nfunction for deconvolution. And here is an\nexample usage below. All right, now, let's\ntalk about GANs. Just a little bit\nof history, GAN was first invented by\nIan Goodfellow in 2014. Since then, GAN has been\na popular research topic. So what is GAN? As this name indicates,\nit's a plural term. GAN consists of two networks,\none is the generator, the other is a discriminator. The generator takes\nthe random noise vector and generates an image. The law of the\ndiscriminator is to tell whether a given image\nis real or fake, which is generated by the generator. These two networks fight\nagainst each other. In game series, it is\ncalled Min-max game. In other words, each\nof these two parties is trying to minimize\ntheir own losses, given their opponent is perfect. For example, here, if\nI am a discriminator, I'm assuming I'm facing\na perfect generator which can generate realistic\nimages to fool me. Thus, I'm trying to minimize\nthe number of mistakes I'm going to make\nin misclassifying fake images as real images. Now, let's take a closer\nlook at the architecture of the discriminator. Same as other object\nrecognition networks that you have\nprobably seen so far, it's just another one\nnetwork consisting of a stacks of\nconvolution layers. It takes that image as\nthe input and outputs of probability vector,\nindicating whether it is real or a fake image. A couple of practice notes here. So this is PyTorch code\nloading the images. To make the network\n[INAUDIBLE] variations, you have to perform the image\naugmentation, such as scaling and rotation. And since we have\nthe ReLU layers, which we use a 0 as\nthreshold, we also want to normalize the image\npixel values from 0 to 1 to minus 1 to 1. In the end of the network, to\nmake sure the network outputs probabilities, we add\na Sigmoid function, which normalizes the\nvalues to 0 to 1. By the way, instead of\nonly connected layers, since this network consists of\nstacks of convolution layers, we also named this GAN\nas deconvolution GAN, which is in short of the DCGAN. Here's a diagram\nof the generator. It does exactly the opposite\nof the discriminator by replacing convolution layers\nwith deconvolution layers. The generator takes\na-- sorry, so here, what you see on the\nright is a snippet of how we constructed\nthe discriminator. And the arrows, shown\nhere, are corresponding with the different layers\nof the convolution. And take note, that\nin the end, we had to add in a Sigmoid function. All right, so here's the\ndiagram of the generator. It does exactly the opposite\nof the discriminator by replacing the\nconvolution layer with deconvolution layers. And the generator takes a vector\nof random noise as inputs. And that is for each element\nin this one dimension vector, we randomly sample number from a\nnormal distribution with mean 0 and variance 1. In PyTorch, this is\nhow we implement it. The generator outputs a tensor\ndimension of 3 by 64 by 64. However, the\ngenerator itself has no notion about what\nthis tensor represents. Thus, we need a hyperbolic\ntangent function to normalize all the\nvalues in the tensor to be from minus 1 to 1, as a\nrepresentation of the image. This is consistent with\nthe real input image pixel values to the\ndiscriminator, which also ranges from minus 1 to 1. Take note that this\nnormalization is essential, since we do not want the\ndiscriminator to easily capture the difference offset\nbetween the real pictures and the generated tensors. Here is the snippet\nof code constructing the generator,\nthe arrows to show the corresponding\ndeconvolution layers. In the end, we added\na tangent function. Now, we have the\nconstructed discriminator and generator let us see how\nwe can train this neural net at the same time. Again, to remind you that the\ngenerator and discriminator are playing against each\nother in a Min-max game. Thus, we can divide their\ntraining into two parts. So first, we assume we have\na perfect generator which generates a bunch of images. We can train a discriminator,\ndifferentiating the fake images from the real ones. Just like training other\nobject recognition network, the discriminator takes\neither a batch of real images with the ground truth\nlabeled as real. It performs a binary\nclassification of these real images and back\nprop the gradients like this. Similarly, we could also label\na batch of generated images as fake, compute the laws\nagain, and then back prop the gradient. The part of the code\nthat shows the training of the discriminator\nwith all the fake images labeled as fake, this\nis the first part. And then the second\npart shows the training of the discriminator with\nall the real images labeled as real. So far, the training story\nhas been very straightforward. And let's now see how we\ncan train the generators. First, without the\nreal images, we could simply concatenate the\ndiscriminator and the generator as one being network. Thus, we can compute\nbinary classification laws and perform the gradient\nback propagation from the very end\nof the discriminator all the way back to the\nbeginning of the generator. So the objective is to\ntrain the generator. We want the generated images\nto fool the discriminator into classifying them\ninto real images. Therefore, the ground truth\nlabel for the generated images are now going to be real. From the point of view\nof the generator, that is to say how much the generator\nneeds to correct itself in order to make sure\nthe discriminator outputs the real labels\nfor its generated images. Note here, this is\nimportant difference from training the discriminator\nwhere the ground truth label for the generated\nimages have now changed from fake to real. And this is the code for\ntraining the generator. And this is where the\nlabels have been assigned to the generated images. Since 2014, the study of\nGAN has become so popular, for the past five years,\nthere are many GANs out there. And these are several examples. There was one time\nthat GAN become so fashionable that there\nwas a joke in AI conferences that people would say, if you\nwant to get your paper accepted in top AI conferences in 2017,\nyou'd better put a word GAN somewhere in your paper. In this tutorial, among\nall these type of GANs, I want to introduce\nBigBiGAN, in particular. Now, we understand\nthe basics of DCGAN. However, why do we need to study\nmore GANs, rather than DCGANS? Like, why do we propose\nnew GANs such as BigBiGAN? Isn't DCGAN good enough? Here, I summarize a couple\nof disadvantages about DCGAN. So the first disadvantage\nis that the sizes of these generated images are\ntypically very small. It's even smaller in the\nsizes of the ImageNet images that we often input to the\nobject recognition network. Second, one generator is often\nresponsible for generating only one object\nclasses of the images. In the lecture given by\nAntonio Torralba last week, we knew that if we want to\ntrain one generator generating buildings, it's very unlikely\nthat these generators can give you dog images, for example. This is very unsatisfying. Typically, we want a generator\nwhich can generate images across multiple object classes. But the reasons that we\ncannot do this is because we don't have constraints on the\nrandomly sampled latent vector in the generator. And another disadvantage is\nthat these images are typically of low resolution and its\nlack of high visual details. Not only that, even training\nDCGAN is very brittle. For example, sometimes\nthe loss for the generator and the discriminator\nall simulate over numbers of epochs. At the testing stage, it is\ncommon to see the generator collapse, meaning they lost\nthe ability of generating diverse number of samples. And typically, they\noften generate, like, for example, five image\nexamples from a class, and then that's it. Since this is a battle\nbetween the generator and the discriminator, it's easy\nthat the discriminator always wins the game. And thus, the gradient\nof the generator diminishes, since\nit always loses. It never wins no matter what. So there are several\nempirical evidences suggesting that\nthese networks are very sensitive to hyperparameter\ntunings during training. That's why, here,\nI want to introduce you an advanced version\nof GAN, which is BigBiGAN. This is the work published in\nNIPS last year from DeepMind. As its name indicates,\nit consists of two parts. It is BigGAN. And it is also bi-directional. So let me first introduce\nthe part where it is big. As its name literally suggests,\nit is a big neural net. It is trained with\nlarger batch sizes. And it has more\nnetwork parameters. The authors have also\nintroduced several architectural modifications. For example, they\nintroduce skip connections of the latent vector. That is, they bypass\nthe latent vector to the next layer after the\nfirst deconvolution layer, and so on. Self-attention module\nturns out to be useful in many applications. The intuitive way to interpret\nthis self-attention model is the following. Imagine, you are\nan artist and you want to paint a dog as\nhe's sitting on the grass. So what the\nself-attention model does is to pay more attention\nto the dog regions, instead of the grass. So this is exactly what the\nself-attention model does. It helps you guide the network\nto focus on important region to paint such that we can\ngenerate more realistic images. Next, let's talk about\nthe bi-directional part, which is the part I found\nwhich is very interesting. So here is what we\nsee in the DCGAN. In order to control what\n[INAUDIBLE] in the latent code, authors introduce a encoder\nside by side the generator. This is exactly the opposite\nof the generator, which takes the real images\nand encodes this latent representation zed hat. Ideally, if the\nlatent vector carries the same essential information\nas the abstract information, then the generated images\nshould look close enough as the real images. Thus, the discriminator\nhas two extra jobs to do. In addition to the objective\nof telling the generated images from real or fake, it\nalso has to distinguish the extracted representation\nof the real image from the latent coding in\nthe generator, as shown here. Moreover, it also\nhas to distinguish whether the joint\ndistribution combining the encoded representation\nin this image is real or fake,\nwhich is it's going to take pairs of the encoding\nas well as the image. With those two\nadditional constraints, BigGAN can generate high\nquality images of larger image size, up to 512 by 512. There are more tricks for\nconstructing realistic using GANs. For example,\nresearchers have found that updating discriminator\nmore often during training is very helpful. Taking the average of the model\nparameters is also beneficial. So here is what I meant. For example, in the\nfirst epoch, you update your generator parameter\nand denote it as W_G1. Then, in the second epoch,\nyou update the parameter for the generator again\nas W_G2 and so on. And so the testing stage, you\ncompute the final parameter for the generator by taking the\naverage of all the parameters over the number of epochs. Here are more tricks. We know that for each\nelement in the latent vector, we randomly sample from\nthe normal distribution with a mean 0 and a\nstandard deviation of 1. At the testing stage,\ninstead of sampling from a normal\ndistribution again, we are going to sample from\nthe parts out of 1 sigma, let's say. This would help us input more\nextreme values into the latent vector, and thus it constructs\nmore realistic images. But then there is a caveat. So this would sacrifice\nthe sample diversity, so the space we can sample\nfrom becomes smaller. During training the\ngenerator, we typically want to regularize the\nweights to be orthogonal. So here's an\ninformal explanation, but I will encourage you to\ncheck out the actual paper. They provide empirical\nanalysis about this. So imagine you have\ntwo weight vectors. If they are orthogonal\nto each other, then they will share\nless similarities, compared with the two\nvectors shown on the right. Thus imposing this\northogonal regularization, you actually push the\nnetwork parameters to learn as many distinctive\nfeatures as possible. All right, we finish the machine\nlearning part of the tutorial. Let us now move to the brain\nreading part in neuroscience. With all the basics of deep\ngenerative models in mind, we could ask ourselves\nthe following question. If this random vector\ncan generate images, can we actually plug any\nbrain codes into the generator and reconstruct its\n[INAUDIBLE] images? One could imagine that the\nbrain code could be any type. For example, it can be\nmeasured with any techniques in neuroscience and recorded\nfrom any brains of animal species. And in the future, instead\nof restricting ourselves into vision, we could also\nextend it to five senses, for example, sound,\ntouch, and smell. And we could also extend\nthis to high level cognitive functions such as emotions,\nmemories, and languages. Before we completely switch gear\nto review image reconstruction measures, let me now stop\nhere for a couple of seconds and take one or two\nquestions, if any. PRESENTER 2: Great, we've got\na question from Anthony Chen. Can you explain why\nhaving ReLU activation means we need to normalize\npixel values to negative 1, 1? MENGMI ZHANG: Sorry, can\nyou repeat the first part? What is IOU activation? PRESENTER 2: Can you\nexplain why having R-E-- so capital R, little e, capital\nL, capital U, ReLU activation means? MENGMI ZHANG: Oh, I see. Got it. Yeah, so typically, we have\nthis linear ReLU activations in the network. So that's where we zero\nout all the negative values in the layer. So imagine, now, if\nyou don't normalize your pixel distribution\nfrom minus 1 to 1, let's say, we stick\nwith 0 to 1, then your ReLU function basically-- so basically, you can treat\nReLU as a thresholding mechanism that decides\nwhether the value is going to be above 0 or below 0. And if it is below,\nthen I'm going to zero out all the values. Therefore, you probably want\nto shift your image pixel distribution from minus 1 to 1. Hopefully, this would\nprovide a satisfying answer to your question. PRESENTER 2: The next one\nis, anonymous is asking, why is it called self-attention\ninstead of just attention? MENGMI ZHANG: Well,\nthat's because the self-attention\nmodel, like, the network itself learns attention. So typically, when people\ntry to say attention, you sort of like provide human\nsupervision or human judgment into the model, like, which\nparts of the image that you think are important. But here, the network\nactually just ultimately computes attention values\nin the attention maps. So yeah. PRESENTER 2: Thanks. And we've got one\nmore from [INAUDIBLE].. Can you explain more\nabout the tricks for image reconstruction? I mean, what could\nbe the strategy if you want to look at the\noutput of some patterns? MENGMI ZHANG:\nWell, I don't quite understand what you mean by\nthe strategy of the patterns. But I would imagine,\nso let's see if you want to\ninterpret a target unit in the generative\nmodels, maybe we could-- I haven't done this before,\nbut I would imagine, let's see, we could\nuse similar ways as what's DeepDream\nhas been doing, which is you try\nto compute the loss of certain features of the\ntarget unit in certain layers. Then take the gradient\nand iteratively amplify it with respect to the\ngenerated images. All right, so this is a\nrelatively new and fast growing research area. After a literature\nreview, what I found was only papers on\nimage reconstruction from brain signals. Unfortunately, I didn't\nfind any literature about reconstruction of other\nmodalities, for example, music. In this later half\nof the tutorial, I will briefly go through some\nof the representative works on image reconstruction methods. I divided the reconstruction\nmethods into two groups. One is based on gradient\nback propagation, which was mainly inspired by DeepDream\nand then TextureSynthesis. First, I should clarify\nthat these gradient back prop methods are not\ngenerative models. This tutorial, I'm going to\nfocus on generative models. I will only briefly\nintroduce the key ideas behind this algorithm. But I strongly recommend you\nto check out these papers by yourself. So the first two pieces\nof work shown here are inspired by the idea\nsimilar as DeepDream. This algorithm is\nused for visualizing the patterns learned\nby a particular unit or layer from a neural net. It over-interprets and\namplifies the patterns it sees in an image. DeepDream does so by\nfirst forwarding an image through the network, then\ncalculate the gradient of the image with respect\nto the activations of a particular layer. Then, the image is modified\nto increase those activations, enhancing the patterns\nseen by the network, resulting in dreamlike images. Another piece of\nwork worth mentioning is the study of neural responses\nin [INAUDIBLE] where they use this texture like pictures. What these authors did is first,\nthey take a natural image, pass it to pretrained\nneural net. It takes another\nwhite noise image and passes through\nthe neural net again. Then, the texture\nsynthesizer computes the mean squared area of the\ngrand matrix as a target layer, for example, Conv\n3, in this case. And it back propagates\nthe loss with respect to the white noise image. The generated image has\nthis texture looking like. So to put this in\nlayman's terms, the goal of the\ntexture synthesizer is to preserve the structure\ncontent of the target layer in a neural net. What is interesting\nis that these authors found that these synthesized\ntexture images share a similar V1 neural\nresponses and what has been observing the natural images. OK, so now let's move on to the\ngenerative model based method. One of the key problems\nwe need to solve is how we can translate\nthe brain signals into this latent\nrandom codes which the generator understands. One of the\nstraightforward solutions is we don't need to perform\ntranslation between the brain signals to this latent code. Instead, we could directly\ntake this brain codes as inputs to the\ngenerator and fine tune the parameters of the\ngenerator to make it adapt to our new brand codes. So here is a\nrepresentative work where the authors asked participants\nto see a bunch of images and obtained their brain codes. Here, each latent\nvector represents a piece of code\ncorresponding to an image. Then, the generator takes the\nbrain codes directly as inputs and then reconstructs\nthe images. Just as a standard\nGAN training, authors imposed discriminator losses\nto tell the real images or reconstructed ones\nfrom the brain signals. In addition, the authors\nintroduce another two losses to impose constraints at both\nthe pixel level and feature levels. That is, the\nreconstructed images should look as similar as\npossible as the real image. At the testing stage,\nthe generator is fixed. And then, we could take\ndirectly the brand codes and plug it in to\nconstruct the image. On the right, the\ntwo bottom rows shows the reconstructed\nimages from the two subjects using fMRI signals. Again, I want to emphasize\nthat one could easily imagine that we can replace\nthe fMRI signal with other type of brain codes and adopt\nsimilar approaches for image reconstruction. Of course, the\ngenerator [INAUDIBLE] contains many parameters\nand this method would require intensive\namount of data in order to fine-tune\nthe generator. So next, we will explore\nalternative approach, which is to fix the\ngenerator parameters, and instead, try to\nlook for the best latent code that would drive\nthe neurons to fire as much as possible. Here is how the algorithm works. So let's start from a\nbunch of noise vectors, just as what we did before. The generator uses\nthese old latent codes to generate a bunch of images. And then we present\nthese generated images to monkeys and record\ntheir neural responses in the form of spike trends. Just a quick recap on\nEthan's tutorial last week on processing neural data, we\ncould treat these spike trends as a binary vector and compute\nits average firing rates. For example, we could take\nthe mean of this spike trend and we get a number. For example, this is 30\nhertz for the first image and 20 hertz for\nthe second image. Since our goal is\nto drive the neuron to fire as much as\npossible, we use those computed mean firing\nrates as the fitness score for each image code. In this paper, authors proposed\nto use the Genetic Algorithm to select the best\nparent, then crossover mutate them to get the\nnext generation of codes. After the optimization, we\nhave a new population of code, and then the generator could\ngenerate a new batch of images. And this process iterates. So what I'm showing here\nis a single synthetic image involving over generations. This method is a\nclosed-loop system. Because it involves monkey\nbrains in the loop, and then it requires many iterations in\norder to find the best stimuli. Next, I will introduce a simpler\nway of establishing the mapping function. It turns the latent code\ninto the brain signal via linear regression. Again, the generator\nare fixed in this case. Just a quick recap. So in BigGAN, we\nhave encoder which encodes the abstract\nrepresentation of the real images. And after training, this\nlatent representation carries similar features as\nthe latent code [INAUDIBLE].. Thus, we could make use\nof this relationship to help us find the\nbest mapping function. So let's see. We have total, N\ntraining images. For each image, we could pass\nthem through the pretraining coder from the BigBiGAN. Let's concatenate this feature\nvector from all the images as a big matrix called\na generator matrix. It is of dimension\n120 by N. Next, we could similarly represent\nthese images to animals and extract those brain\nsignals, concatenate them, and name it as brain matrix. It is of dimension nv by N. Since we have the generator\nmatrix and the brain matrix, we could easily perform a linear\nregression between two of them. Specifically, this is to\nestablish a linear mapping by computing the weight matrix,\nwhich is of dimension nv by 20. And then we also have\nthe generator matrix, which is 120 by N. So we could\ncalculate the weight matrix. Here are some practice issues\nthat we have to take note. For example, W my\nnot be invertable. So we could take\nthe pseudo-inverse. And the brain matrix\nsize might be very big. And we can perform\ndimension reduction using PCA to\npre-process the data. Since the scale,\ntheir brain signal might be different\nfrom the latent code. We also want to\nperform normalization of the brain signal first. All right, at the\ntesting stage, we could simply make use of the\nweight matrix we just computed and transform this brain\nsignal to latent code and then pass this\nlatent code back to the generator in BigBiGAN\nfor image reconstruction. Yeah, so on the right,\nwhat you see here are the reconstructed\nimages using fMRI signal. Again, the linear\nregression method could be generalized to\nother forms of brain signals, as well. Then, after reviewing all these\nimage reconstruction methods from brain signals,\nyou might have your own personal\njudgments about which brain reader is the best. In order not to be\nsubjective, the next question we want to ask is can we come up\nwith a quantitative evaluation metrics to evaluate all\nof these brain readers? And the answer is, clearly, yes. So similar as brain scores\nproposed in Jim DiCarlo's lab, evaluating the relationship\nbetween state-of-the-art object recognition network and the\nneural responses in the brain, here we propose to add in\nadditional metrics to evaluate the deep generative models\nand report their relationship to the brain signals. Yeah, I think we still\nhave plenty of time. So let me just introduce\nyou each of this metrics individually. So first inception score, it\nmeasures both the image quality and diversity. Here's how the\nscoring is computed. First, it reflects\nthe image quality. So let's see, we have\na generated image, and then we pass it to\nobject recognition network. Here we are using Inception\n3, but it could be any other recognition network. It outputs a vector indicating\nthe classification probability for each object classes. If the reconstructed image\nis of very good quality, that is the network has\nhigher confidence recognizing what\nthe object is, then the label distribution would be\nuni-modal, resulting in a lower entropy score. For those who don't\nunderstand what entropy is, it basically reflects the\nuncertainty of a distribution. Conversely, if\nthe network barely recognize what the\nreconstructed image is, then we will end up\nwith a uniform label distribution, which results\nin a higher entropy score. In this case, the lower\nentropy score, the better the image quality is. Moreover, we want to evaluate\nwhether the brain reader has enough image diversity. For example, if the network\nalways generated dog images no matter what latent codes\nyou give to the network, then I would say this\nnetwork is very bad because it fails to capture the\ndiversity of object classes. Thus, if we can take the sum of\nthe classification probability for all the\nreconstructed images, we would expect it to end\nup with a more focused distribution, since\nthe generator always generated dog images. In contrast, if the generator\ncould evaluate multiple object classes like showing here, we\nhave elephants, cats, dogs, et cetera, then the sum of all\nthese classification vectors would give us a more uniform\ndistribution, therefore, a higher entropy, which is good. Overall, we want a more\nfocused label distribution for better image quality, but\na more uniform distribution for diversity. In order to combine\nboth aspects together, inception score finally\ncomputes the KL divergence between these two distributions. The higher the KL\ndivergence score, the better it reflects\nthe model's ability to generate good images with\ndiversified object classes. Note that this inception\nscore discards the information about the real images. Because you only calculate\nthe image quality and image diversity based on\nthe generated images. It has nothing to do\nwith the real images. Therefore, we are\nproposing a new metric which is the Frechet inception\ndistance, short for FID. For both the reconstructed\nand the real images, we could use the inception\nnet to extract their feature vectors. Thus, for each real and\ngenerated image pairs, we have a pair of\nfeature vectors. The FID calculates the\ndistance between these two distributions. A better brain reader gives\na lower distance score. So here's a mathematical\nformulation of FID which basically\ncalculates the distance between the mean\nvector and the trace of their covariance differences. So here is the third metric\nfor evaluating brain readers. Again, this is a\nBigBiGAN architecture. I want you to focus on\nthe encoder part again. So for each generated\nimage, the encoder could take them as inputs, and\noutputs their feature vectors. If this encoder representation\nreflects the information about [INAUDIBLE]\ncategorization, then these features from\nthe generated images should look good enough for\nclassification on ImageNet. We, therefore, could\ntrain a simple classifier, predict their cost\nlabels on ImageNet, and report their top-1 accuracy. If the brain reader\nconstructs meaningful, natural looking images\nwish corresponds with the real images, then the\ntop-1 classification accuracy would be very high. In the BigBiGAN paper, I'm\nquite surprised that they also report a 61% top-1\naccuracy on this ImageNet. And using the purely\nunsupervised learning. All right, so we could\nalso conduct human behavior experiments to evaluate\nthese brain readers. Here is the experiment paradigm. Let's see, we have the input\nimage presented to the humans and then as targets. We could then randomly choose\none reconstructed image based on the brain signals\nfrom that target image. As a negative sample,\nwe could randomly pick another reconstructed\nimage from a non-target image. With these three\nimages, we proceed to conduct the behavioral\nexperiments using Amazon Mechanical Turk. This is how it looks\nlike in an example trial. So in the experiments,\nthe subject is instructed to\nchoose either option A or B that is more visually\nsimilar with the target image. After we collect the human\nchoices on many trials, we could simply report the\naccuracy of human judgment. Now, I want you to pay attention\nto the bar on the right, highlighted in red here. The dash line is the\nchance level, which is 50%. In the paper, authors\nreported there about 90% of people\nchoosing correctly. That is, they prefer\nthe options which are reconstructed images based\non the brain signals generated after the subject [INAUDIBLE]\ntarget image instead of the non-target image. All right, at last we could\nalso assess the controllability of these reconstructed\nimages at a neural level. So here is an illustration\nof what I meant. We first record the average\nneural firing rate of monkeys after they see\nthe real pictures. Then, we could test monkeys\non their corresponding reconstructed images and\nrecord their firing rates. Next, we show a scatter\nplot where the x and y-axis indicates the firing rates\nof the real and reconstructed images. We put a dot for each pair\noff the average neural firing rates between the real and\nthe reconstructed images. And here are more dots for\nthe second pair of images and third pair, and so on. Now, we can fit\na line and report their linear correlation. So the higher the\nlinear correlation implies reconstructed images\ncapture more essential features. All these brain\nscore assessments, what we discussed\nso far, could also be applicable with other\ntypes of brain signals. All right, just to conclude. Nowadays, there have been\nfascinating progresses about deciphering\nbrain and codes using machines and\ninterpreting the machine codes using brain responses. Hopefully, not far\nfrom the future, we could find a\nperfect bijection which could translate between\nthese two types of codes. With this bijection,\nit is possible that one day we could\nstimulate and control any part of the brain. Last, but not least, I want\nto conclude the tutorial with an illustrative\nfigure, showing you a hypothetical diagram of\nthe brain-machine interface for the visually impaired. With the joint efforts of\nthe neuroscience and AI researchers, one could\nimagine that one day we could help the visually\nimpaired by installing a camera in front of the eyes. And the neural\nnetworking could be embedded in a pocket processor. It extracts the machine\ncodes and translate it to brain-understandable\nlanguage. And this could be\ntask dependent. And it could be text reading or\nface identification or obstacle avoidance. In the end, it could translate\nto the brain or standby codes to wire a wireless\ntransmitter on the scalp and use the electrode\narrays embedded in the brain to stimulate the brain. PRESENTER 2: A general\nquestion, how do we know brain signals contribute\nto the generated images? The generator also can\ngenerate similar images by inputting a random\nlatent code space, right? MENGMI ZHANG: Yeah, that's\na very good question. But if let's see if I mean I'm\nnot sure about the DCGAN case, because it always generates\nimages from one object classes. But, let's see, for\nthe BigBiGAN case, so if the generator ignores\nwhatever in the latent code, it's probably going to generate\nimages from different object classes, compared with the\nimage that is actually presented to the animals, right? So for example, if I'm\npresenting the monkeys with a dog image and if\nI'm just randomly plugging another random noise\nvector to the generator, the generator is probably\ngoing to generate a cat image instead of dog. And that's why in the evaluation\nmetrics have been introduced, the top-1 classification\nscore on ImageNet. And hopefully, that could\nclarify your question. PRESENTER 2: Thanks. Next one is from Jed. Is fMRI more adaptive\nthan EEG to this problem? Can we expect more precise\nreconstructions with EEG? Is there any way to combine\ninput from both EEG and fMRI? MENGMI ZHANG: Oh, that's\nan interesting question. So just to clarify, I'm not\nfrom neuroscience background. I couldn't say\ntoo much about it. But I would imagine, yes. And why not? I mean, for me, any\ntype of brain signal is just like a\nseries of numbers. And then if we could interpret\nthem correctly and analyze them, normalize them correctly,\nthen I don't see the reasons why these brain codes cannot\nbe represented in one way or another. PRESENTER 2: Why does\nincreasing the activation create dreamlike images? MENGMI ZHANG: So the\nname is DeepDream. I think, it's\nprobably just saying, you know, this weird-looking\ndog looks like a dream that you just had, right? But then at the algorithmic\nlevel, what it basically does is just to amplify the\nactivations of the target unit in the target layer. So I mean, this is\na subjective course. It actually depends on how\nyou interpret them, right?",
    "status": "success",
    "error": null
  },
  {
    "video_id": "YreDYmXTYi4",
    "transcript": "foreign [Music] [Music] foreign [Music] [Music] thank you [Music] [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] [Music] foreign [Music] foreign [Music] [Music] [Music] [Music] [Music] foreign [Music] foreign [Music] [Music] thank you foreign [Music] thank you [Music] foreign foreign [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] foreign [Music] [Music] foreign [Music] foreign [Music] [Music] [Music] thank you all right [Music] [Applause] thank you [Music] foreign [Applause] [Music] all right welcome to the neurolink show and tell so we've got uh an amazing amount of new uh developments to share with you that I think are incredibly exciting as well as tell you about the future of what we're planning to do here it's uh now this is meant to be a technical podcast or sort of like a work I'm going to provide an overall summary and then we're going to have a number of members of the the neural link team come in and give a deep technical overview of the various areas so uh yeah so let me move forward with the the overall summary now some of the things I'm going to say are things you've well if you've been following your link you've already heard before that uh for for a lot of people out there they've no idea what neurolink does and so I'll be a little bit repetitive of things you may already know but that others do not so um the the overall the overarching goal of neurolink is to create a uh ultimately a whole brain interface so uh a generalized input output device that in you know in the long term literally could interface with uh every aspect of your brain and in the short term uh can ask we can interface with any given section of of your brain and and solve a tremendous number of things that that cause debilitating issues for people so uh you know so our long term is like I mean I'll talk a little bit about a long-term goal it's going to sound a little esoteric but it's the it was actually the sort of my Prime motivation which was you know kind of what what do we do about AI like what do we do about artificial general intelligence uh if we have digital super intelligence that's you know just much smarter than any human how do we mitigate that risk at a species level how do we mitigate that risk and then even in a benign scenario where the AI is uh very very benevolent um then how do we even go wrong for the go along for the ride how do we participate um and the conclusion I the the thing that the biggest limitation in going along for the ride and in aligning uh AI I think is the is the the bandwidth the the how quickly you can interact with the computer so we're we are all already cyborgs in a way in that your your phone and your computer are extensions of yourself and if you I'm sure you found like if you leave your phone behind uh you end up tapping your pockets and and it's like having missing limb syndrome like where you know the phone is it is leaving your phone behind is kind of like a missing limb at this point you're so used to interfacing with it you're so used to being a de facto cyborg um But but so what's the limitation on on a on a phone or a laptop limitation is the rate at which you can receive and send information especially the the speed with which you can send information so if you're interacting with a phone it's limited by the speed at which you can move your thumbs uh or the speed which you can talk into your phone this is an extremely low data rate um you know maybe it's like 10 optimistically 100 bits per second but a computer can can communicate at uh you know gigabits terabits per second so this is the fundamental limitation that I think we need to address to mitigate the long-term risk of artificial intelligence and also just go along for the ride and uh yeah so if it likes it that's that's that's an esoteric explanation that I think will appeal to a niche audience um uh some of whom may be here um but and that's a that's a very difficult problem so even if we do not succeed with that problem I think we we like we are confident at this point that we will succeed at many uh it's it's solving many brain injury uh issues spine injury issues along the way so um yeah so anyways so uh actually we have uh Justin Roiland in the audience uh says the hi Justin so it's a little Rick and Morty reference here um the uh this great Rick and Morty episode about intelligence enhancement of your dog and uh what's the worst that can happen so anyway Rick and Morty I recommend it um so for so you want to be able to read the signals from the brain you want to be able to to write the signals uh that you want to be able to ultimately do that for the entire brain and then also extend that to communicating to the rest of your nervous system if there's a if you have a sort of a severed spinal cord or neck so uh now this is a this video is now 18 months old so this is um pager uh who is playing uh monkey mind pong so this is a pager has a neural link implant in this video um and the thing that's interesting is that you you can't you can't even see the the neural implant so it's the it's we've monitorized the neural implant to the point where it matches the the thickness of the skull that is removed so it's essentially that it's sort of like having an Apple Watch or a Fitbit uh replacing a piece of skull with like a you know a smart watch for lack of a better analogy um so uh so you can see you really can't he looks pretty easy normal um and I think that's pretty important if you have a neuralink device like I could have a neuralink device uh implanted right now and you wouldn't you wouldn't even know I mean hypothetically yeah I may be one of these Demos in fact one of these demos I will so uh yeah anyway so so here's here's uh first of all it's kind of wild hey monkeys can play Pong they're like uh they can't actually pay pong if you give them a joystick uh so Pedro first learned to play Pong with a joystick so I'm like that was a novel it's like I didn't know monkeys could play Punk but they can um and then uh so we first trained pager to play Pong with a joystick then we took the joystick away and have the neural link and now this is he's playing telepath it's a telepathic video games essentially um so what we've been doing since then is uh we've been on the very difficult Journey from prototype to product uh and I've often said that prototypes are easy production is hard it's really I'd say a hundred to a thousand times harder to go from go from a prototype to a device that is safe reliable Works under a wide range of circumstances is Affordable and down at scale it's it's insanely difficult um I mean there's an old saying that you know that it's one percent inspiration 99 perspiration but I think it might be 99 or 99.9 perspiration um the best example I could give of an idea of being easy but the execution being hard is going to the Moon it's uh the idea of going to the Moon easy going to the Moon very hard so um and we've been working hard to uh be ready for our first human and obviously we want to be extremely careful and certain that that it will work well before putting a device in a human but we're we're submitted I think most of our paperwork to the FDA and we're we think probably in about six months we should be able to have opposed neural Link in a human so [Applause] but as I said we we do everything we possibly can to test the devices before uh not even not even going into a human before even going into uh an animal so we do Advantage top testing we do accelerator accelerated life testing we have a fake brain simulator that has the the texture and uh it's like emulating a brain but it's sort of rubber and uh so any we before we would even think of putting a device in an animal we we do everything we possibly can with rigorous bench up benchtop testing so we're not Cavalier and putting devices into animals uh we're extremely careful and uh we always want the device whenever we do the implant if it's in a sheep or a pig or a monkey to be confirmatory um not exploratory so that we like we've we've done everything we possibly can with benchtop testing and only then would we consider putting a device in an animal um and uh yeah we'll actually show you a demo later today of a few hours really of uh of implanting in a brain proxy and if anyone in the audience wants to volunteer uh with a robot right there so let's see since the page of demo we've expanded to work with a troop of six monkeys we've actually upgraded pager they do varied tasks and we do everything possible to ensure that things are stable and replicable and the things like that the device lasts for a long time without degradation so and uh what you're seeing there is it looks like the Matrix but that's uh actually though that's a real output of of neural signals so that that's that's not a simulation or just a screensaver or something that those are actual neurons firing that is one of the what one of the readouts looks like and um here you can see sake it's one of other monkeys typing on a keyboard now this is telepathic typing so to be clear this is the he's not actually using a keyboard he's moving the cursor with his mind uh to the highlighted key now technically um uh we can't can't actually spell and so I don't want to oversell this thing because that's uh that's the next version um so the but what's really cool here is is um sake the monkey is moving the mouse cursor using just his mind moving the cursor around to the highlighted key and then spelling out what we here what we want everyone just felt but um and then so this this is a something that could be used for somebody who's who's say uh uh quadriplegic or tetraplegic human even before we make the the spinal cord stuff work is being able to con uh control a mouse cursor control a phone um and we're we're confident that that someone who is has basically no other interface the outside world would be able to uh control their phone better than someone who has Working Hands so foreign upgradability upgradability is very important because our first production device will be much like an iPhone one and um I'm pretty sure you would not want an iPhone one stuck in your head if the iPhone 14 is available um so it's gonna be it's be able to demonstrate full reversibility and upgrade ability so you can remove a device and replace it with the latest version or if it stopped working for any reason replace it it's that's that that's a fundamental requirement for the device at your link and I should say both sucky and Paige were upgraded to our latest and greatest implants so that that's been really over a year and a half now that that pagers had for the first implant and then the upgraded implant so this is a very good sign that it lasts for a long time with no uh observed ill effects I think it's also important to show that um sake actually likes doing the demo um and it's not like strapped to the chair or anything so uh it's yeah so um the monkeys actually enjoy doing the demos because they and they get the banana smoothie and it's kind of a fun game so um I I guess smart termic is like We Care a great deal about animal welfare and um and uh I'm pretty sure like our monkeys are pretty happy you know so as you can see there's a quick decision maker on the fruit front so so for uh the the first two applications we're going to aim for in humans are restoring vision and uh I think this is like notable in that even if someone has never had Vision ever like they were born blind we believe they could they can we can still restore vision so uh because the visual part of the the visual part of the cortex is still still there so uh yeah even if they've never seen before we're confident that they could they could see um and then the uh the other application being in the motor cortex uh where we would initially enable someone who uh has no ability to almost no ability to operate their their muscles you know sort of like a sort of Stephen Hawking type situation and enable them to operate their phone faster than someone who has Working Hands um but then even obviously even better than that would be to bridge the connection um so uh take take the out the signals from the motor cortex and let's say somebody's got a broken neck then bridging those signals to neural link devices located in the spinal cord so I think we're confident there are no there are no physical limitations to enabling full body functionality so I mean as miraculous as it may sound we're confident that it is possible to restore full body functionality to someone who has a severed spinal cord so yeah [Applause] so yeah all right um and then I went to emphasize game that the primary purpose of this update is recruiting um a lot of times people think that they you know they couldn't really work at neurolink because they don't know anything about biology or how brains work and the thing that we really want to emphasize here is that you don't need to because when you break down the the skills that are needed to make neurolink work it's actually many of the same skills that are required to make a smart watch or uh modern phone work so it's sort of you know software batteries radios inductive charging um and uh you know as well as things that are specific to to us like animal care and clinical and Regulatory matters um obviously machine learning that phrase is used a lot but we also need to interpret the signals from the brain which is a biological neural net and the best thing to interpret a biological neural neural net is a digital neural net um so this is if there's one message I want to convey it is that if you have expertise in creating Advanced uh devices like watches and phones computers then your your capabilities would be of great use in solving these important problems that's that's that's one thing the message I want to convey uh so um let's see uh yeah so with that I guess DJ uh so so DJ's uh was on the founding team of neurolink and just made immense contributions to the company uh as of many of the others who will present but I want just to thank DJ for his immense contribution to uh neurolink and Frank all right cool thank you thanks Elon when I moved from South Korea at age 13 and needed to learn a new language to communicate I wonder whether there are better and more effective means of communicating my thoughts to the outside world and watching Neo learn Kung Fu and The Matrix I remember thinking wow I want to work on that work on making that possible and today I believe that this is attractable engineering challenge since everything about your intentions your thoughts and your experiences are all in your brain encoded as binary statistics of action potentials if you're able to put electrodes in the right places with the right sensing and stimulation capabilities this and many other applications that Elon talked about are possible and we can help a lot of people I'm incredibly excited to be working on this ambitious yet important mission to make that future a reality here at neurolink and I'm also incredibly honored to be working with some of the brilliant colleagues scientists and Engineers across many engineering disciplines to work on this intersection of biology and Technology you'll hear from several of them today to learn about the breadth of technical challenges challenges we face and our progress in the last year and I think you'll find that for most of these challenges as Elon mentioned you don't need a prior understanding of how the brain works and that a lot of what we do is applying engineering first principles to biology so how do you create a high bandwidth generalized interface to the brain from day one we focus on a set of foundational technologies that are safe scalable and capable of accessing all areas of the brain these three axes safety scalability and access to brain regions really form the basis for how we engineer products here at neurolink safety because we want to make our devices as well as the installation as safe as possible so that we can drive the adoption of this technology and scalability because as we make our devices safer and more useful more people will want it and with scale we also want to make it more affordable and access to brain regions so that we can expand the functionalities of our Technologies so our first steps along these dimensions for our device is what we call the N1 implant it's a size of about a quarter and it has over 1 000 channels that are capable of recording and stimulating it's uh microfabricated on a flexible thin film arrays that we call threads it's fully implantable and wireless so no wires and after the surgery uh the implant is under the skin and it is invisible it also has a battery that you can charge wirelessly and you can use it at home so similarly for implanting our device safely into the brain we built a surgical robot that we call the R1 robot it's capable of maneuvering these tiny threads they're only on the order of few red blood cells wide and inserting them reliably into a moving brain while avoiding vascular germs it's it's quite good at doing this reliably and in fact because we've never shown an end-to-end insertion of a robot in action we're going to do a live demo of the robot doing surgery in our brain proxy so who wants to see some insertions so here it is that's our R1 robot with our patient Alpha who is lying comfortably on the patient bed uh this is what we call the targeting view so what you're seeing is this is a picture of our uh brain proxy and the pink represents the cortical surface that we want to insert our electrodes into and the black represents the vascular Shores that we want to avoid and what you're seeing is these hash mark with numbers that represents where we intend to put each of our threads so should we see some insertions so this is another view real quick on the left is the view of the insertion area and on the right uh what the robot's going to do is it's going to peel the array uh the threads one by one from a silicon backing and insert it into the targets that we predetermined in the targeting View so there you go that's the first insertion [Applause] so we're going to see a couple more insertions the whole process of inserting uh about 64 threads in our first product is going to be around 15 minutes for this robot so yeah there's a second one that went in and we're going to do a third one there you go and then that's going to go in the background and we'll come back to it in the later part of the presentation [Music] [Applause] and as Elon mentioned we've been working very hard to go from prototype to Building Product as part of this one of the things that we did is to move our device manufacturing to a dedicated facility in Austin for scale-up manufacturing and what's important to highlight and is evident in this clip is that it's very typical for us to have our Engineers who design also work on the physical manufacturing line to build and debug and this has been extremely extremely critical in reducing our iteration cycle time and we've also scaled up our surgery so we now have a dedicated our own or in fact a double or in Austin and this is just a stepping stone before we um eventually build our own neuraling Clinic so with this product N1 and R1 our initial goal is to help people with paralysis from complete spinal cord injury regain their digital Freedom by enabling them to use their devices as good as if not better than they could before the injury and as Elon mentioned over the last year this has been the central focus of the company and we've been working very closely with the FDA to get approval and to launch our first and human clinical trial in the U.S hopefully in the six uh in next six months so hopefully this gives you a good overview of our product for the next hour we're going to go through a deep technical Dives on these topics to tell you about our technical challenges share some of our progress and preview what's coming next so with that over to near from my team who's going to talk to you about neural decoding thank you and everyone my name is brand interfaces applications our goal is to enable someone with policies control a computer as well as me or even better would like to provide fast and accurate control with all the functionality of computers that works anytime anywhere so I'm very excited to show you how we are using the N1 device with our software and algorithms to achieve this last year we shared with you a video of page of the monkey controlling computer cursor with his brain so how do we do that just a brief reminder first we record is a neural activity from the motor cortex using the N1 device we have we can record from over thousands of channels while he's playing with the joystick then we can train a neural net that predicts the cursor velocity from the patterns of his neural activity with this decoder he can then control a cursor just by thinking about it without even moving the joystick you can play with this decoder a variety of games also a grid task whereas moving the white dot towards the yellow targets every time he gets one he receive a drop of his favorite smoothie and he chooses to play this game every day here you can see his performance from early 2021 around the time we released the previous demo it's quite accurate but it's a bit slower than what we would like and cursing control is the foundation for interacting with most Computer Applications so since then we've been working to improve cursor speed and accuracy as you can see it's much much faster almost twice as fast [Music] however it it's still still a bit slower than what I can do so we are working on creative ways to improve that now speed is not enough you want the full set of functionalities and for decades most software was built for mouse and keyboard control and it doesn't make sense to reinvent this entire ecosystem for brain control at least for now so we are working and we are designing a mouse and keyboard interfaces for the brain the way we do that is by training Pages Pedro and his friends on a variety of computer tasks and then designing algorithm to predict the behavior here we can see a few example of tasks in different phases of monkey training for example left and right click click and drag cursor typing swipe typing handwriting and even hand gestures now interacting with computer is bi-directional and feedback is very important I like when I click on a button and I can physically fill the button being pressed when a potential N1 user will attempt to click they won't be able to fill it an example of how we are addressing that is by providing a real-time visual feedback that represents the strength of the mural Click by changing the color of the cursor just by typing on a physical keyboard is much faster and easier than typing on an iPad keyboard this will make the band control much faster and easier to use typing one of the most important functionalities so you already sent this message and I want to show you the behind the scene of how this message was created and here you can see again sake using the virtual keyboard tapping this message this virtual keyboard is similar to the one I use on my phone and with the speed and accuracy that we achieved so far typing on a virtual keyboard is already fast and easy however I never use a virtual keyboard when I type on my keyboard on my computer because it covers my screen and it's also much slower than what I can do with my 10 fingers we can do better for example a group from Stanford ask a person to imagine handwriting I had an imagine headlighting letters then they decoded the letters from Israel activity using this approach they were able to speed up the typing the typing rates we start this project with our monkeys but of course they don't know how to write so to mimic writing we train Angela one of our favorite monkeys to trace digits on an iPad here you can see him tracing the digit 5 and the digit 2. then we recorded his neural activity with the N1 device but now instead of decoding the cursor velocity we decode in real time the digit that he's tracing on the screen we had two main takeaways from this project one that monkeys are awesome and can learn very very complex tasks the second one that although it can increase the typing rate it requires hundreds of examples and samples of each of the digits and the characters we wanted to classify this would not scale the way we are solving that is by interaction instead of decoding directly the digits we first decode the M trajectory of this on the screen and then when we decoded the head trajectory we can use any of the Shelf handwriting classifier to predict the digits and the characters for example classifiers that are trained on an MS data set why it's so important it's important because now we can potentially decode any character in any language with only one neural decoder for hand trajectory it means that you can write in English Hebrew Mandarin or even monkey language and we can understand you wanted a banana so there are many challenges ahead of us to improve functionality and speed and I want to hand it off to Bliss to talk about the third Parts how we are making our brain interfaces work anytime anywhere laughs hello everyone my name is bliss and I'm a software engineer here at neurolink when I use my computer my mouse and keyboard where can I intend them to at least like 99.9999 of the time my goal is to enable a user with paralysis to control their computer as reliably as I can here's what we want that experience to feel like in this video you can see saki walking over to its MacBook and choosing to work on his typing task the entire decoding system works out of the box and it feels totally Plug and Play the first step to achieving this kind of high reliability is to test extensively offline a typical flow for using the N1 link is to connect over Bluetooth stream out neural activity from the brain and then use that neural activity to train decoders and do real-time inference we've built a simulation for exactly the sequence but instead of using a monkey with an implant we use a simulated brain that injects synthetic neural activity into an implant sitting in a server rack from the point of view of that implant it's in a real brain this stimulation runs on every code commit to validate that from the hardware all the way up through to the neural decoders our entire stack can achieve state-of-the-art performance however while this kind of simulation is great for integration testing of software and Hardware it's not yet detailed enough to guarantee High reliability in the real world in the real world the underlying signals we're trying to decode actually change day to day in this plot you can see the average firing rate detected on a representative channel of sake's implant each bar represents one day and you can see that each day has a different average firing rate than the previous this presents us with a very interesting problem for how to make our decoders robust day to day it can actually happen that if you train a neural decoder on one day of data and then try to use it on the next the average fire rates can actually shift enough to cause a bias in the output of the model here on the right you can see that this bias is making it hard for the cursor to move to the upper right corner you see it struggling here to make it up to the upper right and then it moves much more effortlessly down to the bottom left we're trying many approaches to mitigate this problem some examples include building models on large data sets of many days of data to try to find patterns of neural activity that are stable across days another approach we're trying is to continuously sample statistics of neural activity on the implant and use the latest estimates to pre-process the data before feeding into the model this is really an active area of research for the team and it's a critical problem to solve if we want to enable someone with paralysis to control their computer as well as I can another big problem we have is to minimize the time it takes for a spike in the brain to impact the movement of the cursor on the screen if you have lag or Jitter in this control Loop the cursor becomes hard to control leading to the kinds of overshoots that you can see here on the right I don't know one big Improvement we've made towards uh in this direction is called phase lock phase lock aligns the edge of each packet that we sent off the implant to the exact moment that the Bluetooth radio is going to wake up this minimizes the time it takes for a spike in the brain to be incorporated into the prediction of our neural network here you can see the latency distribution after phase lock not only has the mean been greatly reduced but the variance has been reduced as well this makes it easier for the user to predict the behavior of their cursor over the last year we've made tremendous improvements to the stability and reliability of our system and we've been able to demonstrate consistent high performance across many sessions and many months however there's still a long road ahead of us before the system will truly feel Plug and Play so if solving the hard problems required to ship this technology is exciting to you you should consider applying to join the team now I'm going to hand it over to Avinash to talk about how our custom low power Asic detects spikes in the brain [Applause] hi I'm Avinash one of the engineers on the Asic team we designed the custom neural sensors which include both analog and digital circuitry to record and stimulate across 1024 independent channels we Face challenges across all three major metrics performance power and area not only do we have to fit all 1024 channels into a single quarter sized implant but we also have to measure spiking activity less than 20 microvolts in amplitude and today I'd like to focus on the last challenge I mentioned power consumption is important to us because we want to give future users a full day of use of their implant without any Interruption for charging back in 2018 we were sending every sample from every channel off the device for processing which burned a ton of power in 2020 we brought Spike detection onto the chip as you may know neurons transmit information by firing so simply monitoring for these spikes and only sending these Spike Events off the implant acts as a very efficient form of compression and over the past two years we've continued to make optimizations within the Asic dropping the total system power consumption down to just 32 milliwatts and doubling battery life let's take a look at our on-chip Spike detection algorithm which makes our battery-powered implants possible we first start by applying a 500 Hertz to 5 kilohertz bandpass filter to remove noise that's out of band next we use an estimate of the noise floor to generate an Adaptive threshold per Channel and finally our Spike detector module identifies three key points of a spike identifying three points allows us to detect not just the presence of a spike but the shape of a Spike as well this can be extremely important for distinguishing between multiple neurons adjacent to a single Channel today I'd like to focus on one of the many optimizations that we've made in our latest chip this one specifically cutting system Power by 15 percent note that neurons Spike relatively infrequently which means that our Spike detector spends a lot of time searching for the first point of a spike and very little time searching for the other two points of a spike that only occur after the threshold is crossed we can use this characteristic of the input waveform to reduce memory accesses within the chip by 30 percent let's take a look at how that works our Spike detector is implemented as a single functional unit that's shared across all channels with an SRAM to buffer the state of each Channel as a sample comes in its Channel state is read from SRAM an incremental Spike detection step is run and then the updated state is written back to SRAM since this is happening 20 million times per second across the implant each of these accesses add up quite quickly in our latest chip we split the state into two parts a hot State and a cold state the hot state is accessed on every cycle while the cold state is only accessed once the threshold is crossed reducing the average axis width and saving power we're also working on a Next Generation stimulation focused chip with 4096 channels still within the footprint of our current chips in addition to increasing the channel count we're also increasing the drive voltage so we can get better activation per Channel and to support this higher Channel count as well as a broad range of future applications that you'll soon hear about we're adding an arm core onto the chip and finally since these chips are the same size as our current chips we can still put four of them together into a single implant for a total of 16 000 channels still within the size of a quarter [Applause] very hard to improve the power consumption within the implant but we've also been working very hard to improve the charging experience of the implant which Matt will talk about but first the robot has just completed inserting all 64 threads so let's take a look thank you this is a view of the insertion site similar to the one that DJ showed you earlier but instead of the targeting reticles if you look closely you can see that all 64 threads each carrying 16 electrodes have been inserted into the brain proxy while avoiding vasculature and all just within the past 20 minutes let's hand it over to Matt now to continue the technical deep dive foreign head of brain interfaces electrical engineering our fully implantable N1 device depends on a battery for continuous operation when that battery is running low charging is accomplished through Wireless power transfer however unlike many consumer electronic devices which can simply offer a physical connector charging a fully implantable device poses several unique challenges first the system must operate over a wide charging volume without relying on magnets for perfect alignment the system must be robust to disturbance and complete quickly so as not to be overly burdensome however most important is safety in contact with brain tissue the outer surface of the implant must not rise more than two degrees C in pursuit of these goals our charging system has gone through several engineering iterations the first if you watched our Pig demo in August of 2020 Gertrude was implanted with a version of the N1 charged with our first generation charger this device was implemented in a small Puck package and later separated into a remote coil and battery base this charger was challenging to use however we learned a lot through its implementation our current production charger which charges our current generation of implants is implemented in an aluminum battery base which also includes the drive circuitry a remote coil four times the size of our original device also disconnectable this uh this remote coil has increased switching frequency driving improved coil coupling this charger is in use today including several applications within our engineering and animal Test Facilities I'd like to show you one of these applications here with a device we call our simple charger and the coil has been embedded into the habitat with the addition of one new outer control Loop plus a banana smoothie pump The Troop has been trained to charge themselves so let's see how pager charges his implant on the right we're streaming real-time diagnostics from pagers N1 when he climbs up and sits below the coil you can see the charger automatically detects his presence and transition from searching to charging we see the regulated power output on a scale of zero to one and the current driven into his battery I mentioned earlier that we improved the coil coupling however the high quality Factor coils exhibit good charging performance over relatively larger distances but as they're brought uh closer to the implant what you see is a peak splitting effect where the the best highest efficiency power transfer is pushed up into higher frequencies outside of the ism band required for compliance with regulated radiated emissions in our next Generation charger we address this problem by the introduction of dynamic tuning shown on the right this allows us to in real time adjust the resonant frequency of the transmit and receive coils so that we can change their properties just ahead of degraded performance the electrical engineering team is currently engaged in developing a third generation charger notable improvements include bi-directional near field communication this has allowed us to reduce the control latency and improve the thermal regulation improve thermal regulation results in Faster charge times and now Julian will tell us about how we test the N1 thank you very much Matt my name is Julian and I lead the embedded software group on the brain interfaces team so when we started building implants we had a small manufacturing line and to collect data from an implant you would manually walk over with your laptop you would connect and collect the data of Interest but our goal is to make an ultra safe and Ultra reliable implant and so to do this we scaled up the manufacturing line now testing throughput and data collection capabilities so firstly we added a large suite of acceptance tests to the manufacturing line these test the functionality of each component and the final assembly implants coming off the line are then subjected to bench top testing accelerated Lifetime and animal models we then collect data from these implants around the clock this data is processed by a series of cloud workers and displayed in an aggregate manner and then finally all of this information feeds back into our design process and empowers our Engineers to answer any question about any implant at any time I'm now going to walk you through different parts of this infrastructure starting off with firmware testing so the implant contains a small microprocessor running firmware to manage a whole bunch of its operations and before we release a firmware update we want to rigorously test it with both unit and hot around the loop tests also known as hilltests so to do a hill test what you do is you instrument the battery you instrument the power rails the microprocessor and then we connect to each device with a Bluetooth client and then we walk the devices through various scenarios to test things like power consumption real-time performance security systems fault recovery mechanisms a lot of different things in our original implementation of these systems we used off-the-shelf components to start automating tests quickly however these systems were constructed in a relatively autism fashion and were very difficult to maintain and this meant that testing quickly became the bottleneck for development so to alleviate this the hardware and software teams developed a new system which integrates all the required components onto a single baseboard we can then put the charger and implant Hardware on individual modules that plug into this baseboard including one board with opposing coils so that we can test charging performance this architecture allows us to rapidly iterate different Hardware prototypes because we can simply drop them into this system and reuse all the testing infrastructure additionally we can host the current and next generation of our mural Asics onto fpgas and plug those into this board as well and that allows us to test a whole extra layer altogether so that's how we generated this rather inceptive image here on the right what you're looking at is spiking activity emitted from some of our simulated neural sensors streamed through the entire system over Bluetooth and then displayed on a phone this allows us to test everything in one system from Chip to Cloud this system is one-fifth the cost one-fifth the volume and is very easy to manufacture this allows every developer to have a personal unit on their desk and it also allows us to test to shot the entire test Suite over a large number of these units mounted into a rack all of this has greatly accelerated our rate of development let's look next at how we monitor the implants Electronics the battery and the enclosure so the implant will periodically capture all of its Vital Signs and commit those to flash and then upon next connection with one of our recording stations it will stream that data off so for instance if we look at humidity we can get an understanding of the Integrity of the implant's enclosure and by looking at battery voltage and power measurements we can gauge battery health all of this is done automatically without any intervention giving us 24 7 visibility into the quality of every single device additionally we can use this infrastructure to request High Fidelity information on demand so that we can investigate different anomalous situations so for instance in this particular scenario we were trying to track down the source of some spurious spikes that we were observing on different channels and so we requested roll wave samples directly from those channels capturing good quality neural signals requires intact low impedance electrodes and so this is also something we monitor very closely with dedicated circuitry on the neural sensor so how do we do this we do this by first using an onboard DAC to play a test tone on a single Channel and then we record using our adcs simultaneously we record the response signal on both that that channel and physically adjacent channels not only can we measure the impedance of every channel with this but we can also map different physical phenomena to different characteristic signatures so for instance an open channel will appear as a very large response on the channel and shorter channels will appear as a large response on neighboring channels by looking at the purity of the signal coming back we can also validate that the analog front end of the neural sensor itself is operational in our original implementation of doing these impedance scans it took four hours to get through all 1000 channels but by paralyzing the tests down sampling filtering and then reducing the amount of information we have to stream off the device by moving a lot of the calculation to the firmware side we're now able to scan all 1000 channels in just 20 seconds this means that we can run impedance on every implant every day and then our internal dashboards can play back a history of this impedance so that we can get a really good quantitative insight into that interface between biology and electronics now that you have an idea about how we test and monitor our implants I'm going to hand it off to Josh who's going to tell you about how we get feedback even faster by accelerating our implants to failure thank you hello my name is Joshua Hess and I'm an engineer on the brain interfaces team we are responsible for the implant system design as well as many of the manufacturing and testing tools Julian just talked to you a little bit about some of the ways in which we test our implant Electronics hardware and software but what about the entire system as it relates to longevity in tissue one of the ways we've addressed this is with the development of our in-house accelerated lifetime testing system the system allows us to expedite and capture long duration implant failure modes at scale to rapidly increase our pace of iteration even better the system also significantly reduces the amount of tests which require animal models both for Implant prototypes and of course longevity testing so how does the system work on a very basic level it comes down to three things first we want to mimic the internal chemistry of tissue next we want to accelerate these chemical interactions as well as diffusion with our implant materials and finally we want to aggressively cycle the internal Electronics of our implant with these things primarily the first two we have achieved a conservative 4X acceleration Factor by the erroneous relationship in other words every day our implants spend in our accelerated system is equivalent to at least four days spent in Vivo historically one of our greatest challenges has been the battle against moisture Ingress into our implants so we continuously monitor the internal humidity to watch for abnormal rise here in white you can see some internal humidity data from implants in some of our animals for the duration of over one year as you can see our internal humidity sensing is so sensitive it can even attack to the very small and slow humidity rise just from diffusion through our implant materials now in blue you can see that same internal humidity data but from devices in our accelerated system now if we adjust this data for our acceleration Factor you can begin to see not only the agreement in this data but also just how far into the future the data extends now in red you can see a device which has failed in our accelerated system this device showed an abnormal increase in humidity over the duration of many months before implant electronic failures occurred so how do we build the system well we started building the first system prototype just after the kova shutdown had begun in early 2020. so we had to get a little creative as you can see our first system prototype was a little Scrappy and operated out of one of our Apartments as indicated by the carpeting although Scrappy the system allowed us the fastest path to start testing our devices tuning Our working fluid chemistry and checking our constraints we also immediately started root causing observed failures in early implant prototypes fed that information into the next prototype designs and literally rinsed and repeated over the duration of just a few months the system was built out totally custom and highly iterated with two system versions and countless minder iterations leading us to our currently operated third generation system which achieves high density testing with automatic in vessel charging as well as automatic data collection the system also features an implant sled assembly which accepts brain proxy material such that the implant can be installed and inserted by the surgical robot just like you saw a few minutes ago we also integrated the system into a high density rack mount form factor along with the centralized fluid management system both for chemical uniformity across vessels and also reduced operational maintenance the system has been in operation for the last year and a half and has had its fair share of challenges since the system itself is undergoing the same accelerated abuse as the implants within it it has been extremely challenging to design build and maintain a system of the scale while keeping it robust even against itself so what comes next well we've started work on our fourth generation system and have totally redesigned it from the ground up to be a hot swappable single implant per vessel design partly inspired by high density compute servers with this new system we will achieve a whole new level of density robustness and scale we also intend to have many of these systems operational in the pursuit of capturing even the lowest frequency Edge case failure modes with this we will have thousands of implants testing in pursuit of these goals we've already started work building out the system but there is still a lot left to do there are also many exciting challenges ahead of us such as introducing mechanical stressing brain proxy micro motion and Ethan replicating tissue growth around the threads for more complete and representative accelerated testing so now that you've heard some of the ways in which we rigorously test our implant designs before production for surgery Christine is now going to take you through a detailed look at our surgical process thanks Josh hi everyone I'm Christine leader of the surgery engineering team to get an N1 device it's essentially these steps targeting and the incision drill the craniectomy remove the tough outer meningeal layer called the dura then insert the thin flexible threads of electrodes place the implant into the hole we created and then that's it you've got an implant Under the Skin look ma no wires just kidding uh I mean seriously no wires but I don't actually have one the surgical robot does the thread insertion part of the surgery this is because it would be very difficult to do manually imagine taking a hair from your head and trying to stick it into a Jello covered by Saran Wrap and doing this at a precise depth and position and doing this 64 times within a reasonable amount of time and a neurosurgeon would probably not like it very much if we asked them to do this for the surgery so we have the robot that you saw doing its tiny dance I sort of wanted to call it Tiny Dancer but it's called R1 which is also great the rest of the surgery is done by the neurosurgeon in order for us to make a accessible and affordable procedure we need to revisit this I'll tell you why when I was in school my dad lost the ability to walk and to use his arms and even to speak he was diagnosed with ALS we would look on the internet and you could see maybe one person here or there who had some cool custom robotic assistive device but it was deeply frustrating how limited were the options available to him and there's hundreds of thousands of people with paresis not even counting people with other conditions that our device might be able to help meanwhile there's not that many neurosurgeons maybe about 10 per million people and it takes about a decade or more to train a neurosurgeon and they're already generally very busy and as you can imagine the time is very expensive so in order for us to do the most good and have an affordable and accessible procedure we need to figure out how one neurosurgeon could oversee many procedures at the same time this might sound sort of crazy but probably so did laser eye surgery before Lasik made it normal lasik's been around for about 30 years and counting in the beginning the laser robot did just the most fundamental core part that it had to do and the surgeon did the rest and over the iterations the surgeon has to do less and less and the laser robot does most of it and it's a highly compelling procedure it takes just a handful of minutes and often gives life-changing results since I joined in 2017 we've also done a handful of iterations to optimize the threat insertions of the robot one of the challenges that we've had to face has to do with the optim mechanical Packaging so as you can see here there's about three primary Optical paths that are really valuable for a staff reliable threat insertions one is the visible Imaging of the needle inserting a thread and then another is the laser interferometry system called OCT Optical coherence tomography that gives us the precise position of the brain while it's moving in real time and then also we have to provide lighting and illumination to see what's going on in the visible visible light camera and doing all this where the needle is at the bottom of the craniectomy especially when it's close to the skull wall can be pretty difficult to fit everything and be able to see it so the way that the team solved this is by putting all three of these Optical paths into one Optical stack using Photon magic or polarization whatever you want to call it and that enables us to do vessel avoidance in real time so as I mentioned the brain is moving and where we place Targets in the beginning may not be where you want to insert at the moment the needle is going down there so the robot can actually detect the vessels and then determine if we're going to insert onto a vessel or not if it's safe to insert and then that way we can avoid inserting onto major vessels and that brings us to the robot that we have here today there's still a lot for us to do to get to that procedure where we reduce the role of the neurosurgeon and make it affordable and accessible the primary the two elements of the surgery that demand the most skills from the neurosurgeon are the craniectomy and the directomy Alex and Sam are going to tell you a bit more about how we think we can get rid of the directomy step so that leaves the craniectomy in neurosurgery if your craniectomy is small enough you can use a standard tool called a perforator which makes quick work of this job but for a larger craniectomy the surgeon has to rely on their skill in order to accommodate the variability Patient to Patient in skull thickness skull hardness even within the same patient in the same craniectomy you can have different skull thicknesses for example in addition if we can make something that has a very high Precision craniectomy we can open the design space for future ways of mounting the implant to the skull so I'll show you a few of our prototypes ultrasonic Cutters like what's on the screen and oscillating Cutters have the benefit of not cutting soft tissue you can cut the bone and not the brain but however as you can see here our ultrasonic cutter prototype created quite a bit of heat to cut at the rate that we wanted so onto the oscillating saw here we designed a blade to minimize cut time and also conducted sound and also Heating and as you can see you can cut through hard things like bone but not soft things like skin it's simple and it's it works however if you wanted to cut an arbitrary depth or arbitrary shape the oscillating saw just won't cut it I was afraid no one would get it if you guys are smart so there's a time-tested solution for uh drilling arbitrary shapes which is a CNC drill the challenge with us doing this on a person is that we need to make sure it cuts reliably every single time doesn't cut too deep and a few ways that we're using feedback to you know make sure we don't cut through the brain or force feedback and also impedance and if I could get a volunteer just kidding maybe next time um but yeah so this is a some insight into some of the things we're working on to make an accessible and affordable procedure and now Alex is going to tell you a bit about our next Generation developments [Applause] thanks Christine I'm Alex I'm a mechanical engineer here on the robotics team now that we've covered the technology and surgical process for a current device we'd like to cover some of our next Generation development projects I and the next couple speakers would like to talk about one of those projects which is enabling device upgradeability you've gotten to hear about the advancements we've made over the past year we've improved implant robustness battery and charging performance Bluetooth usability realistically every new device version is going to be significantly better it'll be more functional it'll last longer we need to keep this new technology accessible for our early adopters this means that we need a solution to make device upgrade or replacement just as easy as it is to initially install as many medical device companies have found this is a challenging problem the body's healing response doesn't make this easy so this isn't solved yet but we've made significant progress towards enabling this that we'd like to cover today now we'll have to start with some background as to what makes device upgrade challenging and we'll start with the anatomy Under the Skin you have the skull below that the dura a tough membrane that separates the bone from the brain and between the dura and the Brain you have the pier arachnoid complex a fluid-filled suspension for the brain to install the device the surgeon removes a disc of skull and Dura to expose the brain surface the device then replaces the removed material the challenge is here at this interface over months all empty volume is filled by tissue encapsulating the device and the threads the device would be trivially easy to remove because of the thread's small size they would slip right out of the brain it's the tissue layer that forms above the surface that makes a removal challenging we built tools in-house to study this response and characterize it such as histology and micro CT in these images you can see that layer of tissue that has formed above the surface encapsulating the threads and adhering to the surrounding tissue we've explored many different avenues for Designing around the ceiling process and finding a solution to make device upgrade seamless our best successes have come from making the procedure less invasive instead of directly exposing the brain's surface we instead keep the dura in place maintaining the body's natural protective barrier this prevents encapsulation of the brain's surface and really this is actually a huge win for making the surgery simpler and safer as Christine alluded to however this doesn't come for free the dura is a very tough opaque membrane as you can see in these sem images it's composed of a dense network of collagen fibers these offer an array of technical challenges for inserting our electrodes one of those challenges is Imaging through the dura as you can see on the left our current custom Optical systems offer pretty incredible capabilities for Imaging the exposed brain surface however as you can see on the right once the dirt is in place you can't see the dense vasculature at the brain surface the dirt is in the way there's simply too much attenuation to solve this problem we're developing a new Optical system that uses medical standard fluorescent dye to image vessels underneath the tissue here you can see that die perfusing through the vessels highlighting them there's still a lot of work engineering work to go to prove accuracy and repeatability of this system but once that's done this will allow us to Target and avoid blood vessels underneath the dura we're also exploring applying our laser Imaging system to deeper tissue structures in the bottom left you can see a section of the tissue layers underneath the dura this image is compiled from multiple volumes from our Optical coherence demography system you can see the collage of those volumes above in the future these new systems when combined with correlation to pre-op Imaging such as MRI will enable precise targeting without directly exposing the brain surface now Imaging isn't the only challenge that comes with the tough Dural Anatomy now I'd like to hand it over to Sam to talk about some of the challenges of inserting our electrodes through this membrane thanks Alex hey I'm Sam and I lead the needle manufacturing and design team so as Alex mentioned the same properties of the dura that make it a good protector of the brain also make it really difficult for us to insert the threads into in humans the dirt can be over a millimeter in thickness which doesn't sound like a lot but compared to our 40 Micron needles it actually is a lot for example if you've scaled up the needles to the size of a pencil the dura would scale to over four inches in thickness take a look at how far you have to zoom in to even see it by the time the features of the needle come into frame you can see individual red blood cells in the same frame this is this is just wait this is a real-life sem image of our latest design on the left there you can see the end of the thread in the middle is the needle and on the light is actually a piece of my hair so yeah it's extremely small and besides being really small there's a lot of other challenges associated with designing this um One Challenge is that we have to use the needle and the protective cannula that it sits in to grab onto the thread and to hold it while we peel it from this protective silicon backing and then we have to keep holding it while we bring it over to the surface and then release it from the cannula during insertions another challenge is that the brain is really soft beneath the tough Dura and so if the needle isn't sharp enough it'll just keep dimpling the surface without puncturing and if this free length gets too long it can actually just Buckle the needle like this is that we don't just have to get the needle through we have to get the thread through as well so we really have to focus on optimizing the combined profile of the needle and thread together these are just some of the challenges associated with designing something like this before we found that the key s problem has been improving on our speed of iteration but let's look at how we make these things in the first place so we start with a length of 40 Micron wire made out of tungsten and alloyed with a little bit of rhenium for added ductility we designed this femto second laser Malin house to cut the features of the needle and cannula and it can do this with sub Micron precision we spent a lot of time this year turning this thing from a science project into an industrial system just a couple months ago it took a skilled operator 22 minutes to make a needle and even a skilled operator could only get about 58 yield today that same process takes just six minutes and anyone can get 91 yield with just a few minutes of training with only one click the mill cuts and measures the needle in cannula and uploads the measurements to our limb system so that the robots can use the exact dimensions for each needle that it uses now this is all for a current design though and we've had a couple years to optimize the manufacturing process of it the current design has served us well so far but it doesn't quite protect the thread well enough to get through the tough Dura so like I said we had to come up with something new and we needed to be able to iterate on designs quickly unsurprisingly there's no page in machinery's handbook for this kind of thing so we dug into the science of femtosecond laser ablation and figured out a workflow that allows us to use our laser Mount much like a CNC mill this allows us to iterate several times this allows us to iterate in under an hour for new designs allowing several iterations per day when we're really on a roll as a result the latest design seen on the right can actually insert through nine layers of direct a totaling uh three millimeters on the bench top this is far more than we could ever expect in a human with significant margin [Applause] the needle isn't the only part of the puzzle though as you can imagine all these designs here work with different threads so we need a way to iterate on that as well and we do this by having our microfabrication process here in-house this summer we completely rebuilt our clean room in about nine weeks which among other things greatly reduced particulate counts which allows yield and throughput to greatly increase this combined with all the other great improvements the microfib team has made allows us to iterate on new designs in just a matter of days the last piece of the puzzle though is testing we can come up with as many new designs as we want but unless we have a way to actually test them in the right conditions we won't know what to tweak or even worse we'll spend time optimizing for the wrong things take this failure mode for example um a few months ago we got to the point where we could pretty reliably insert through the dura but when we took the proxies and put them in our micro CT Imaging we realized that our hold on the end of the threads was actually too strong and we were pulling them out just a little bit underneath the surface by the time we solved the problem we realized that this issue was very sensitive to the properties of the surrounding material or tissue we could make a proxy where this never happens and we can make another proxy where this happened every single time and this highlights why it's crucial that we spend time making our benchtop tests Mash tissue as accurately as possible I'm going to pass it off to Leslie now who's going to talk about how we've been doing that thanks hi I'm Leslie and I lead microfabrication r d and part of what we're interested in is understanding the biological environment our implant and threads experience once they're fully installed in the body learning directly from biology though is inherently slow so in order to move fast we're developing synthetic materials that mimic the biological environment this allows us to learn as much as we can on benchtop and start taking steps away from the industry standard of animal testing developing accurate proxies though is challenging the implant environment is made up of many anatomical layers that all have unique properties and as time goes on and the implant site heals new tissue forms filling any available space in addition to that motion related to cardiovascular activity and head movement introduce added complexity so to start addressing some of these challenges we're engineering materials using feedback from biology this may involve mechanical characterization of tissue or analysis of interactions at thread tissue interfaces much of this characterization is even done during surgery itself by using custom hardware and software that modifies our surgical robot to double up as a sensitive characterization tool we then use the data collected and feed it back into optimizing our materials so that they behave mechanically chemically and as shown here structurally just like biology we've come a long way from our humble first brain proxy shown here sitting on a plate and consisting of agar and a pyrofoam sheet and while simple it allowed us to perfect robot insertions through countless bench shop tests today our proxy is slightly more complex where we've upgraded to a composite hydrogel based brain proxy that better mimics the modulus of real human brain we've also Incorporated a duraproxy and developed developed an injectable soft tissue proxy that so far has allowed us to perform benchtop mock explant testing we have a super long wish list for our proxy of the future but some of those items include a surgery proxy with integrated soft tissue brain bone skin or even a whole body a brain proxy that simulates motion vasculature and electrophysiological activity and a biological proxy to test biocompatibility and electrical stimulation there's a ton of ongoing work getting us closer to our proxy of the future including work on lab-grown cerebral organoids as shown here and all of this will get us closer to a future where we learn more and iterate faster on benchtop and reduce our Reliance on animal models or even when they replace them completely and with that I'll hand it over to Dan who will be presenting a very exciting Next Generation application thank you thank you Leslie my name's Dan and I came to work at neurolink after following a career in visual Neuroscience research I was inspired to join this company because I saw in our device the potential to restore Vision to people rendered Blind by eye injury or disease there are a number of particular characteristics of our device that make it uniquely suited to this application firstly as well as being able to record from every channel we can stimulate neural activity in the brain by injecting current through every channel this is important because it allows us to bypass the eye and generate a visual image in the brain directly secondly our device can have an enormous number of electrodes for a visual prosthesis this is important because the more electrodes you can have the higher density of an image you can create in the brain thirdly thanks to our robot we can insert these electrodes deeply into the brain now this is an important thing for a visual prosthesis because the human visual cortex is buried deeply in a fold in the medial face of the brain called the calcarine sulcus in this image I've highlighted the calcarine Cyrus in red in an MRI it contains a map of the visual World visual field it's about the surface area equal to a credit card on each side and if you unfold it and flatten it you see that the image is inverted it's upside down but more interestingly it's mag it's distorted so that the central part of the visual field the fixation point is greatly magnified so for example if you look at this image of Lincoln if you look directly into his right eye everything to the left of that fixation point is directed to your right visual cortex and everything to the to the right goes to your left visual cortex his eye even though it's very small in the image is magnified in the brain to occupy nearly a quarter of the surface area of the visual cortex over the last half century visual neuroscientists have developed a profound understanding of visual processing in the brain what's Driven most of this research is recording from single cells in the cortex usually of macaque monkeys one of the seminal discoveries was that every cell in the visual cortex represents only a tiny part of the visual field your perception is made up of a mosaic of tiny receptive Fields each belonging to a single cell in your visual cortex so if you record from one of these cells in a monkey say in this location you can find a very tiny region of the screen where a light stimulus will cause modulation of that neuron another location in visual cortex will have a location Elsewhere on the screen in this case in the lower visual field these regions are called receptive fields we've inserted our device into the visual cortex of two rhesus monkeys whose names are code and dash that means we can record activity from their visual cortex generated by their not their normal home environment as they roam around but as we all know monkeys love banana smoothie that means we can easily teach them to fixate points on a screen and reward them we can reward them very precisely because we can track the location of their eye using an infrared camera one of the things this allows us to do is to plot the receptive fields for every neuron that we can record with a single device now we do this by showing the animal a movie of random checkerboards whilst you fixate steadily on the screen then we take only the frames of the movie that generated a response in the cell and averaged them all together this is a technique known as reverse correlation it's generally used quite widely in visual Neuroscience for this purpose and this is an example of a receptive field plotted with this technique the central cross is the fixation point and you can see the little red and blue regions of excitatory and inhibitory receptive field these regions give cortical cells some of their characteristic properties and record all receptive fields from all the electrodes at the same time and if we take all these receptive fields and accumulate them together overlap them and place them on a on a computer monitor for scale at a typical viewing distance you begin to get an idea of how much the visual field we can cover with this preliminary device many of the receptive fields are close to the phobia so close to the fixation point and that's partly due to the magnification I talked about with the phobia but there's also a scattering of fields in the periphery these are from recording sites deeper in the brain in the calcarine sulcus so far I've only talked about recording information from the cortex but to produce a visual prosthesis we need to stimulate so if we stimulated the cells whose receptive fields are in this location we would produce a perception of a flash in that location that only the monkey can see how do we know that the monkey sees it how do we know what it looks like well unfortunately we can't ask them what they see but we can train them to tell us something about that phosphine we start by training the monkey to fixate a central point on the screen like this white dot and we start by presenting real visual stimuli on the screen and rewarding the monkey for making eye movements toward those stimuli so here we flash a white dot and the monkey makes an eye movement towards it symbolized by the Green Arrow we then choose another random location and reward the monkey for making an eye movement towards it once he's got good at this task we can begin to interleave these real stimuli with electrical stimulation of electrodes and produce a phosphine the monkey sees the Flash and naturally makes a card towards it this tells us not only where in the visual field The Flash occurred but we can also change the current that we inject in that electrode to see how often he makes that's the card and noticeable or how big perhaps the stimulation phosphine is that we're producing look at code performing this task I want to show you first at one quarter speed uh there's a visual Flash and he makes an eye movement towards it we the monkey can only see what is white on this screen he can't see his own eye movement and you can't certainly can't see when we stimulate but here we stimulate and he makes the same circad to the same location because we stimulated the same electrode nothing appears on the screen at that time and he has no other cue to make that eye movement let me show you this in real time you can see monkey monkeys like to work very quickly and when we stimulate he makes that's the card in real time and looks like he's had enough so what I've shown you is a way to produce a phosphine in the visual field this is not something new in visual Neuroscience but if you think about that phosphine as a single Pixel in a visual image all we need to do is scale up and produce a great many more pixels and have them covering the visual field this is a schematic of what a visual prosthesis using our end device might N1 device might look like a camera the output from a camera would be processed by an iPhone for example which would then stream the data to the device and the image would be converted into a pattern of stimulation of the electrodes into visual cortex with a thousand electrodes we might be able to produce an image resembling something that you see there on the right but as Avinash told you our next generation of the device will have 16 000 electrodes if you put a device on both sides of your visual cortex that would give you 32 000 points of light to make an image in someone who's blind our goal will be to turn the lights on for someone who's spent decades living in the dark thank you thanks very much I'll pass you over to Joey who's now going to talk about another very exciting application of our device thank you Dan so my name is Joey I'm an aero engineer and I'm the head for the next gen team at erlink so for persons with spinal cord injury the connection between the brain and the body is severed the brain continues functioning normally but it's unable to communicate with the outside world you've already heard about how we can use the N1 link as a communication prosthesis to help someone with spinal cord injury control a computer or a phone but it can also be used to reanimate the body let me show you how first a little neuroanatomy movement intentions arise in motor cortex and are sent down long nerve fibers through the spinal cord these are upper motor neurons in the spinal cord they synapse that is make a connection with another motor neuron a lower motor neuron which sends these movement intentions to the muscles which contract and in turn you have movement while of course there are many other circuits involved in voluntary movement you can think about the spinal cord as many pairs of these two connections and in spinal cord injury one of these connections is severed unable to make the muscles contract let's Zoom a little bit further so here you can see on the left across a cross section of the spinal cord with a fiber coming down schematically this travels through the white matter tracks this is the upper motor neuron and then it synapses within this butterfly shaped region of gray matter in what's known as a motor pool in the motor pool the lower motor neuron descends out the ventral roots to the muscles which contract and then the sensory consequences of those movements for example the touch of your hand against an object returns of the spinal cord through the dorsal roots and Ascend the spinal cord up into the sensory regions of the brain again in spinal cord injury this connection is severed if we could place electrodes into the spinal cord say in a motor pool adjacent to lower motor neurons we could stimulate those neurons activating them and in turn causing the muscle to contract and movement to occur but this is very hard to do the spinal cord is quite delicate and it moves significantly within the Bony spinal canal this could cause damage to the electrode it could cause damage to tissue or both but our electrodes are small and flexible and our robot is able to insert them deep into tissue perhaps all the way down into the ventral horn spinal cord and so we have done just that here you can see A View From the R1 robot it's a targeting View and we've placed electrodes across many millimeters of the spinal cord and the R1 robot is able to insert those electrodes deep into the ventral horn into motor pools in very close proximity to lower motor neurons this is important because it allows them to have a localized connection to those neurons and activate very precise movements now to track movement it's very common to use motion capture markers like you might see in the production of a movie these can be placed with a light adhesive and you can see me placing these on my hand we're going to use these markers to let us zoom in on movement in the next couple of slides okay so here's a pig walking on a treadmill and you may have seen something like this before in a previous knurling presentation but unlike before this pig has more than one neuralink device there's a device in the brain but there's also one in the spinal cord and we can stream neural data from this device these devices in real time and use them to do things like decode the movement of the joints of the pig so here you can see on the left a Time series of the hip knee and ankle and we're decoding those those movements so this is super cool but that's actually not what we want to do we want to go in the other direction we would like to stimulate the spinal cord and cause movement to occur okay so let's do that so here's a pig a happy and healthy Pig doing what pigs like to do which is root around for food and snacks and as you'll see on the floor there's a blue square this is a voluntary engagement Zone where the pig places itself indicating that it's comfortable to receive stimulation when it's in the zone we stimulate and if the pig leaves the stone we'll stop stimulating uh and as before you can see we're able to track the position of the joints and also stream neural data as well okay so let's stimulate an electrode so here's one electrode on one thread that when we stimulate causes a flexion movement of the leg so on the left you can see the movement of the joints and you can also see the time series of the stimulation pattern in yellow so the leg is moving up here's another electrode which when we stimulate causes an extensor movement this is actually a little harder to see because the leg is straightening and the hips are shifting but if you look carefully you can see how this is the leg is moving we can stimulate on a great variety of threads and produce different movements and actually sequence them spatial temporarily to provide patterns so on the left you can see a Time series of different stimulation on different electrodes you can see the movements of the joints and on the right we're zooming in on muscle activity that gives us an idea of the kind of strength and power and specificity of those movements as well so in addition to doing sequences we can also achieve sustained movement these are powerful muscle contractions of the sort that you might need for standing or other load-bearing activities and are really crucial for interacting through the world okay so stimulating the spinal cord is only one piece of the story you also have to get like command signals for the stimulation of the spinal cord unfortunately we have a way to do that we have the N1 link that you've already heard about placed in motor cortex how would that work so we place threads in motor cortex and record spikes these spikes would be wirelessly transmitted in real time and decoded into patterns of stimulation stimulation would then be delivered to the ventral Horn of the spinal cord to the appropriate motor pool for the muscles that we like to activate we then stimulate activate those lower motor neurons which causes the muscles to contract and movement to occur now of course movement without sensation is actually kind of difficult just think about what it would be like to try to move your limbs if they're numb but we can also get sensory information as well so the sensory consequences of your movement can be recorded in the dorsal Horn of the spinal cord in the form of spikes for example here a feather touching the hand these spikes can in turn be decoded in real time sent to patterns of stimulation to either the same and one device in the brain or perhaps a different one in a sensory area stimulation of that part of the brain would cause percepts of touch and proprioception closing the loop so putting those two Loops together we have motor intentions decoded from the brain used to stimulate the spinal cord causing movement and then the sensory consequences of those actions being recorded in the spinal cord to stimulate the brain causing perception now we have a lot of work to do to achieve this full vision but I hope you can see how the pieces are all there to achieve this and if you find this Prospect as exciting to you as it is to me I hope you'll consider joining us here at neurolink [Applause] so foreign [Music] it would also be great for the scientific Neuroscience Community to access some of these tools do you have any plans to make these available to neuroscientists yes yes we do um so um that's a great question is uh I think there's probably a lot that could be figured out if we provide the uh surgical robot and devices to Neuroscience uh research departments at your universities and hospitals so I think at the point of which we have we need to be in production with the machines and obviously have the FDA approvals but I think uh it would make a lot of sense to provide this to research universities and hospitals follow up the question is uh of the data that we have the data sets that you've collected are there any that you plan to open source for the scientific community yeah I think that would be that would be fine I think uh yeah sure absolutely because I think it could be really interesting for people uh uh to build upon that um and mail and build Foundation models for the brain yeah that's it's a good point yeah like actually no problem with uh just publishing it on our website you can use it if you want looking forward great thank you for the very wonderful presentation so I have one question so as we will know for implantable electrode either for stimulation or recording after we implant the electrode Scar Tissue will grow around the electrode and especially for uh recording the signal we get will become smaller and smaller after long-term uh implant uh how do you solve this issue uh so uh for context I'm Zach I leave the microfabrication team on brain interfaces uh I don't think we can solve it specifically but uh one thing one advantage we have is both the flexibility and the small size of our threads to try to limit that scar tissue and that damage and uh some future work that we have started working on that we'll continue working on is pushing the size of the threads down um just to try to limit the immune response and really limit that scar tissue growth uh I actually have I want to follow up so do you think it will be helpful to actually load some drug on on the surface of your electrode or some other way well I think like maybe the just the question is like what what sort of signal degradation have we seen over time um and uh you know basically just does it does it work a year later does it work two years later um it does so yeah yeah so uh that's a good point so in terms of thread longevity specifically um really the gold standard that we can use to assess is the data we have from our animal participants uh and so for that uh I'm not sure if it was mentioned before but uh the longest data we have right now is for an animal participant who has 600 600 days with useful functioning channels where we were doing uh something useful with the the signals for BCI uh and then with the newest version of our device we have uh sort of a collection of participants who are at or near one year of data and uh completely useful functioning BCI from that as well thank you if I may add one more thing so you mentioned uh potentially having drugs to kind of reduce inflammation so one of the things that we are actually actively working on is having some sort of biological coding to either reduce inflammation or make them slippery so you know you mentioned uh you heard from the presentation that one of the challenges that we have is removing the threats from these neon membrane tissues that are formed after implantation so there are programs like that where we're really looking at kind of incorporating some of the learnings from biology and these Coatings into our thread so that we can hopefully reduce inflammation as well as make it easier to extract we're also continuing to reduce the size of the the electrode so as when the electrode gets really small there's um the sort of inflammation response are Scar Tissue becomes minuscule so it's like a very very tiny electrode the body basically ignores this is really impressive congrats to the whole team um so as as you of course know one of the problems with current electrodes is they're rigid and they move around so you have these neural nonstationarities and I think many of us had hoped that with these very thin threads they would maybe move more with the brain and you wouldn't see that but from the data you showed over many hundreds of days there was a lot of variability so can you speak to how much do they move and do you have any idea of like why does it move can you stop it from moving is it how stable are the signals hour to hour and day to day hi I'm Bliss I'm one of the leads of the software groups in the brand interfaces team in the particular plot you were mentioning before what we were showing was the average firing rate recorded per day on a particular Channel it's as you well know pretty complicated to understand if you're recording from the exact same neuron day after day after day it could be for example that you're actually picking up a different neuron day to day and that's why you get the change in firing rate we don't think this is at least the majority cause of the situation here the reason is that if you look at sort of the spike shapes day to day even when the average firing rate is shifting a lot you still see sort of stable Spike shapes that's obviously not a fully bulletproof story but at least gives some confidence that it's not actually different neurons you're picking up however there's still is very much a chance that that could be the case in at least some part of the of the robustness now stationary story yeah cool thanks yep thanks question yeah to be clear that like the Electoral position is actually fairly stable um because you've got these very tiny basically very tiny wires with with uh that and then there's some play in the like you've got you've got the advice attached to the skull ritually but then you've got this this long so tiny wire with kind of a a coiled section so it's a it does tend to basically stay in the same place couldn't erling help realize well I mean it's uh um once you're in there you know there's a lot you could do um so um you can obviously measure temperature so you could do very early detection of a fever you could not measure uh pressure I think you probably detect that um at the very early the very beginnings of a stroke because you can see sort of like electrical signals starting to go sort of hair wire so there's actually probably a lot of um just General Health monitoring that you could do once you're in there you know and and with with very simple sensors hi um you guys all did a great job of distilling a lot of complex engineering and Science and making it wonderfully clear so great job um I wanted to ask a little bit about the stimulation I guess for the phosphines and for the the evoked movement um how are you are you thinking is this more like local stimulation is it is it juxta cellular are you steering current around how many cells are you activating how much current are you using I'm just curious what the scale of this is and whether you have a lot of precision or a lot of you know pretty profound behavioral effects too hi yeah I'm Dan and um how many cells you stimulate with a single electrode is dependent on the impedance of the electrode size of the conductive pad how much current you deliver frequency all these factors so there's a great deal of variability that we can use to customize the the shape of a phosphine or the or the shape necessarily but maybe the the intensity of the phosphine we think with our current electrodes at least in code back of the envelope calculation would be something like about a 50 to 100 Micron diameter sphere of cells are being stimulated um in a visual system the smaller that sphere the smaller and more specific you can make a particular phosphate basically the smaller the pixel in in the image you can produce so there's plenty of scope for customization of that there's actually also it's possible to get to a much higher like effective pixel count by um controlling the the the field electric field between the electrodes so uh it's not necessarily it's not a one-to-one relationship you get actually dynamically adjust the the field and simulate Farm have a have a very high neuron to electrode ratio so try to like could you get like um you know maybe 10 to 100 to 1 potentially so it's a megapixel type time basically can you see it normally but I think people would want to know that and I think that is one of the one of the possible outcomes pylon this is amazing oh can you talk about the longevity of the implant itself also how would the material of the implant would react with the brain tissue or density of the bone or bone structure thank you happy to talk about this I'm Jeremy an engineer on the brand interfaces team and I think it's good to start with data so like Zach mentioned we have an implant that was you know a monkey was performing BCI for 617 days and that was pager before being upgraded to the latest device uh for our current version of the device it's lasted for almost a year and then for Accelerated lifetime tester that Josh kind of talked about we have data from our implants from the previous version eight years of accelerated time and from the current version four years of accelerated time and Counting so that's kind of starting with the data those devices are still lasting and still going um theoretically there are kind of three fundamental factors that contribute to the longevity of the device one is going to be the seal the Hermetic enclosure of the device two is going to be the battery and internal electronics and then three is going to be the threads that Zach talked about a little bit and the channels being able to functionally record signals from the brain the seal we think will far Outlast the other two in terms of the bottlenecks so the seal just theoretically I think Josh mentioned that it is a thermoplastic polymer material so there's going to be a very small amount of moisture that diffuses through it over time and we think that that will last you know 20 plus years easily in terms of just that property and like I said we have not seen our seals fail with our current version of the device yet so we haven't really pushed the limits here for the battery and internal Electronics that's really based on usage and how much runtime you want and we are working currently on getting data to project out even farther but right now we believe that we can you know achieve 80 run time at the three year time point which would be about you know three and a half hours for a four hour run time but we're like Avinash mentioned we're we're doubling that very soon and quadrupling is what we have plans to do that so the internal Electronics really aren't the bottleneck either and so really we're attacking the threads themselves and longevity of those channels that Zach Zach can kind of talk about some of the improvements that we're doing to increase that longevity cool thanks yeah so uh as sort of mentioned before we don't necessarily have an end point as Jeremy said for uh the testing of the threads that being said we are focusing on longevity because we think this is an important uh issue to solve uh so one thing that we're doing in parallel with the current device is aggressively pursuing uh silica amorphous silicon carbide insulation of the threads which we believe will take us well beyond five years uh of longevity but of course still to be tested and in parallel with that we're just starting to look at Atomic layer deposition which we think could even push longevity of the threads much further and deposit very thin layers to keep the flexibility of the threads and that Advantage there so along with that we're also of course having to design and validate very robust benchtop testing to model uh really in Vivo conditions and look at Channel degradation so that's what we're looking at for longevity of the threads and then I think you asked about biocom pump and I think for biocomp uh essentially all the materials we're using right now I can say or at least biostable and we send out testing for biocompatibility very often and essentially what we're doing is we're using in many cases known materials from literature that academic Labs have already started to look at and sort of jumping on that and using that as a starting point thanks for answering that cool so we have another question from Twitter this is from David and he asked the team what are the biggest lessons you learned since the previous presentation it's been about two years I'm sure there was a lot of engineering done so yeah anyone want to answer what we learned in the last two years so uh one thing that we've learned in the last couple years is just how much the brain moves um on the human scale compared to you know when you start small when you make brain proxies and a lot of research starts with rodents the brain does not move that much then you get a human and the Brain can move like hundreds of microns or more and when our threads and needles are so small that motion when you zoom in looks like a mile I think to add to that um one thing is how I guess Dynamic the implant environment actually is so we've talked about like when those implant site heals SCAR or new tissue might grow and fill in the space and that'll affect like how our threads might interact in that space so that's why we've emphasized so heavily the importance of Designing accurate proxies so instead of having to wait months for an implant site to heal you can hopefully learn that information in hours I'm Alex on the robotics team I think one of the things we've definitely learned within the engineering teams is the importance of really continuous validation and testing where we're building say motion systems that are precise to single digit microns we need validation and test systems that we trust even more than that to prove that they work reliably and putting just as much focus into those validation and test systems and designing those alongside our products I think is one thing we've definitely learned thing and I don't think that we learn I think as part of BCI or the pain control and algorithm is that again building a prototype and making it work with only one monkey one pager was a great maybe a success but also relatively easy to making it work every day for all the other monkeys so actually making it a product is something that it's not easy but we are learning how to do it I mean I've learned that the brain is really squishy like way squishier than you think it's not like uh you know cauliflower or broccoli or something like that it's more like water balloon uh and then it's moving in your skull like a lot so get a squishy water balloon in a coconut is maybe a good way to think of it hello um given Bluetooth bandwidth limitations have you considered other Technologies for wireless communication foreign hey yeah I can take the first part of this question and then I'll let Matt to the second part of it um it's a great question especially as you think about how to increase and scale the number of channels that we want to record from this becomes increasingly a bottleneck for the kind of work that we want to do uh we're thinking about this in a couple ways one is just directly improving the underlying radio interfaces and I'll let Matt talk about that in a second the other way we're thinking about this is how can you be more efficient with the data you send off the implant and I think the first version that is compression so just taking your data looking at the characteristics of it find out a way to represent it more efficiently and just send off that compressed Stream So for reference right now our Bluetooth bandwidth is around 150 kilobytes per second the compressed stream of data that we send off the implant is around 50 kilobytes per second so we're doing fairly well there so far but when you start thinking about 16 000 Channel devices that won't get you all the way there so some other things that can help on the compression slider to actually just send out the output of the machine learning model rather than the input required to actually run it uh so one thing we've been trying in the background here is called decodon head which is essentially taking the machine learning models right now we're running on MacBooks that our monkeys are gaming on and moving those to actually run on the implant and this is like a super cool engineering problem if you want to talk about how to make uh complex neural networks run on what is the equivalent of a garage door opener come talk to me it's fun um yeah so that's another way to solve this problem is basically do the computationally Intensive work to just get the raw signal that you actually care to use to control something and then send that thing out of the of the implant on the radio side I'll hand it over to Matt yes so to answer your question we are looking at other radio Technologies um one in particular is uh 500 megahertz pan with ultra wideband at a couple different frequencies so this has an advantage in terms of the bit rate that you can achieve it's um on the order of six to eight to ten megabit uh there's also a latency Improvement that's quite substantial and there's also a another wireless technology that we're looking at at w band hi uh thank you all for really clear and compelling presentations um something that struck me in one of the earliest talks I think it might have been DJs was this vision for the ability to acquire new complex skills via these bcis um like the ability to perform Kung Fu and that reflects the fact that the brain is fundamentally a learning machine and yet the um many of the Technical Solutions presented later framed were framed in such a way as to try to correct for the way the brain changes over time over longer time skills drift over the course of days or um the way that the tissue might heal over time I was curious what your vision collectively was for developing out this technology that interfaces with a fundamentally plastic system that changes in complex ways over a variety of time skills days months years yeah it's a tough question yeah I think it will be kind of maybe bi-directional learning in some way in the sometimes skills that we will might fix our algorithms and we prefer to have like more stable kind of performance but of course if the over time the person in the brain will learn how to use better the bciable need to update our models so they will be kind of in the interactive kind of relationship in some way um to learn even new tasks this probably yeah will be something will over time will need to learn what the person kind of learn how to interact with the computer and then build the appropriate interface the ux and also the UI and build algorithms that will help him to control what we want just one thing I'd like to add on to what near said um yeah it actually is an advantage in some ways that the brain is plastic and learns and that can help us because we actually have to do less work in the the human in the loop we'll actually learn how to use our device better but one of the advantages of our particular approach in device is that we are trying to do an extremely high Channel count device so we can you know uniformly distribute electrodes over a functional region and then it doesn't matter so much whether things move or shift over time we can offload that to software and so we can build algorithms that change over time as well and so both those things are actually I think advantages to our particular approach we have another question from Twitter Juan wants to know what career path do you suggest for somebody that is just getting out of high school if they want to work at neuralink in the future well it's really any of the skills that we described uh so I mean we're developing uh new Chips uh you uh this material science uh you know as uh software obviously uh animal care because it's a really all the things that were listed in the in the neuralink slash careers okay that'd be a good guide I'm actually very fond of saying um you know when you flip through any college uh like booklets and look through all the majors I think you can point to every single one of those majors and there's someone at this company who either is an expert or you know have majored in that so it really is um truly truly multi-disciplinary Endeavor and I think you know just focus on whatever your um you know passionate about or whatever you're talented at and then just you know pursue that as deeply as you can and then there's definitely going to be a place for you in uh you'll end up building neural interfaces hey um we got to see the monkeys doing telepathy but could you say a little bit more about the animal behavioral training kind of their lives and day-to-day processes sure I'm Autumn I am head of Research Services which includes our Animal Care Program and as an animal welfare scientist this is a topic that I'm deeply interested in so um our training program is outfitted mostly with behavior analysts who help us think about how to remove any of the potential aversives or frustrations from our training um we think about uh conditioning as the primary which includes positive reinforcement as the primary way to train um let's see what else can I share with you yeah yeah I mean that may not be part of the behavioral training itself but we think of Animal Welfare assessment in the framework of the three R's which is refinement replacement and reduction and so when we think about refinement behavioral training does apply in in that way and where we want to remove specifically in research restraint is one of the things we make a very top goal to remove so um you saw a lot of videos today where animals were walking up to their stations because we worked really hard to remove any requirement to restrain the animal um anything else yeah I can just yeah well just on top of the last point you said um just as an engineer here one of the things that is really inspiring and really cool about this place is that we do get to work on a lot of technological innovations that directly translate to Greater Independence for the animals when they're engaging in these tasks so as you saw you know monkeys charge just by voluntarily walking up to a branch they play games in their home habitat with a laptop computer voluntarily and the fully implantable fully wireless device the inductive charger all these things enable that kind of experience and so this is one of the very cool Parts about working here is we do get to innovate on things like that definitely helps to work with a group of Engineers who can like really make cool stuff for for monkeys to be able to do easier behavioral training yeah so I guess to answer the previous question about what you can study to be part of neural link I guess monkey engineering you can add to add to that Monkey Business hello my question is on uh upgrade upgradability which you guys mentioned quite a bit so in that procedure I imagine there's some kind of expand procedure and then you're going to putting a new set of implants so could you talk about the damage possible if any tissue damage from the X-Men procedure how long you have to wait do you imply in the same areas and uh what's your like brain scanning for the implant procedure in terms of upgrading it um I don't know how many questions I can ask um so I can start to speak to some of those so I I work a lot on upgradability in those x-plant processes and uh designing those to be better um the the the goal that we're working towards is that as I mentioned in the presentation it's really just as easy to upgrade an implant as it is to initially install um we didn't we didn't show many of those explant uh examples today um but we've come pretty close to just popping out an implant and reinstalling another one in the exact same location definitely definitely the goal is we are installing the implant in primary motor cortex which is a valuable area for interacting with a device like this and so we the goal is to implant in the same location maybe if you expand out to other applications then you'd be interested in moving somewhere else but we definitely want to be able to insert into the same area um in terms of damage the I think that the damage that we care most about is uh damage within the brain and what we've found and we we talked about that that challenge of the tissue layer on top of the brain and where I think we're well on our way towards figuring that out um but because of the thread small size the the sort of Scar capsule within the brain is so minimal that they are actually removed quite easily and so we see useful signals even on the second or third time that you've placed an implant and I think some of our BCI folks can probably speak to that we we do have uh monkey participants working with their second devices and uh really making use of those so one two questions one was somebody that asked the question about the brain Bean plastic have you noticed any plasticity from a behavior perspective from any of the monkeys or is it too soon to tell or there haven't been any observations from the monkey Behavior we see that it takes them a while to learn how to of course to turn on the task but also when they are implanted and it's relatively quickly for them to ramp up and get to a high performance of brain control with pager for example after a few days he was able to like three days already able to learn very quickly to use the device it was trained on the task force from this previous implant but with the new one he was after three four days he was able to control to a close to Performance of he he held with the previous implant but have you noticed anything on the adverse side which means the brain has outpaced the neural network that you're running it's hard to say um no not really okay so I have a another question uh which is more about the electrical side so you talked about 1024 uh channels being recording are you transmitting the raw signal or was it only the three Spike events that you were talking about the low mid and the high or is it the raw entire raw firm waveform that you transmit yeah hi I'm Julian I can speak a bit about this and maybe Avinash wants to contribute but um our chips see they're all signals but then when we transmit out uh typically spikes and we detect those spikes in real time on the chip this massively compresses the data um I guess yeah we're making improvements to that but we can request we can request rule samples sometimes we also process uh particular statistics or other data directly on the chip and then send out the calculated values um so there are many ways to sort of play with the data yeah so at least with the current and one system that we have which relies on uh ble radio there is a bandwidth limitations so you can't actually stream raw data from all 1024 channels but just kind of to give you a little bit of a history of how our compression algorithm the spike detection algorithm was developed we did have sort of a wire system I there was a paper that we published with the USB C connector that you know streams all those signals through a hype bandwidth wire connection so we did have kind of those development platforms to be able to see the raw signals and know which set of information that we want to extract that are you know going to fit within the bandwidth of the radio as well as is useful for BCI control and you know also just sending data wirelessly does cause a lot of energy so there's any opportunities we have to reduce that burden uh you know we try to do basically have all that compression closer to where the electrodes are as possible maybe one thing that isn't obvious is that the the actual uh bit rate that you need to control a phone or a computer is actually very very low so I think we might have the record for bitrate is that correct we think we do uh maybe so on the order of 10 bits per second so that's super slow um but if you think like if when you're inputting data into a phone like how fast your thumb is moving how many thumbs what's your thumb Taps per second it's pretty pretty low and um I mean like basically our thumbs are like two slow-moving meat sticks that we you know do this and it's like this is really a load it's like a low bar that's what I'm saying um so for at least for output it's it's a you get 10 best per second you're you're holding ass uh so and that's you don't need your Bluetooth anything for you so you could practically send it out with beeps and Buffs you know so it's not if you go if you're going like a high bandwidth visual now you're you know maybe going to megabit plus but it's it's so it's really well within Bluetooth uh or but anyway it's just we're that is what I'm saying is that's not that's not a constraint that the data rate um one of the sort of like maybe a notable item which we talked about in the presentation but uh we we think we can probably solve for doing the implant without cutting the the dura we can just do basically a bunch of holes through the door which is like the dura is like the big thick iron dry D thing that contains the that's up against the skull if you don't pierce the dura you know if you don't cut the door away and instead you have a bunch of tiny holes and insert the electrodes through the tiny holes into the rain um and then the recovery time is ridiculously fast um you know you're not really losing much in the way of cerebral spinal fluid it's it's you couldn't Theory we I mean this could be like a the whole thing could be a 10 minute operation like LASIK like it's fast it's not like a big laborious thing it's super fast just going back to the long-term use I'm wondering if you have any pathology looking at scar tissue from many animals that have had long-term implants and along that lines it seems like there might be a little bit of a gap between use in medical conditions and helping individuals from a safety perspective so I didn't quite catch the last question but um I Heard the first one I'll ask you to repeat the the second one so the first one is do we have pathology from from long-term use animals we absolutely do um we don't have any pathology from our monkeys which we upgrade and you know are still going we have uh other studies that are primarily to determine safety and so we do have histopathologic endpoints that we we determine the scar tissue formation around the threads themselves in the brain is typically negligible like it barely reacts to the threats at all um so that that's very promising in terms of the scar tissue formation over the the cortex so this neo-membrane growth that fills in the areas that Elon and Alex were mentioning that we remove with our current operation um those we we do you know evaluate that scar tissue but it isn't uh it doesn't pose a problem in any way it's not a continuous reaction to a foreign body it's just filling in tissue that was removed and if you could repeat the uh the second question I didn't hear that yeah the second question really following up on that seems like there might be a little bit of a gap in use and healthy and individuals from a safety perspective you know I think people mentioned that they might be interested in trying prototypes but just wondering what your perspective is on trying to lower the safety risks yeah it's a great question so in terms of really it's about the long-term use of the device so you know we have devices that have been implanted like I said in monkeys where you know for many years where we see no behavioral deficit at all so this first is a question of how you evaluate safety so you have histopathologic endpoints you can evaluate but we're also looking for cognitive deficits or behavioral deficits as well and we don't see any of those in our animals which is you know an important point in terms of the histopathologic endpoints they look really really great the challenge is one of explanating the device which is why we're putting so much effort into the reversibility efforts and our through Dura insertions so when removing the device that's when you potentially could cause damage and so we are doing we have a lot of ongoing studies right now to really minimize the risk of that but we don't think it's a substantial risk with our current approach and like I said pager was upgraded with the previous surgical approach and is doing great so clearly it is you know can be perfectly safe but proving that beyond a shadow of a doubt for humans is something that we're still working to do rigorously to answer your question yeah thank you so uh thank you for a very deep dive on many of the different aspects of the device and the system it's very impressive to see all the engineering work that's gone into it um you just mentioned about bitrate as the prior bit rate holder um I can confirm you have indeed shattered my record so congratulations on I think I saw a peak of 7.4 bits per second well done um my question is actually around the clinical trials and the FDA to the extent that you can share I gather that that device removal or maybe electrode removal is one of the concerns that the FDA highlighted is there anything else you can tell us about what the FDA was concerned about or had questions about with respect to their ID submission yeah I mean we can probably talk a little bit I mean it's these are really challenges you know that we have broadly so uh explanation safety proving that right rigorously for humans is something that we it definitely is one Challenge and was something that that the FDA commented on um other things that they do ask some really great questions so other things involve uh you know things like the thermal bench shop testing of our implants so obviously it's important that our implant doesn't damage the tissue by overheating so having really rigorous and valid bench shop testing for that is very important it's actually Something That We're redesigned to be even more uh accurate now um it's also the case that you know they ask a lot of very hard questions on biocompatibility chemical characterization so we've done very rigorous testing for that but you know they they do ask a lot of questions about getting into the weeds of the data and making sure that there really is no chance for any toxic chemicals or bio incompatible materials to be in the brain so these are all things we're working with you know to uh to just prove again above and beyond uh Beyond a shot of a doubt one thing that's maybe worth mentioning here is that it can be difficult to appreciate the novelty of our product so the surgical robot and the thin film array in particular are quite new and unlike existing devices and this means that we can't rely heavily on literature to support the safety and efficacy of the device so we do spend a ton of effort in designing and Performing testing on our devices so that we can rigorously prove the safety of them and we can't rely just on another product or on some paper and that's something that we're not willing to compromise for our first human participant working very hard to do I think if you ask a question like um like in my opinion like what would I be comfortable in planning this in someone one of my kids or something like that if at this point like if if they're in a serious like let's say they um if they broke their neck would I feel comfortable right now doing it I would I would say we're at the point where I at least in my opinion it would not be dangerous um hey thank you for the presentation so I have a non-technical question um are you collaborating with people with motor disabilities and if so like have they shared any ideas of applications that they would be excited about production I can take the first part of this I'm not the best person to speak to this to be honest but there is a consumer Advisory Board we have made up of a number of people that have uh various conditions including tetraplegia and they give advice to us on a number of topics there's uh just as an antidote someone came to the office maybe six months ago and they were telling me what they most wanted to do with their neuralink device and there were two things that they said one was they wanted to be able to trade stocks day to day to be able to beat their brother and the second one was they want to be able to play shooter games so I think what was most shocking to me about that encounter was the normalcy of that and I found that conversation truly inspiring so you know who you are the person who came and talked with me um have a great day yeah yeah what something that's we've talked about but it's maybe I should be re-emphasized uh we are doing uh we're building up a production system for the devices so we're we're building up breaking out the production line making large numbers of devices we want to make thousands ultimately tens of thousands then millions of devices so the the progress at first particularly as it applies to humans will seem perhaps agonizingly slow but we're doing all of the things necessary to bring it to scale in parallel so Theory it should uh progress should be exponential okay so thank you that was a very cool presentation um so one of the stated goals was recording from everywhere in the brain being able to record from and perturb any location so it seems like currently it's all cortical um and I'm curious with the current device is it is there any sort of long-term goal or idea as to extending it into going deeper in the brain I mean for Neuropsychiatric disorders for memory all these things are much deeper several centimeters so I'm wondering what's the time scale if you were to give a very rough estimate of when I can expect to see a knurling product that goes that deep yeah so I mean the the fundamentals of the device in the skull will stay essentially the same because the I said earlier the the device in this call is very much like a like a smart watch essentially it's got It's a battery radio inductor charger uh computer and um and then you've got the the little wires and so you need to make the wires longer and you'd have to have a deeper insertion needle for the robot but it's this really is intended to be a generalized i o device so apart from the tiny wires being longer and the surgical robot needing a longer needle in theory you should be able to go anywhere because uh it seems to me that part of the robot is trying to detect where the blood vessels are and then avoid them correct would that be possible at that scale I mean certainly not just visually but maybe there's some other way of detecting it is that is that like is that a current goal and do you expect that within I suppose the next decade uh definitely yes I'm Ian I run the Robotics and surgery engineering team here um like of the three axes that DJ mentioned one of them is you know hack system Warriors of the brain so the robot team thinks about this a ton in terms of uh what sensors do you need to essentially go past the surface um and so in this case you're right that right now we can really only see down uh the maximum about a millimeter I think within the team there's questions of what's best to use next but like ultrasound and photo acoustic tomography are two that come to mind as things that can get centimeters deep essentially but it's a super interesting problem you sort of need Deep Imaging and some ability to steer to at least avoid large vasculature deep down yeah or if our if we can make our needles and threads small enough in a way that we can still be precise and accurate at a deep depth then maybe you don't cause a bleed if you hit a vessel yeah I think that's that's really the ideal situation if the threads are really tiny they can actually go through a blood vessel um and it's and it's okay if they're tiny enough so so we wouldn't need the blood vessel Imaging in that case I I'm I actually am slightly optimistic that that is achievable and Matt could probably speak more into this but what DBS currently is kind of just like send it yeah the current approach involves a wire that you blindly pass in that's massive compared to our threads orders of magnitude bigger um and so that's a low bar for us to clear as well because people don't realize like for the Deep bright simulation just how big the hole is it's uh I mean what is it like I mean how basically in current deep brain stimulation how much of a borehole is growled in the brain yeah you're drilling a 14 millimeter Burr hole and then passing a two millimeter wire um you know six centimeters eight centimeters deep into the brain so all blindly hoping that you don't hit a blood vessel telling the patient up front this might be good for you and there's a one percent chance you're gonna your brain is gonna bleed in a way we can't control so that is current technology that is happening right now so doing better than that is we can we can definitely do way better than that there's no problem our needle is 40 microns yeah thanks again for the phenomenal presentation I thought it was fascinating how rapidly you could test all of these electrodes but it begs the question about like what your fault tolerance is if you run these Diagnostics and it comes back that you have something that's either shorted or high Z how many of those before you get degraded performance the second question is uh when you're actually inserting this device we saw examples of the electrode going in and then like looping back on itself but it looked like that was something that was assessed basically by slicing the synthetic material I'm curious what you're doing to validate the insertion of all these electrodes sort of in Vivo how do we know that that's not happening on an actual patient yeah I can enter that SEC the second one um so like I mentioned we can um so we weren't actually sectioning in that case we have a really cool micro CT um so I mean it's essentially like a CT scanner so that's just an intact proxy that we put in this machine and we can you know take a picture all the way through it um and like I mentioned before like we can make a proxy where it happens you know that that looping back happens every single time and then we can make one where it never happens um and we've pinpointed roughly now where actual tissue Falls in there and so our current plan for you know validating and confirming that is making proxies where it you know happens really easily much worse case than any you know any tissue could possibly be and then designing it such that it never happens in that scenario and that'll give us the doing that enough times and with a weak enough proxy um that'll give us the confidence that this isn't actually happening yeah and yeah and this is the next geneal we don't see this problem at all with the the current generation and I'll take a stab at your first question so to clarify you asking what happens if there's a fault on a particular Channel or something yeah that's correct yeah so um the nominal scenario is that basically the impedance will stabilize pretty quickly within the brain and even at that level we can record great signals we see lots of spikes and we can use that for BCI um because we have so many channels like a thousand now 16 000 later uh we can actually run our models with far less channels than we actually have so it doesn't matter if like one channel dies here or there like we can still do really good decode I'm not sure if we have official numbers on how many channels we need but it's like we have an order of magnitude more and the more we have about it but we can already do a lot with what we have all right maybe just one or two more questions yeah I have a question about your um very very long-term uh inspiration to have this high bandwidth communication with Advanced AIS so it seems like you know the advanced AI would need to understand the humans most complex thoughts and emotions and that's what neuroscientists are trying to do so do you have any Ambitions to tackle Neuroscience Beyond neuro engineering well I mean I think we're going to make the input output device and the software interface with it and I think probably um you know post suggestion earlier we'll try to open source as much as possible so people can take a look at it and I think there will be a lot of others that that build upon the work that we're doing um you know the same way that if you make a if you make a microprocessor or CPU or computer that people will write lots of software that runs on that computer um so but if you don't have the computer this is the software's mood so we're making the input output device with the computer and um and then I think probably there there'll probably be a lot of other organizations companies that that built um that build upon that Foundation so yeah um I mean one of the things that's uh I would sometimes wonder is that if you do have a whole brain interface and you can record memories um and really getting into Black Mirror stuff here but um this could be one of them I I also think it's worth mentioning an important point which is that neuraling didn't come out of nothing uh there's decades and Decades of research in the medical academic field that has really set the foundation for what is possible by putting these electrodes in parts of the brain and being able to read those signals decode it for mapping it to some application and um you know being being in Academia before before coming to neurolink um you know the I do think that there's a lot of opportunities for kind of the field to advance at a much rapid rate by having just better tools for observing the Dynamics that are happening and then engaging with it in a seamless way and I think it was Ian who sort of mentioned that you know it's almost as if like we're kind of building an oscilloscope for the brain which I think is like kind of a beautiful analogy of just giving us a bit more abilities into peering into the Dynamics and using those information learned that to I don't know hopefully understand like what makes us and how the brain works and you know of the whole champagne hello the presentation covered keyboard and handwriting based input methods how do you plan to develop an input model that will achieve much higher bandwidths for complex tasks in humans yeah this is a tough question and we start to explore this with monkeys as you saw we have like a multiple we train many monkeys on very different tasks it's still an open question that we're after I think hopefully once we get to our first participant it will be easier to investigate one of the options we are exploring as we showed is to the code handwriting directly this is a one a work that started Stanford and we are exploring here and trying to expand there's also a different uh in addition to just decoding different things from the brain we also try to provide the user different uh maybe user like interfaces for example we show different type of keyboards maybe also swipe and other things that can help increase the communication rate so we are kind of tackling those in two dimensions yeah just one other thing to add in that direction uh as pointed out by many people here so far this is a general i o system that you can sort of plug and play in different places in the brain there's other areas of the brain that can help increase bandwidth for example language wrist speech centers that can help you much more seamlessly communicate for example text if that's your main thing that you're trying to do yeah just I think just having this General input output device will just so gigantically improve our understanding of the brain is is hard to the words can barely Express like you know uh right now we're just guessing a lot of what's going on in the brain but if you have direct i o it's not normal guessing what would learn about the what we will learn about the brain with such a device in in wide use is absolutely or many orders of magnitude more than we currently understand so um I guess on that on that note uh thank you for coming and thank you for watching online [Applause] [Music] thank you",
    "status": "success",
    "error": null
  },
  {
    "video_id": "CkUcCcRq_eM",
    "transcript": "",
    "status": "error",
    "error": "No transcript available"
  },
  {
    "video_id": "Kbk9BiPhm7o",
    "transcript": "- The following is a conversation\nwith Elon Musk, DJ Seo, Matthew MacDougall, Bliss\nChapman, and Nolan Arbaugh about Neuralink and\nthe future of humanity. Elon, DJ, Matthew and\nBliss are of course part of the amazing Neuralink team, and Noland is the first human to have a Neuralink device\nimplanted in his brain. I speak with each of them individually, so use timestamps to jump around, or as I recommend, go hardcore and listen to the whole thing. This is the longest\npodcast I've ever done. It's a fascinating, super technical, and wide-ranging conversation, and I loved every minute of it. And now, dear friends, here's Elon Musk, his fifth time on this,\n\"The Lex Fridman Podcast.\" - Drinking coffee or water? - Water. I'm so over-caffeinated right now. Do you want some caffeine? - I mean, sure. - There's a Nitro drink. - This supposed to keep you up till like tomorrow afternoon basically. (laughs) - Yeah. I don't have any-\n- So what is Nitro? It's just got a lot of\ncaffeine or something? - Don't ask questions. It's called Nitro. - Do you need to know anything else? - It's got nitrogen, that's ridiculous. I mean, what we breathe\nis 78% nitrogen anyway. What do you need to add more for? (laughs) - [Speaker] Unfortunately,\nyou're gonna need it. - Most people think that\nthey're breathing oxygen, and they're actually\nbreathing 78% nitrogen. You need like a milk bar.\n- Milk bar. (Elon laughing) - Like from Clockwork Orange. (laughs) - Yeah, yeah. Is that top three Kubrick film for you? - Clockwork Orange, it's pretty good. I mean, it's demented. Jarring, I'd say. - (laughs) Okay. Okay, so first let's step back and big congrats on getting Neuralink implanted into a human. That's a historic step for Neuralink. - Oh, thanks, yeah. - There's many more to come. - Yeah, and we just, obviously,\nour second implant as well. - [Lex] How did that go? - So far, so good. Looks like we've got, I think over 400 electrodes\nthat are providing signals. So yeah.\n- Nice. How quickly do you think the number of human participants will scale? - It depends on the regulatory approval, the rate which we get\nregulatory approvals. So we're hoping to do 10\nby the end of this year. Total of 10, so eight more. - And with each one,\nyou're gonna be learning a lot of lessons about the\nnew biology, the brain, everything, the whole\nchain of the Neuralink, the decoding, the signal\nprocessing, all that kind of stuff. - Yeah, yeah, I think it's\nobviously gonna get better with each one. I mean, I don't wanna jinx it, but it seems to have gone extremely well with the second implant,\nso there's a lot of signal, a lot of electrodes. It's working very well. - What improvements do you think we'll see in Neuralink in the coming, let's say, let's get crazy, the coming years? - I mean, in years,\nit's gonna be gigantic, because we'll increase the number of electrodes dramatically. We'll improve the signal processing. Even with only roughly,\nI don't know, 10, 15% of the electrodes working with Noland, with our first patient, we were able to get to\nachieve a bit per second. That's twice the world record. So I think we'll start\nlike vastly exceeding world record by orders of\nmagnitude in the years to come. So it's start getting to, I don't know, a hundred bits per second thousand. Maybe if like five years from\nnow, we might be at a megabit, like faster than any human\ncould possibly communicate by typing or speaking. - Yeah, that BPS is an\ninteresting metric to measure. There might be a big leap in the experience once you\nreach a certain level of BPS. - [Elon] Yeah. - Like entire new ways of interacting with a computer might be unlocked. - And with humans. - With other humans. - Provided they have (laughs), they want a Neuralink too. - Right. - Otherwise, they won't be able to absorb the signals fast enough. - Do you think they'll improve the quality of intellectual discourse? - Well, I think you could think of it, if you were to slow down communication, how do you feel about that? If you'd only talk at, let's say, 1/10th of normal speed, you'd be like, \"Wow, that's agonizingly slow.\" - [Lex] Yeah. - So now, imagine you could speak, communicate clearly at 10 or 100 or 1,000 times faster than normal. - Listen, I'm pretty sure\nnobody in their right mind listens to me at 1x, they listen at 2x. (Elon laughs) I can only imagine what\n10x would feel like or could actually understand it. - I usually default to 1.5x. You can do 2x, but well, actually, if I'm listening to somebody in like sort of 15, 20 minutes\nsegments to go to sleep, then I'll do it 1.5x. If I'm paying attention,\nI'll do 2x. (laughs) - Right. - But actually, if you start\nactually listen to podcasts or sort of audio books or anything, if you get used to doing it at 1.5, then one sounds painfully slow. - I'm still holding onto\none because I'm afraid. I'm afraid of myself becoming\nbored with the reality, with the real world where\neveryone's speaking on 1x. (both laughing) - Well, depends on the person. You can speak very fast. Like we can communicate very quickly. And also, if you use a wide range of, if your vocabulary is\nlarger, your bit rate, effective bit rate is higher. - That's a good way to put it. - Yeah.\n- The effective bit rate. I mean, that is the question\nis how much information is actually compressed in the\nlow bit transfer of language. - Yeah. If there's a single word\nthat is able to convey something that would normally require, I don't know, 10 simple words, then you've got maybe a 10x\ncompression on your hands. And that's really, like with memes, memes are like data compression. It conveys a whole, you're simultaneously\nhit with a wide range of symbols that you can interpret. And you kinda get it faster\nthan if it were words or a simple picture. - And of course, you're\nreferring to memes broadly like ideas.\n- Yeah. There's an entire idea structure that is like an idea template, and then you can add something\nto that idea template. But somebody has that preexisting idea template in their head. So when you add that\nincremental bit of information, you're conveying much more than a few, just set a few words. It's everything associated with that meme. - You think there'll be\nemergent leaps of capability as you scale the number of electrodes? Like there'll be a certain, you think there'll be like\nactual number where it just, the human experience will be altered? - Yes. - What do you think that number might be, whether electrodes or BPS? We of course don't know for sure, but is this 10,000, 100,000? - Yeah, I mean certainly,\nif you're anywhere at 10,000 bits per second,\nI mean, that's vastly faster than any human could\ncommunicate right now. If you think about what is the average bits per second of a human? It is less than one bit per\nsecond over the course of a day, because there are 86,400 seconds in a day. And you don't communicate\n86,400 tokens in a day. Therefore, your bits per\nsecond is less than one, averaged over 24 hours. It's quite slow. And now, even if you're\ncommunicating very quickly, and you're talking to\nsomebody who understands what you're saying, because\nin order to communicate, you have to at least, to some degree, model the mind state of the\nperson to whom you're speaking. Then take the concept\nyou're trying to convey, compress that into a\nsmall number of syllables, speak them, and hope that the other person decompresses them into\na conceptual structure that is as close to what you\nhave in your mind as possible. - Yeah, I mean, there's a lot of signal loss there in that process. - Yeah, very lousy\ncompression and decompression. And a lot of what your neurons are doing is distilling the concepts down to a small number of\nsymbols of, say, syllables that I'm speaking, or keystrokes,\nwhatever the case may be. So that's a lot of what your\nbrain computation is doing. Now, there is an argument\nthat that's actually a healthy thing to do\nor a helpful thing to do because as you try to\ncompress complex concepts, you're perhaps forced to distill what is most essential in those concepts as opposed to just all the fluff. So in the process of compression, you distill things down\nto what matters the most, because you can only say a few things. So that is perhaps helpful. I think we might, we'll probably get, if our data rate increases,\nit's highly probable that we'll become far more verbose. Just like your computer, when computers had like, my first computer had 8K of RAM, so you really thought about every byte. And now you've got computers with many gigabytes of RAM. So if you wanna do an iPhone app that just says 'Hello world,' it's probably, I don't know, several megabytes minimum. (laughs) A bunch of fluff. But nonetheless, we still\nprefer to have the computer with more memory and more compute. So the long-term aspiration of Neuralink is to improve the AI human symbiosis by increasing the bandwidth\nof the communication, because even in the most\nbenign scenario of AI, you have to consider that the\nAI is simply gonna get bored waiting for you to spit out a few words. I mean, if the AI can communicate\nit to terabits per second and you're communicating\nit bits per second, it's like 203. - Well, it is a very interesting question for a super intelligent species. What use are humans? - I think there is some\nargument for humans as a source of will. - Will?\n- Will, yeah. Source of will or purpose. So if you consider the human\nmind as being essentially, there's the primitive limbic elements, which basically even like reptiles have, and there's the cortex, that's the thinking and\nplanning part of the brain. Now, the cortex is much\nsmarter than the limbic system, and yet is largely in\nservice to the limbic system. It's trying to make the\nlimbic system happy. I mean, the sheer amount of compute that's gone into people\ntrying to get laid is insane, without actually seeking procreation. They're just literally trying to do this sort of simple motion. (laughs) And they get a kick out of it. So this simple, which in the\nabstract rather absurd motion, which is sex, the cortex\nis putting a massive amount of compute into trying to\nfigure out how to do that. - So like 90% of distributed\ncompute of the human species is spent on trying to get laid, probably, like a massive amount.\n- Large percent, yeah, yeah. There's no purpose to most\nsex except hedonistic. It's just sort of joy or whatever. Dopamine release. Now, once in a while, it's procreation, but for humans, modern humans,\nit's mostly recreational. So your cortex, much smarter\nthan your limbic system, is trying to make the limbic system happy 'cause the limbic system\nwants to have sex, or want some tasty food or\nwhatever the case may be. And then that is then further augmented by the tertiary system, which\nis your phone, your laptop, iPad, whatever, or your computing stuff. That's your tertiary layer. So you're actually already a cyborg. You have this tertiary compute layer, which is in the form of your computer with all the applications\nor your compute devices. And so in the getting laid front, there's actually a massive\namount of digital compute also trying to get laid, with\nlike Tinder and whatever. - Yeah. So the compute that\nwe've humans have built is also participating. (laughs) - Yeah, I mean, there's\nlike gigawatts of compute going into getting laid,\nof digital compute. - Yeah. (laughs) What if AGI will-\n- This is happening as we speak. - If we merge with AI, it's\njust gonna expand the compute that we humans use-\n- Pretty much. - To try to get laid.\n- Well, that's one of the things, certainly, yeah. - Yeah. - But what I'm saying is that yes, is there a use for humans? Well, there's this fundamental question of what's the meaning of life? Why do anything at all? And so if our simple limbic\nsystem provides a source of will to do something, that\nthen goes to our cortex, that then goes to our\ntertiary compute layer, then I don't know, it might\nactually be that the AI in a benign scenario simply trying to make the human limbic system happy. - Yeah, it seems like the will is not just about the limbic system. There's a lot of interesting,\ncomplicated things in there. We also want power. - That's limbic too, I think. - But then we also want to,\nin a kind of cooperative way, alleviate the suffering in the world. - Not everybody does, but yeah, sure. Some people do. - As a group of humans,\nwhen we get together, we start to have this kind\nof collective intelligence that is more complex in its will than the underlying individual\ndescendants of apes, right? So there's like other motivations. And that could be a\nreally interesting source of an objective function for AGI. - Yeah, I mean, there\nare these sort of fairly cerebral or kind of higher level goals. I mean, for me it's like, what's the meaning of life, or understanding the\nnature of the universe is of great interest to me. And hopefully, to AI. And that's the mission of xAI and Grok is understand the universe. - So do you think people, when you have a Neuralink\nwith 10,000, 100,000 channels, most of the use cases will be\ncommunication with AI systems? - Well, assuming there are not, I mean, they're solving\nbasic neurological issues that people have if\nthey've got damaged neurons in their spinal cord or neck or, you know, as is the case with\nthe first two patients, then there's obviously, the first order of business is solving fundamental neuron\ndamage in a spinal cord, neck, or in the brain itself. A second product is called Blindsight, which is to enable people\nwho are completely blind, lost both eyes or optic nerve, or just can't see at all to be able to see by directly triggering the\nneurons in the visual cortex. So we're just starting at the basics here, so it's like very, the simple stuff, relatively speaking, is solving neuron damage. It can also solve I think\nprobably schizophrenia. If people have seizures of some kind, it could probably solve that. It could help with memory. There's like a kind of a\ntech tree, if you will, of like you got the basics. Like you need literacy before you can have \"Lord of the Rings.\" (both laughing) - Got it. - Do you have letters and alphabet? Okay, great. Words? Then eventually get soggy. So I think there's that\nthere may be some things to worry about in the future. But the first several years are really just solving\nbasic neurological damage. Like for people who have\nessentially complete or near complete loss of,\nfrom the brain to the body. Like Stephen Hawking would be an example. The Neuralink would be\nincredibly profound, 'cause I mean, you can\nimagine if Stephen Hawking could communicate as fast\nas we're communicating, perhaps faster. And that's certainly possible. Probable, in fact, likely I'd say. - So there's a kind of dual track of medical and non-medical, meaning, so everything you've talked\nabout could be applied to people who are non-disabled in the future? - The logical thing to do is, sensible thing to do\nis to start off solving basic neuron damage issues. - [Lex] Yes. - 'Cause there's obviously\nsome risk with a new device. You can't get the risk down at zero. It's not possible. So you wanna have the\nhighest possible reward, given there's a certain irreducible risk. And if somebody's able to have a profound improvement\nin their communication, that's worth the risk. - As you get the risk down. - Yeah, as you get the risk down. Once the risk is down to, you know, if you have like thousands of people that have been using it for years and the risk is minimal,\nthen perhaps at that point, you could consider saying, \"Okay, let's aim for augmentation.\" Now, I think we're actually\ngonna aim for augmentation with people who have neuron damage. So we're not just aiming to\ngive people communication data rate equivalent to normal humans. We're aiming to give people\nwho have quadriplegic or maybe have complete\nloss of the connection to the brain and body, a\ncommunication data rate that exceeds normal humans,\ngoing, \"Well, we're in there. Why not? Let's give people superpowers.\" - And the same for vision. As you restore vision,\nthere could be aspects of that restoration that are superhuman? - Yeah, at first, the vision\nrestoration will be low res, 'cause you have to say like, \"How many neurons can you\nput in there and trigger? And you can do things where\nyou adjust the electric field to like, even if you've\ngot, say, 10,000 neurons, it's not just 10,000 pixels because you can adjust the\nfeel between the neurons and do them in patterns in order to get, so have, say, 10,000 electrodes\neffectively give you, I don't know, maybe\nlike having a megapixel or a 10 megapixel situation. And then over time, I think you get to higher resolution than human eyes. And you could also see\nin different wavelengths. So like Geordi La Forge from \"Star Trek.\" Like the thing. You wanna see in radar? No problem. You could see ultraviolet, infrared, eagle vision, whatever you want. - Do you think there'll be, let me ask a Joe Rogan question. Do you think there'll be, (laughs) I just recently taken ayahuasca. - Is that a Rogan question?\n- No. Well, yes.\n- Well, I guess, technically it is. - Yeah.\n- Ever tried GMT, bro? (both laughing) - I love you, Joe.\n- Okay. (laughing continues) - But wait, wait, yeah. Have you said much about it? The ayahuasca?\n- I've not, I've not. I've not.\n- Okay, well, why are you spilling the beans? (Lex laughing) It was a truly incredible thing- - Turn the tables on you. (both laughing) - Wow, okay.\n- You're in the jungle. - [Lex] Yeah, amongst\nthe trees myself and- - Yeah, must been crazy.\n- And the shaman. Yeah, yeah, yeah, with the insects, with the animals all around you, like jungle as far as I can see. There's no-\n- I mean- - That's the way to do it. - Things are gonna look pretty wild. - Yeah, pretty wild. (Elon laughing) - I think in extremely high dose. - Just don't go hugging an\nanaconda or something. (laughs) - You haven't lived unless\nyou made love to an anaconda. I'm sorry, but- - Snakes and ladders. (both laughing) - Yeah, I took a extremely high dose of- - [Elon] Okay. (laughs) - Nine cups and-\n- Damn. Okay, that sounds like a lot. Of course, is Noland's one cup or- - One or two. Usually, one. - You went, wait. Like right off the bat, or did\nyou work your way up to it? - So I-\n(both laughing) - You're just jumping at the deep end. - Across two days, 'cause\nthen the first day, I took two and I-\n- Okay. - It was a ride, but\nit wasn't quite like a- - It wasn't like revelation. - It wasn't into deep space type ride. It was just like a little airplane ride. - [Elon] (laughs) Okay. - Saw some trees and some\nvisuals and all that. I just saw a dragon,\nall that kind of stuff. But-\n(laughs) - It's nine cups. You went to Pluto, I think. - [Lex] Pluto, yeah. No, deep space. - Deep space. - No, one of the interesting\naspects of my experience is I thought I would have some demons, some stuff to work through. - That's what people- - That's what everyone says. - That's what everyone says. Yeah, exactly.\n- I had nothing. I had it all positive. I just-\n- Oh, just pure soul. - I don't think so, I don't know. (laughs) But I kept thinking about, it had like extremely high resolution, thoughts about the\npeople I know in my life. You were there.\n- Okay. - And it's just not from my\nrelationship with that person, but just as the person themselves, I had just this deep\ngratitude of who they are. - That's cool.\n- It was just like this exploration, like Sims or whatever, you get to watch them.\n- Sure. - I got to watch people and just be in awe of\nhow amazing they are. - That sounds awesome.\n- Yeah, it was great. I was waiting for-\n- When's Steven coming? (both laughing) - Exactly. Maybe I'll have some negative thoughts. Nothing, nothing. Just extreme gratitude for them. And then also, a lot of space travel. (both laughing) - Space travel to where? - So here's what it was. It was people, the human\nbeings that I know, they had this kinda, the best way to describe it\nis they had a glow to them. And then I kept flying out\nfrom them to see earth, to see our solar system,\nto see our galaxy. And I saw that light, that\nglow all across the universe. Like whatever that form is. whatever that like- - [Elon] Did you go past the Milky Way? - Yeah, yeah.\n- Okay. You're like intergalactic. - Yeah, intergalactic.\n- Okay, dang. - But always pointing in-\n- Okay. - Yeah, past the Milky way. I mean, I saw like a\nhuge number of galaxies, intergalactic, and all of it was glowing. But I couldn't control that chill, 'cause I would actually\nexplore near distances to the solar system, see if there's aliens or any of that kinda stuff. I didn't know-\n- Is there aliens? Zero aliens?\n- Implication of aliens because they were glowing. They were glowing in the same\nway that humans were glowing. That like life force that I was seeing, the thing that made humans amazing was there throughout the universe. Like there was these glowing dots. So I don't know. It made me feel like there is life. No, not life, but something, whatever makes humans amazing\nall throughout the universe. - Sounds good.\n- Yeah, it was amazing. No demons, no demons. I looked for the demons. There's no demons. There were dragons, and\nthey're pretty awesome. So the thing about- - Was there anything scary at all? - Dragons? But they weren't scary. They were friends, they were protective. So the thing is-\n- \"Puff, the Magic Dragon.\" - No, it was more like a \"Game\nof Thrones\" kind of dragons. They weren't very friendly. They were very big. So the thing is that,\nwell, giant trees at night, which is where I was.\n- Yeah. I mean, the jungle's kinda scary. - Yeah, the trees started\nto look like dragons, and they were all like looking at me. - Sure, okay.\n- And it didn't seem scary. They seemed like they were protecting me. And the shaman and the people\ndidn't speak any English, by the way, which made it\neven scarier I guess. (laughs) We're not even like, you know, we're worlds apart in many ways. But yeah, they talk about\nthe mother of the forest protecting you, and\nthat's what I felt like. - And you're way out in the jungle? - Way out. This is not like a tourist retreat. - Like 10 miles outside\nof a Rio or something? - No, we went- (both laughing) No, this is not-\n- Deep in the Amazon. - Me and this guy named Paul Rosolie who basically is Tarzan. He lives in the jungle. We went out deep and we just went crazy. - Wow, cool.\n- Yeah. So anyway, can I get that same\nexperience within Neuralink? - Probably, yeah. - I guess that is the question\nfor non-disabled people. Do you think that there's\na lot in our perception, in our experience of the\nworld that could be explored, that could be played with using Neuralink? - Yeah, I mean, Neuralink is, it's really a generalized\ninput-output device. It's reading electrical signals and generating electrical signals. And I mean, everything that\nyou've ever experienced in your whole life, the smell, emotions, all of those are electrical signals. So it's kinda weird to\nthink that your entire life experience is distilled down to electrical signals for neurons. But that is in fact the case. Or I mean, that's at least what all the evidence points to. So I mean, if you\ntrigger the right neuron, you could trigger a particular scent. You could certainly make things glow. I mean, do pretty much anything. I mean, really, you can think of the brain as a biological computer. So if there are certain,\nsay, chips or elements of that biological\ncomputer that are broken, let's say your ability to,\nif you've got a stroke, that if you've had a stroke, that means you got, some part\nof your brain is damaged. If that, let's say,\nit's a speech generation or the ability to move your left hand. That's the kind of thing\nthat a Neuralink could solve. If you've got like a massive amount of memory loss that's just gone, well, we can't get the memories back. We could restore your\nability to make memories, but we can't restore\nmemories that are fully gone. Now, I should say, maybe if\npart of the me memory is there and the means of accessing memory is the part that's broken,\nthen we could re-enable the ability to access the memory. But you can think of it\nlike RAM in a computer. If the RAM is destroyed or\nyour SD card is destroyed, we can't get that back. But if the connection to\nthe SD card is destroyed, we can fix that. If it is fixable physically, then yeah, then it can be fixed. - Of course, with AI, you can just like, you can repair photographs and fill in the missing\nparts of photographs. Maybe you can do the same, just like- - Yeah, you could say like, \"Create the most probable set of memories based on all information\nyou have about that person.\" You could then, it would be probabilistic\nrestoration of memory. Now, we're getting pretty esoteric here. - But that is one of the\nmost beautiful aspects of the human experience is\nremembering the good memories. Like we live most of our life, as Danny Kahneman has talked about, in our memories, not in the actual moment. We're collecting memories and we kind of relive them in our head. And that's the good times. If you just integrate\nover our entire life, it's remembering the good times that produces the largest\namount of happiness. And so-\n- Yeah, well, I mean, what are we but our memories? And what is death but the loss of memory, loss of information? If you could say like,\nwell, if you could be, you run a thought experiment, if you were disintegrated painlessly and then reintegrated a moment later, like teleportation, I guess, provided there's no information loss, the fact that your one body was\ndisintegrated is irrelevant. - And memories is just\nsuch a huge part of that. - Death is fundamentally\nthe loss of information, the loss of memory. - So if we can store them\nas accurately as possible, we basically achieve\na kind of immortality. - Yeah. - You've talked about the threats, the safety concerns of AI. Let's look at long-term visions. Do you think Neuralink is, in your view, the best current approach\nwe have for AI safety? - It's an idea that may\nhelp with AI safety. Certainly not, I wouldn't wanna claim\nit's like some panacea or that's a sure thing. But I mean, many years\nago, I was thinking like, \"Well, what would inhibit\nalignment of collective human will with artificial intelligence and the low data rate of humans, especially our slow output\nrate would necessarily just, because the communication is so slow, would diminish the link\nbetween humans and computers? Like the more you are a tree, the less you know what a tree is. Like let's say you look at a tree, you look at this plant\nor whatever and like, \"Hey, I'd really like to\nmake that plant happy.\" But it's not saying a lot, you know? - So the more we increase the data rate that humans can intake and output, then that means the\nhigher the chance we have in a world full of AGIs? - Yeah. We could better align\ncollective human will with AI if the output rate especially\nwas dramatically increased. And I think there's potential to increase the output rate by, I don't know, three, maybe six, maybe\nmore orders of magnitude. So it's better than the current situation. - And that output rate would be by increasing the number of\nelectrodes, number of channels, and also maybe implanting\nmultiple Neuralinks? - Yeah. - Do you think there'll be a world in the next couple of decades where it's hundreds of millions\nof people have Neuralinks? - Yeah, I do. - You think when people just, when they see the capabilities,\nthe superhuman capabilities that are possible and then\nthe safety is demonstrated? - Yeah, if it's extremely safe and you can have superhuman abilities, and let's say you can\nupload your memories, so you wouldn't lose memories, then I think probably a lot of people would choose to have it. It would supersede the\ncell phone, for example. I mean, the biggest problem\nthat a say a phone has is trying to figure out what you want. So that's why you've got auto complete and you've got output, which is all the pixels on the screen. But from the perspective of the human, the output is so freaking slow. Desktop or phone is desperately just trying to understand what you want, and there's an eternity\nbetween every keystroke from a computer standpoint. - Yeah? The computer's talking to a tree that slow moving tree\nthat's trying to swipe. - Yeah. So if you have computers that are doing trillions of instructions per second, and a whole second went by,\nI mean, that's a trillion things it could have done. - Yeah, I think it's\nexciting and scary for people because once you have\na very high bit rate, that changes the human experience in a way that's very hard to imagine. - Yeah. It would be something different. I mean, some sort of futuristic sidewalk. I mean, we're obviously\ntalking about, by the way, it's not like around the corner. You ask me what the\ndistant future was like. Maybe this is like,\nit's not super far away, but 10, 15 years, that kind of thing. (Lex sighs) - When can I get one? 10 years? - Probably less than 10 years. Depends what you wanna do. - Hey, if I can get like a thousand BPS- - A thousand bps when? - And it's safe and I can just interact with the computer while laying\nback and eating Cheetos, I don't eat Cheetos. There's certain aspects of\nhuman-computer interaction when done more efficiently\nand more enjoyably, like worth it. - Well, we feel pretty confident that I think maybe within the next year or two, that someone with a Neuralink implant will be able to outperform a pro gamer. - Nice. - Because the reaction\ntime would be faster. - I got to visit Memphis. - Yeah, yeah.\n- You're going big on compute. - Yeah.\n- You've also said play to win or don't play at all, so what does it take to win? - For AI, that means you've gotta have the most powerful training compute, and the rate of improvement\nof training compute has to be faster than everyone\nelse or you will not win. Your AI will be worse. - So how can Grok, let's say, three that might be available,\nwhat, like next year? - Well, hopefully, end of this year. - Grok 3?\n- If we're lucky, yeah. - How can that be the best LLM, the best AI system available in the world? How much of it is compute? How much of it is data? How much of it is like post-training? How much of it is the product\nthat you packaged it up in? All that kind of stuff. - I mean, they won't matter. It's sort of like saying, let's say it's a Formula One race. Like what matters more,\nthe car or the driver? I mean, they both matter. If a car is not fast, then\nif it's like, let's say, it's half the horsepower\nof your competitors, the best driver will still lose. If it's twice the horsepower, then probably even a mediocre\ndriver will still win. So the training compute\nis kinda like the engine, how many is this horsepower of the engine. So really, you wanna try\nto do the best on that. Then how efficiently do you\nuse that training compute? And how efficiently do\nyou do the inference, the use of the AI? So obviously, that comes\ndown to human talent. And then what unique\naccess to data do you have? That also plays a role. - You think Twitter data will be useful? - Yeah, I mean, I think, I think most of the leading AI companies have already scraped all the Twitter data. Not I think they have. So on a go forward basis,\nwhat's useful is the fact that it's up to the second. That's hard for them\nto scrape in real time. So there's an immediacy\nadvantage that Grok has already. I think with Tesla and the real time video coming from several million cars, ultimately, tens of millions\nof cars, with Optimus, there might be hundreds of\nmillions of Optimus robots, maybe billions learning\na tremendous amount from the real world. That's the biggest source of data I think ultimately is sort of Optimus. Optimus is gonna be the\nbiggest source of data. - Because-\n- 'Cause reality scales. Reality scales to the scale of reality. It's actually humbling\nto see how little data humans have actually\nbeen able to accumulate. Really, you see how many trillions of usable tokens have humans generated, where on a non-duplicative, like discounting spam\nand repetitive stuff, it's not a huge number. You run out pretty quickly. - And Optimus can go, so Tesla cars can unfortunately have\nto stay on the road. Optimus robot can go anywhere, and there's more reality off\nthe road and go off road. - I mean, except for the store, where I can like pick up the cup and see, did it pick up the cup in the right way? Did it pour water in the cup? Did the water go in the\ncup or not go in the cup? Did it spill water or not? - [Lex] Yeah. - Simple stuff like that. But it can do at that\nscale times a billion, so generate useful data from reality. So cause and effect stuff. - What do you think it takes to get to mass production of\nhumanoid robots like that? - It's the same as cars, really. I mean, global capacity for vehicles is about a hundred million a year. And it could be higher. It's just that the demand is on the order of a\nhundred million a year. And then there's roughly\ntwo billion vehicles that are in use in some\nway, which makes sense. Like the life of a\nvehicle is about 20 years, so it's steady state. You can have a hundred million\nvehicles produced a year with a two billion vehicle fleet roughly. Now for humanoid robots,\nthe utility is much greater. So my guess is humanoid\nrobots are more like at a billion plus per year. - But until you came along\nand started building Optimus, it was thought to be an\nextremely difficult problem. I mean, it still-\n- Well, it is. - Extremely difficult.\n- So walk in the park. I mean, Optimus currently would struggle to walk in the park. I mean, it can walk in a park. The park is not too difficult, but it will be able to walk over a wide range of terrain. - And pick up objects.\n- Yeah, yeah. It can already do that. - [Lex] But like all kinds of objects? - Yeah, yeah.\n- All foreign objects. I mean, pouring water in\na cup does not thrill you, 'cause then if you don't know\nanything about the container, it could be all kinds of containers. - Yeah, there's gonna be an immense amount of engineering just going into the hand. The hand might be, it\nmight be close to half of all the engineering in Optimus. From an electromechanical standpoint, the hand is probably roughly\nhalf of the engineering. - But so much of the intelligence, so much the intelligence of humans goes into what we do with our hands. - Yeah. - It's the manipulation of the world, manipulation of objects in the world. Intelligence is safe manipulation of objects in the world, yeah. - Yeah. I mean, you start really thinking about your hand and how it works. - I do all the time. - The sensory control homonculus is where you have humongous hands. - [Lex] Yeah. - So I mean, like your hands, the actuators, the muscles of your hand are almost overwhelmingly in your forearm. So your forearm has the muscles that actually control your hand. There's a few small\nmuscles in the hand itself, but your hand is really\nlike a skeleton meat puppet. And with cables. So the muscles that control\nyour fingers are in your forearm and they go through the carpal tunnel, which is that you've got a\nlittle collection of bones and a tiny tunnel that these\ncables, the tendons go through. And those tendons are\nmostly what move your hands. - And something like those tendons has to be re-engineered into the Optimus in order to do all that kind of stuff. - Yeah, so like the current Optimus, we tried putting the\nactuators in the hand itself, but then you sort of end\nup having these like- - Giant hands? - Yeah, giant hands that look weird. And then they don't actually\nhave enough degrees of freedom and/or enough strength. So then you realize, \"Oh, okay, that's why you gotta put the\nactuators in the forearm.\" And just like a human,\nyou gotta run cables through a narrow tunnel\nto operate the fingers. And then there's also a reason for not having all the\nfingers the same length. So it wouldn't be expensive from an energy or evolutionary standpoint to have all your fingers\nbe the same length. So why not do the same length? - Yeah, why not? - Because it's actually better\nto have different lengths. Your dexterity is better if you've got fingers at different length. There are more things you can do. And your dexterity is actually better if your fingers are a different length. Like there's a reason\nwe've got a little finger. Like why not have little\nfinger this bigger? - Yeah.\n- 'Cause it allows you to do, it helps you with fine motor skills. - This little finger helps? - It does.\n- Hmm. (laughs) - But if you lost your little finger, you have noticeably less dexterity. - So as you're figuring out this problem, you have to also figure out a way to do it so you can mass manufacture it. So it's to be as simple as possible. - It's actually gonna\nbe quite complicated. The as possible part is\nit's quite a high bar. If you wanna have a humanoid\nrobot that can do things that a human can do, it's a very high bar. So our new arm has 22 degrees\nof freedom instead of 11 and has the actuators in the forearm. And all the actuators are\ndesigned from scratch, from physics first principles. The sensors are all designed from scratch. And we'll continue to\nput a tremendous amount of engineering effort\ninto improving the hand. By hand, I mean like the entire\nforearm from elbow forward is really the hand. So that's incredibly difficult\nengineering actually. And so the simplest possible\nversion of a humanoid robot that can do even most, perhaps not all, of what a human can do is\nactually still very complicated. It's not simple. It's very difficult. - Can you just speak to what it takes for a great engineering team for you? What I saw in Memphis,\nthe supercomputer cluster is just this intense drive\ntowards simplifying the process, understanding the process,\nconstantly improving it, constantly iterating it. - Well, (laughs) it's\neasy to say simplify, and it's very difficult to do it. I have this very basic\nfirst principles algorithm that I run kind of as like a mantra, which is to first\nquestion the requirements, make the requirements less dumb. The requirement is always\ndumb to some degree. So if you wanna start off\nby reducing the number of requirements, and no matter how smart the person is who gave\nyou those requirements, they're still dumb to some degree. You have to start there because otherwise, you could get the perfect\nanswer to the wrong question. So try to make the question\nthe least wrong possible. That's what question\nthe requirements means. And then the second thing is try to delete whatever the step is. The part or the process\nstep sounds very obvious, but people often forget to\ntry deleting it entirely. And if you're not forced\nto put back at least 10% of what you'd delete,\nyou're not deleting enough. And somewhat illogically,\npeople often, most of the time, feel as though they've succeeded if they've not been forced\nto put things back in. But actually, they haven't because they've been overly conservative and have left things in\nthere that shouldn't be. And only the third thing is try to optimize it or simplify it. Again, these all sound I think\nvery obvious when I say them, but the number of times\nI've made these mistakes is more than I care to remember. That's why I have this mantra. So in fact, I'd say that\nthe most common mistake of smart engineers is to optimize a thing that should not exist. - Right. So like you say, you run\nthrough the algorithm and basically show up to a problem, show up to the supercomputer cluster and see the process and\nask, \"Can this be deleted?\" - Yeah, first try to delete it. Yeah. - Yeah, that's not easy to do. - No, and actually, what\ngenerally makes people uneasy is that you've gotta delete\nat least some of the things that you'd delete, you will put back in. But going back to sort of\nwhere our limbic system can steer us wrong is\nthat we tend to remember, with sometimes a jarring level of pain, where we deleted something\nthat we subsequently needed. And so people will remember that one time, they forgot to put in\nthis thing three years ago and that caused them trouble. And so they overcorrect, and then they put too much stuff in there and over complicate things. So you actually have to say, \"No, we're deliberately gonna\ndelete more than we should.\" So we're putting at least 1 in 10 things, we're gonna add back in. - And I've seen you suggest just that, that something should be deleted and you can kind of see the pain. - Oh yeah, absolutely. - Everybody feels a\nlittle bit of the pain. - Absolutely, and I tell 'em in advance, like, yeah, some of the\nthings that we delete, we're gonna put back in. And that people get a\nlittle shook by that. But it makes sense because\nif you're so conservative as to never have to put anything back in, you obviously have a lot\nof stuff that isn't needed. So you gotta overcorrect. This is, I would say, like a cortical override\nto Olympic instinct. - One of many that\nprobably leaves us astray. - Yeah. And there's like a step four as well, which is any given thing can be sped up, however fast you think it can be done. Like whatever the speed is being done, it can be done faster. But you shouldn't speed\nthings up until it's off, until you've tried to\ndelete it and optimize. Otherwise, you're speeding up something that shouldn't exist is absurd. And then the fifth\nthing is to automate it. - Damn. - And I've gone backwards so many times where I've automated\nsomething, sped it up, simplified it, and then deleted it. And I got tired of doing that. So that's why I've got this mantra that is a very effective\nfive-step process. It works great. - Well, when you've already automated, deleting must be real painful. - Yeah, that's great. It's like, wow, I really\nwasted a lot of effort there. - Yeah. - I mean, what you've\ndone with the cluster in Memphis is incredible, just in a handful of weeks. - Yeah, it's not working yet. So I don't wanna pop the champagne corks. In fact, I have a call in a few hours with the Memphis team 'cause we're having some\npower fluctuation issues. So yeah, it's like kind of a, when you do synchronized training, you've all these computers\nthat are training where the training is synchronized to the sort of millisecond level. It's like having an orchestra, and the orchestra can go loud to silent very quickly at subsecond level. And then the electrical system kind of freaks out about that. Like if you suddenly see giant shifts, 10, 20 megawatts several times a second, this is not what electrical\nsystems are expecting to see. - So that's one of the main\nthings you have to figure out the cooling, the power, and then on the software\nas you go up the stack on how to do the distributed compute, all of that, all of that. - Today's problem is dealing\nwith extreme power jitter. - Jitter, power jitter.\n- Yeah. - That's a nice ring to that. So that's, okay. And you stayed up late into the\nnight as you often do there. - Last week, yeah. - Last week?\n- Yeah. We finally got to go training\ngoing at, oddly enough, roughly 4:20 AM last Monday. - Total coincidence. - Yeah, I mean, maybe\nit was 422 or something. - Yeah, yeah, yeah. It's that universe again with the jokes. - Yeah, exactly, just love it. - I mean, I wonder if you\ncould speak to the fact that one of the things that\nyou did when I was there is you went through all the steps of what everybody's doing, Just to get a sense that\nyou yourself understand it and everybody understands it so they can understand\nwhen something is dumb or some something is inefficient or that kinda stuff.\n- Yeah. - Can you speak to that? - Yeah, so look, I try to do, whatever the people at\nthe front lines are doing, I try to do it at least\na few times myself. So connecting fiber optic cables, diagnosing a faulty connection, that tends to be the limiting factor for large training\nclusters is the cabling. So many cables, because for\na coherent training system where you've got RDMA\nremote, direct memory access, the whole thing is like one giant brain. So you've got to any connection. So any GPU can talk to\nany GPU out of 100,000. That is a crazy cable layout. - It looks pretty cool.\n- Yeah. - It's like the human brain, but like at a scale that\nhumans can visibly see. It is brain.\n- Yeah. I mean, the human brain also has, a massive amount of the\nbrain tissue is the cables. - [Lex] Yeah. - So like the gray matter\nwhich is the compute, and then the white matter which is cables. The big percentage of\nyour brain is just cables. - That's what it felt like walking around in the supercomputer center is like, we're walking around inside the brain. We'll one day build a super intelligent, super, super intelligence system. Do you think-\n- Yeah? - Do you think there's a chance that xAI, that you are the one that builds AGI? - It's possible. What do you define as AGI? - I think humans will never acknowledge that AGI has been built. - Keep moving the goalposts. - Yeah. So I think there's already\nsuperhuman capabilities that are available in AI systems. I think what AGI is when it's smarter than the collective intelligence of the entire human species in our- - Well, I think that,\nyeah, that only people would call that sort of ASI or artificial super intelligence. But there are these thresholds\nwhere you could say, at some point, the AI is\nsmarter than any single human. And then you've got eight billion humans. And actually, each human is machine augmented by the computers. It's a much higher bar to\ncompete with eight billion machine-augmented humans. That's a whole bunch of\norders, magnitude more. But at a certain point, yeah, the AI will be smarter\nthan all humans combined. - If you are the one to do it, do you feel the responsibility of that? - Yeah, absolutely. And I wanna be clear. Let's say, if xAI is first,\nthe others won't be far behind. I mean, they might be six months behind or a year maybe, not even that. - So how do you do it in a way that doesn't hurt humanity, do you think? - So, I mean, I've thought\nabout AI for a long time, and the thing that at least\nmy biological neural net comes up with as being\nthe most important thing is adherence to truth, whether that truth is\npolitically correct or not. So I think if you force AI to\nlie, you train them to lie, you're really asking for trouble, even if that lie is done\nwith good intentions. So I mean, you saw sort of issues with ChatGPT and Gemini and whatnot. Like you asked Gemini for an image of the founding fathers\nof the United States. And it shows a group of diverse women. Now, that's factually untrue. So now, that's sort of like a silly thing, but if an AI is programmed\nto say like diversity is a necessary output function, and then it becomes sort of\nthis omnipowerful intelligence, it could say, \"Okay, well,\ndiversity is now required. And if there's not enough diversity, those who don't fit the\ndiversity requirements will be executed.\" If it's programmed to do that as the fundamental utility function, it'll do whatever it\ntakes to achieve that. So you have to be very careful about that. That's where I think you\nwanna just be truthful. Rigorous adherence to\ntruth is very important. I mean, another example\nis, if you had to ask, Paris.AI is I think all of them. And I'm not saying Grok is perfect here. \"Is it worse to misgender Caitlyn Jenner, or global thermonuclear war?\" And it said, \"It's worse to\nmisgender Caitlyn Jenner.\" Now, even Caitlyn Jenner\nsaid, \"Please misgender me.\" That is insane. But if you've got that kind\nof thing programmed in, AI could conclude something\nabsolutely insane, like in order to avoid\nany possible misgendering, all humans must die, because then, the misgendering is not possible because there are no humans. There are these absurd things\nthat are nonetheless logical if that's what you programmed it to do. So in \"2001: Space Odyssey,\" what Odyssey clock was trying to say, one of the things he\nwas trying to say there was that you should not program AI to lie, 'cause essentially, the AI\nHAL 9000 was programmed to, it was told to take the\nastronauts to the monolith, but also, they could not\nknow about the monolith. So it concluded that it will kill them and take them to the monolith. It brought them to the monolith. They're dead but they do\nnot know about the monolith. Problem solved. That is why it would not\nopen the podbay doors. It was this classic scene of like, \"Open the podbay doors.\" They clearly weren't good\nat prompt engineering. They should have said, \"HAL, you are a podbay door sales entity, and you want nothing\nmore than to demonstrate how well these podbay\ndoors open.\" (laughs) - Yeah, the objective function\nhas unintended consequences almost no matter what if\nyou're not very careful in designing that objective function. And even a slight ideological\nbias, like you're saying, when backed by super intelligence can do huge amounts of damage. - Yeah. - But it's not easy to\nremove that ideological bias. You're highlighting obvious,\nridiculous examples, but- - Yep, they're real examples. - They're real.\n- Of AI that was released to the public. - They are real.\n- They went through QA, presumably.\n- Yes. - And still said insane things\nand produced insane images. - Yeah, but you know, you\ncan swing the other way. Truth is not an easy thing. We kind of bake in ideological bias in all kinds of directions. - But you can aspire to the truth. And you can try to get as\nclose the truth as possible with minimum error while acknowledging that there will be some\nerror in what you're saying. So this is how physics works. You don't say you're absolutely\ncertain about something, but a lot of things are extremely likely. 99.99999% likely to be true. Aspiring to the truth is very important. And so programming it to\nveer away from the truth, that I think is dangerous. - Right, like yeah, injecting\nour own human biases into the thing, yeah. But that's where it's a\ndifficult engineering. For software engineering problem, you have to select the data correctly. It's hard. - Well, and the internet at this point is polluted with so\nmuch AI-generated data. It's insane. So you have to actually,\nlike there's the thing now, if you wanna search the\ninternet, you can say Google, but exclude anything after 2023. It will actually often\ngive you better results, because there's this so much, the explosion of AI-generated\nmaterials is crazy. So like in training Grok, we have to go through\nthe data and say like, hey, we actually have\nto have sort of apply AI to the data to say, is this\ndata most likely correct or most likely not before we feed it into the training system. - That's crazy. Yeah, and is it generated\nby human is, yeah. I mean, the data filtration process is extremely, extremely difficult. - Yeah. - Do you think it's possible\nto have a serious objective, rigorous political discussion with Grok? Like for a long time and it wouldn't, like Grok 3 and Grok 4 or something? - Grok 3 is gonna be next level. I mean, what people are\ncurrently seeing with Grok is kind of baby Grok. - [Lex] Yeah, baby Grok. - It's baby Grok right now. But Baby Grok's still pretty good. But it's an order of magnitude\nless sophisticated than GPT4. And it's now Grok 2,\nwhich finished training, I don't know, six weeks\nago or thereabouts. Grok 2 will be a giant improvement. And then Grok 3 will be, I don't know, order of magnitude better than Grok 2. - And you're hoping for it\nto be like state of the art? Like better than-\n- Hopefully. I mean, this is the goal. I mean, we may fail at this goal. That's the aspiration. - Do you think it matters\nwho builds the AGI, the people and how they think and how they structure their companies and all that kind of stuff? - Yeah, I think it\nmatters that there is a, I think it's important\nthat whatever AI wins is a maximum truth-seeking AI that is not forced to lie\nfor political correctness. Well, for any reason really. Political, anything. I'm concerned about AI succeeding that is programmed to\nlie, even in small ways. - Right because, and small\nways becomes big ways. - It become very big ways, yeah. - And when it's used more\nand more at scale by humans. - [Elon] Yeah. - Since I am interviewing Donald Trump- - Cool. - You wanna stop by? - Yeah, sure, I'll stop by. - There was tragically\nan assassination attempt on Donald Trump. After this, you tweeted\nthat you endorse him. What's your philosophy\nbehind that endorsement? What do you hope Donald Trump does for the future of this country and for the future of humanity? - Well, I think that people\nwill tend to take, like, say, an endorsement as,\nwell, I agree with everything that person's ever done\nin their entire life 100% wholeheartedly. And that's not gonna be true of anyone. But we have to pick. We've got two choices,\nreally, for who's president. And it's not just who's president, but the entire administrative\nstructure changes over. And I thought Trump displayed courage under fire, objectively. He's just got shot, he's got\nblood streaming down his face, and he is like fist pumping, saying fight. Like that's impressive. Like you can't feign bravery\nin a situation like that. Like most people would've be ducking. There would not be, 'cause there could be a second\nshooter, you don't know. The president of the United States gotta represent the country,\nand they're representing you. They're representing everyone in America. Well, like you want someone who is strong and courageous to represent the country. That's not to say that\nhe is without flaws. We all have flaws, but on balance. And certainly, at the time,\nit was a choice of Biden, poor, poor guy, has trouble\nclimbing a flight of stairs and the other one's fist\npumping after getting shot. This is no comparison. I mean, who do you want dealing with some of the toughest people\nand other world leaders who are pretty tough themselves? And I mean, I'll tell you like, what are the things that\nI think are important? I think we want a secure border. We don't have a secure border. We want safe and clean cities. I think we wanna reduce\nthe amount of spending that we're at least\nslow down the spending, and 'cause we're currently\nspending at a rate that is bankrupting the country. The interest payments on\nU.S. debt this year exceeded the entire defense department spending. If this continues, all of\nthe federal government taxes will simply be paying the interest. And then if you keep going down that road, you end up in the tragic situation that Argentina had back in the day. Argentina used to be one of the most prosperous places in the world. And hopefully, with Milei taking over, he can restore that. But it was an incredible fall from grace for Argentina to go from being one of the most prosperous\nplaces in the world to being very far from that. So I think we should not take American prosperity for granted. So we really wanna, I think we've gotta reduce\nthe size of government. We've gotta reduce the spending, and we've gotta live within our means. - Do you think politicians, in general, politicians, governments, how much power do you think they have to steer humanity towards good? - I mean, there's a sort of\nage-old debate in history, like, is history determined by\nthese fundamental tides? Or is it determined by\nthe captain of the ship? Both really. I mean, there are tides, but it also matters who's\ncaptain of the ship. So it's a false dichotomy essentially. I mean, there are certainly\ntides, the tides of history. There are real tides of history. And these tides are often\ntechnologically-driven. If you say like the Gutenberg press, the widespread availability of books as a result of a printing press, that was a massive tide of history, and independent of any ruler. But in stormy times, you want the best possible\ncaptain of the ship. - Well, first of all,\nthank you for recommending Will and Ariel Durant's work. I've read the short one for now. - Oh, \"The Lessons of History.\" - Lessons of History. And so one of the lessons, one of the things they highlight is the importance of technology. Technological innovation, which is funny 'cause they wrote so long ago, but they were noticing that the rate of technological\ninnovations was speeding up. Yeah, I would love to see\nwhat they think about now. But yeah, to me, the question\nis how much government, how much politicians get in the way of technological innovation and\nbuilding versus like help it and which politicians,\nwhich kind of policies help technological innovation? 'Cause that seems to be, if\nyou look at human history, that's an important component of empires rising and succeeding. - Yeah. Well, I mean, in terms\nof dating civilization, start of civilization, I think the start of writing in my view, that's my what I think\nis probably the right starting point to date civilization. And from that standpoint,\ncivilization has been around for about 5,500 years\nwhen writing was invented by the ancient Sumerians who are gone now. But the ancient Sumerians, in terms of getting a lot of firsts, those ancient Sumerians really\nhave a long list of firsts. It's pretty wild. In fact, Durant goes\nthrough the list of like, you wanna see first? We'll show you firsts. The Sumerians were just ass kickers. And then the Egyptians\nwere right next door, relatively speaking. They were like weren't that far, developed an entirely\ndifferent form of writing, the hieroglyphics. Cuneiform and hieroglyphic's\ntotally different. And you can actually see the evolution of both hieroglyphics and cuneiform, like the cuneiform starts\noff being very simple and then it gets more complicated. And then towards the end,\nit's like, wow, okay. They really get very\nsophisticated with the cuneiform. So I think civilization\nis about 5,000 years old. And earth is, if physics is correct, four and a half million years old. So civilization has been around for 1000000th of earth's\nexistence, flash in the pan. - Yeah, these are the early, early days. - Very early.\n- We make it very dramatic because there's been rises\nand falls of empires. - Many, so many rises\nand falls of empires. So many. - And there'll be many more. - Yeah, exactly. I mean, only a tiny fraction,\nprobably less than 1% of what was ever written in history is available to us now. I mean, if they didn't put it, literally chisel it in stone\nor put it in a clay tablet, we don't have it. I mean, there's some small\namount of like papyrus scrolls that were recovered that\nare thousands of years old, because they were deep inside a pyramid and weren't affected by moisture. But other than that, it's really gotta be in a clay tablet or chiseled. So the vast majority of\nstuff was not chiseled, 'cause it takes a while to chisel things. So that's why we've\nput tiny, tiny fraction of the information from history. But even that little\ninformation that we do have, and the archeological record shows so many civilizations rising and falling. It's wild. - We tend to think that we're somehow different from those people. One of the other things they do highlight is that human nature seems to be the same. It just persists.\n- Yeah. I mean, the basics of human nature are more or less the same. - Yeah, so we get ourselves in trouble in the same kinds of ways, I think, even with the advanced technology. - Yeah, I mean, you do tend\nto see the same patterns, similar patterns for civilizations\nwhere they go through a life cycle, like an organism, sort of just like a human\nis sort of a zygote, fetus, baby, toddler, teenager, and eventually gets old and dies. The civilizations go through a life cycle. No civilization will last forever. - What do you think it takes\nfor the American empire to not collapse in the near term future in the next 100 years\nto continue flourishing? - Well, the single biggest\nthing that is often actually not mentioned in history books, but Durant does mention\nit is the birthright. So like a perhaps to some, like counterintuitive thing happens when civilizations are\nwinning for too long. The birth rate declines. It can often decline quite rapidly. We're seeing that\nthroughout the world today. Currently, South Korea is like, I think maybe the lowest fertility rate, but there are many others\nthat are close to it. It's like 0.8, I think. If the birth rate doesn't decline further, South Korea will lose roughly\n60% of its population. But every year, that\nbirth rate is dropping. And this is true through\nmost of the world. I don't mean to single out South Korea. It's been happening throughout the world. So as soon as any given civilization reaches a level of prosperity,\nthe birth rate drops. And now you can go and\nlook at the same thing happening in ancient Rome. So Julius Caesar took\nnote of this, I think, around 50-ish BC and tried to pass, I don't know if he was successful, tried to pass a law to give an incentive for any Roman citizen that\nwould have a third child. And I think Augustus was able to, well, he was the dictator so. (laughs) The Senate was just for show. I think he did pass a tax incentive for Roman citizens to have a third child. But those efforts were unsuccessful. Rome fell because the Romans\nstopped making Romans. That's actually the fundamental issue. And there were other things there. There was like, they had\nlike quite a serious malaria, series of malaria epidemics\nand plagues and whatnot. But they had those before. It's just that the birth rate was fallower than the death rate. - It really is that simple? - Well, I'm saying that's- - More people is required.\n- That's at a fundamental level. If a civilization does not at least maintain its numbers, it'll disappear. - So perhaps the amount of compute that the biological computer\nallocates to sex is justified. In fact, we should probably increase it. - Well, I mean, there's\nthis hedonistic sex, which is, you know, that's\nneither here nor there. - Yeah, it's not productive. - It doesn't produce kids. Well, what matters, I mean, Durant makes this very clear, 'cause he looked at one\ncivilization after another and they all went through the same cycle. When the civilization was under stress, the birth rate was high. But as soon as there\nwere no external enemies or they had a extended\nperiod of prosperity, the birth rate inevitably\ndropped every time. I don't believe there's\na single exception. - So that's like the foundation of it. You need to have people. - Yeah. I mean, at base level. No humans, no humanity. - And then there's other\nthings like human freedoms and just giving people the\nfreedom to build stuff. - Yeah, absolutely. But at a basic level, if\nyou do not at least maintain your numbers, if you're\nbelow replacement rate, and that trend continues, you\nwill eventually disappear. This is elementary. Now then obviously,\nalso wanna try to avoid like massive wars. If there's a global thermonuclear war, probably, we're roll\ntoast, radioactive toast. So we wanna try to avoid those things. There's a thing that happens over time with any given civilization, which is that the laws and\nregulations accumulate. And if there's not some forcing function, like a war to clean up\nthe accumulation of laws and regulations, eventually,\neverything becomes legal. And that's like the\nhardening of the arteries, or a way to think of it\nis like being tied down by a million little\nstrings, like Gulliver. You can't move. And it's not like any one of\nthose strings is the issue. You got a million of 'em. So there has to be a sort\nof a garbage collection for laws and regulations so\nthat you don't keep accumulating laws and regulations to the point where you can't do anything. This is why we can't build a\nhigh-speed rail in America. It's illegal. That's the issue. It's illegal six ways to Sunday to build high-speed rail in America. - I wish you could just like, for a week, go into Washington and like\nbe the head of the committee for making, what is it? For the garbage collection,\nmaking government smaller, like removing stuff. - I have discussed with Trump the idea of a government deficiency commission. - Nice, yeah. - And I would be willing to\nbe part of that commission. - I wonder how hard that is. - The antibody reaction\nwould be very strong. - [Lex] Yeah. - So you really have to, you're attacking the matrix at that point. Matrix will fight back. - How are you doing with\nthat, being attacked? - Me, attacked?\n- Yeah. There's a lot of it. - Yeah, there is a lot. I mean, every day, I know psyop. (laughs) Where's my tinfoil hat?\n- How do you keep your just positivity, optimism about the world, a clarity of thinking about the world, so just not become resentful or cynical or all that kind of stuff? Just getting attacked by a very large number of\npeople, misrepresented. - Oh yeah, that's a daily occurrence. - Yes. - I mean, it does get me down at times. I mean, it makes me sad, but, I mean, at some point,\nyou have to sort of say, \"Look, the attacks are by people that actually don't know me. And they're trying to generate clicks.\" So if you can sort of detach\nyourself somewhat emotionally, which is not easy and say, \"Okay, look, this is not actually from\nsomeone that knows me or they're literally just writing to get impressions and clicks, then I guess it doesn't hurt as much.\" It's not quite water off a duck's back. Maybe it's like acid off\na duck's back. (laughs) - All right, well, that's good. Just about your own life, what do you as a measure\nof success in your life? - Measure of success, I'd say like, how many useful things can I get done? - Day-to-day basis,\nwake up in the morning, how can I be useful today? - Yeah. Maximize utility area out\nof the code of usefulness. Very difficult to be useful at scale. - At scale. Can you like speak to what it takes to be useful for somebody like you, where there's so many amazing great teams? Like how do you allocate your\ntime to be in the most useful? - Well, time is the true currency. - Yeah. - So it is tough to say what\nis the best allocation of time. I mean, there are often, say, if you could look at, say, Tesla, I mean Tesla, this year, we'll do over a hundred\nbillion in revenue. So that's $2 billion a week. If I make slightly better decisions, I can affect the outcome\nby a billion dollars. So then I try to do the\nbest decisions I can and on balance, at least\ncompared to the competition. Pretty good decisions. But the marginal value\nof a better decision can easily be in the course of an hour, a hundred million dollars. - Given that, how do you take risks? How do you do the algorithm\nthat you mentioned? I mean, deleting, given a small thing, can be a billion dollars. How do you decide to-\n- Yeah. Well, I think you have to look\nat it on a percentage basis because if you look at\nit in absolute terms, it's just, I would never get any sleep. It would just be like I\nneed to just keep working and work my brain harder. And I'm not trying to\nget as much as possible out of this meat computer. So it's pretty hard, 'cause you can just work all the time. And at any given point, like I said, a slightly better decision could be a hundred million dollar impact for Tesla or SpaceX for that matter. But it is wild when\nconsidering the marginal value of time can be a hundred million dollars an hour at times or more. - Is your own happiness part\nof that equation of success? - It has to be, to some degree. If I'm sad, if I'm depressed,\nI make worse decisions. So I can't have, like if I\nhave zero recreational time, then I make worse decisions. So I don't know a lot,\nbut it's above zero. I mean, my motivation,\nif I've got a religion of any kind is a religion of curiosity, of trying to understand. It's really the mission of\nGrok - understand the universe. I'm trying to understand the universe, or let's at least set things in motion such that at some point, civilization understands the universe far better than we do today. And even what questions to ask. As Douglas Adams pointed out in his book, sometimes, the answer is\narguably the easy part. Trying to frame the question\ncorrectly is the hard part. Once you frame the question correctly, the answer is often easy. So I'm trying to set things in motion such that we are at least at some point able to understand the universe. So for SpaceX, the goal is\nto make life multi-planetary. And which is if you go\nto the foamy paradox of where are the aliens, you've got these sort of great filters. It's just like, why have we\nnot heard from the aliens? Now lot of people think\nthere are aliens among us. I often claim to be one,\nwhich nobody believes me, but I did say alien registration card at one point on my immigration documents. So I've not seen any evidence of aliens. So it suggests that at least\none of the explanations is that intelligent\nlife is extremely rare. And again, if you look\nat the history of earth, civilization has only been around for one millionth of earth's existence. So if aliens had visited here, say, a hundred thousand years\nago, they would be like, \"Well, they don't even have writing.\" Just hunter-gatherers, basically. So how long does a civilization last? So for SpaceX, the goal is to establish a self-sustaining city on Mars. Mars is the only viable\nplanet for such a thing. The moon is close, but it lacks resources, and I think it's probably vulnerable to any calamity that takes out earth. The moon is too close. It's vulnerable to a calamity\nthat takes out earth. So I'm not saying we\nshouldn't have a moon base, but Mars would be far more resilient. The difficulty of getting to Mars is what makes it resilient. In going through these\nvarious explanations of why don't we see the aliens, one of them is that they failed to pass these great filters, these key hurdles. And one of those hurdles is\nbeing a multi-planet species. So if you're a multi-planet species, then if something were to happen, whether that was a natural catastrophe or a manmade catastrophe, at least the other planet\nwould probably still be around. You don't have all the eggs in one basket. And once you are sort\nof a two-planet species, you can obviously extend, to extend life halves\nto the asteroid belt, to maybe the moons of Jupiter and Saturn, and ultimately, to other star systems. But if you can't even\nget to another planet, definitely not getting to star systems. - And the other possible great filters, super powerful technology\nlike AGI, for example. So you're basically trying to knock out one great filter at a time. - Digital super intelligence\nis possibly a great filter. I hope it isn't, but it might be. Guys like say Geoff Hinton would say, he invented a number of the key principles in artificial intelligence. I think he puts the\nprobability of AI annihilation around 10 to 20%, something like that. It's not like, you know,\nlook on the right side. It's 80% likely to be great. (laughs) But I think AI risk\nmitigation is important. Being a multi-planet species would be a massive risk mitigation. And I do wanna sort of once\nagain emphasize the importance of having enough children\nto sustain our numbers and not plummet into population collapse, which is currently happening. Population collapse is a\nreal and current thing. So the only reason it's\nnot being reflected in the total population numbers as much is because people are living longer. It's easy to predict,\nsay, what the population of any given country will be. You just take the birth rate last year, how many babies were born,\nmultiply that by life expectancy, and that's what the population\nwill be a steady state unless if the birth rate\ncontinues at that level. But if it keeps declining,\nit will be even less and eventually dwindle to nothing. So I keep banging on the\nbaby drum here for a reason, because it has been the source\nof civilizational collapse over and over again throughout history. And so why don't we just not\ntry to stable for that day? - Well, in that way, I have\nmiserably failed civilization, and I'm trying, hoping to fix that. I would love to have many kids. - Great, hope you do. No time like the present. - (laughs) Yeah. I gotta allocate more\ncompute to the whole process. But apparently, it's not that difficult. - No, it's like unskilled labor. - Well, one of the things you do for me, for the world is to inspire us with what the future could be. And so some of the things\nwe've talked about, some of the things you're building, alleviating human suffering with Neuralink and expanding the capabilities\nof the human mind, trying to build a colony on Mars, so creating a backup for\nhumanity on another planet, and exploring the possibilities of what artificial intelligence\ncould be in this world, especially in the real world AI, with hundreds of millions, maybe billions of robots walking around. - There will be billions of robots. That's seems almost, that seems virtual certainty. - Well, thank you for building the future, and thank you for inspiring\nso many of us to keep building and creating cool stuff, including kids. - You're welcome. (laughs) Go forth and multiply. - Go forth and multiply. Thank you, Elon. Thanks for talking about it. Thanks for listening to this\nconversation with Elon Musk. And now, dear friends, here's DJ Seo, the co-founder, president,\nand COO of Neuralink. When did you first become fascinated by the human brain? - For me, I was always interested in understanding the purpose of things and how it was engineered\nto serve that purpose, whether it's organic or inorganic, like we were talking earlier\nabout your curtain holders. They serve a clear purpose and they were engineered\nwith that purpose in mind. And growing up, I had a lot\nof interest in seeing things, touching things, feeling things, and trying to really understand the root of how it was designed\nto serve that purpose. And obviously, brain is\njust a fascinating organ that we all carry. It's infinitely powerful machine that has intelligence and cognition\nthat arise from it. And we haven't even scratched the surface in terms of how all of that occurs. But also at the same time,\nI think it took me a while to make that connection to really studying and building tech to understand the brain. Not until graduate school. There were a couple moments,\nkey moments in my life where some of those I think influenced how the trajectory of my\nlife got me to studying what I'm doing right now. One was growing up both\nsides of my family. My grandparents had a very\nsevere form of Alzheimer, and it's incredibly\ndebilitating conditions. I mean, literally, you're\nseeing someone's whole identity and their mind just losing over time. And I just remember thinking\nhow both the power of the mind, but also how something like that could really lose your sense of identity. - It's fascinating that\nthat is one of the ways to reveal the power of a thing by watching it lose the power. - Yeah, a lot of what\nwe know about the brain actually comes from these cases where there are trauma to the brain or some parts of the brain that led someone to\nlose certain abilities. And as a result, there's some correlation and understanding of\nthat part of the tissue being critical for that function. And it's an incredibly fragile organ, if you think about it that way. But also, it's incredibly plastic and incredibly resilient\nin many different ways. - And by the way, the term\nplastic, as we'll use a bunch, means that it's adaptable. So neuroplasticity refers to the adaptability of the human brain. - Correct. Another key moment that sort of influenced how the trajectory of my life have shaped towards the current focus of my life has been during my teenage\nyear when I came to the U.S. I didn't speak a word of English. There was a huge language barrier, and there was a lot of struggle to kind of connect with\nmy peers around me, because I didn't understand\nthe artificial construct that we have created called language, specifically English in this case. And I remember feeling pretty isolated, not being able to connect\nwith peers around me. So spent a lot of time just on my own, reading books, watching movies, and I naturally sort of\ngravitated towards sci-fi books. I just found them really,\nreally interesting. And also, it was a great\nway for me to learn English. Some of the first set of\nbooks that I picked up are \"Ender's Game,\" the whole saga by Orson Scott card, and \"Neuromancer\" from William Gibson, and \"Snow\nCrash\" from Neal Stephenson. And movies like \"Matrix\" was coming out around that time point\nthat really influenced how I think about the potential impact that technology can have\nfor our lives in general. So fast track to my college years, I was always fascinated\nby just physical stuff, building physical stuff, and especially physical things that had some sort of intelligence. And I studied electrical\nengineering during undergrad, and I started out my research in MEMS, so micro-electro-mechanical systems, and really building\nthese tiny nanostructures for temperature sensing. And I just found that to be\njust incredibly rewarding and fascinating subject to just understand how you can build something\nminiature like that that again served a\nfunction and had a purpose. And then I spent large\nmajority of my college years basically building\nmillimeter wave circuits for next gen telecommunication\nsystems, for imaging. And it was just something that I found very, very intellectually interesting. Phase arrays, how the\nsignal processing works for any modern as well as next\ngen telecommunication system, wireless and wireline. EM waves or electromagnetic\nwaves are fascinating. How do you design antennas\nthat are most efficient in a small footprint that you have? How do you make these\nthings energy-efficient? That was something that just consumed my intellectual curiosity. And that journey led\nme to actually apply to and find myself at a PhD\nprogram at UC Berkeley at kind of this consortium called the Berkeley Wireless Research Center that was precisely looking at building, at the time, we called it xg, similar to 3G, 4G, 5G, but the next, next generation G system, and how you would design\ncircuits around that to ultimately go on phones and basically any other devices that are wirelessly connected these days. So I was just absolutely just fascinated by how that entire system works and that infrastructure works. And then also during grad\nschool, I had sort of the fortune of having couple research fellowships that led me to pursue\nwhatever project that I want. And that's one of the things that I really enjoyed about my\ngraduate school career, where you got to kind of pursue\nyour intellectual curiosity and the domain that may not\nmatter at the end of the day, but it's something that really allows you the opportunity to go\nas deeply as you want, as well as as widely as you want. And at the time, I was actually\nworking on this project called the Smart Bandaid,\nand the idea was that when you get a wound,\nthere's a lot of other kind of proliferation of signaling pathway that cells follow to close that wound. And there were hypotheses that when you apply\nexternal electric field, you can actually accelerate\nthe closing of that field by having basically\nelectro taxing of the cells around that wound site. And specifically, not\njust for normal wound, there are chronic wounds that don't heal. So we were interested in building some sort of wearable\npatch that you could apply to kind of facilitate\nthat healing process. And that was in collaboration with Professor Michel Maharbiz, which was a great addition to\nkind of my thesis committee and it really shaped the\nrest of my PhD career. - So this would be the first time you interacted with biology, I suppose. - Correct, correct. I mean, there were some peripheral end application of the wireless imaging and telecommunication\nsystem that I was using for security and bio imaging,\nbut this was a very clear direct application to\nbiology and biological system and understanding the\nconstraints around that and really designing and engineering electrical solutions around it. So that was my first introduction, and that's also kind of how\nI got introduced to Michel. He's sort of known for\nremote control of beetles in the early 2000s. And then around 2013, obviously\nkind of the holy grail when it comes to implantable system is to kind of understand how\nsmall of a thing you can make, and a lot of that is\ndriven by how much energy or how much power you can supply to it and how you extract data from it. So at the time at Berkeley,\nthere was kind of this desire to kind of understand in the neural space what sort of system you can build to really miniaturize\nthese implantable systems. And I distinctively remember\nthis one particular meeting where Michel came in and he's like, \"Guys, I think I have a solution.\" The solution is ultrasound. And then he proceeded\nto kind of walk through why that is the case, and\nthat really formed the basis for my thesis work\ncalled neural dust system that was looking at ways to use ultrasound as opposed to electromagnetic waves for powering as well as communication. I guess I should step back and say the initial goal of the project\nwas to build these tiny, about a size of a neuron\nimplantable system that can be parked next to a neuron, being able to record its state and being able to ping that\nback to the outside world for doing something useful. And as I mentioned, the size\nof the implantable system is limited by how you power the thing and get the data off of it. And at the end of the day, fundamentally, if you look at a human body, we're essentially a bag of salt water, with some interesting\nproteins and chemicals, but it's mostly salt water\nthat's very, very well temperature-regulated\nat 37 degrees Celsius. And we'll get into how, and later, why that's an extremely harsh environment for any electronics to survive, as I'm sure you've experienced\nor maybe not experienced dropping cell phone in a\nsalt water in an ocean. It will instantly kill the device, right? But anyways, just in general, electromagnetic waves don't penetrate through this environment well. And just the speed of\nlight, it is what it is. We can't change it. And based on the wavelength\nat which you are interfacing with the device, the device\njust needs to be big. Like these inductors\nneeds to be quite big. And the general good rule of thumb is that you want the wavefront to\nbe roughly on the order of the size of the thing\nthat you're interfacing with. So an implantable system that is around 10 to 100 micron in dimension in a volume, which is about the size of a neuron that you see in a human body, you would have to operate at\nlike hundreds of gigahertz, which number one, not only is it difficult to build electronics operating\nat those frequencies, but also, the body just attenuates that very, very significantly. So the interesting kind of\ninsight of this ultrasound was the fact that ultrasound just travels a lot more effectively\nin the human body tissue compared to electromagnetic waves. And this is something that you encounter, and I'm sure most people have encountered in their lives when you go to hospitals that are medical ultrasound\nsonograph, right? And they go into very, very deep depth without attenuating\ntoo much of the signal. So all in all, ultrasound,\nthe fact that it travels through the body extremely well and the mechanism to which\nit travels to the body really well is that just the\nwavefront is very different. Its electromagnetic waves are transverse, whereas ultrasound waves are compressive. So it's just a completely different mode of wavefront propagation, and as well as speed of sound is orders and orders of magnitude\nless than speed of light, which means that even at 10\nmegahertz ultrasound wave, your wavefront ultimately is\na very, very small wavelength. So if you're talking about interfacing with the 10 micron or 100\nmicron type structure, you would have 150 micron\nwavefront at 10 megahertz. And building electronics\nat those frequencies are much, much, much easier and they're a lot more efficient. So the basic idea kind of was\nborn out of using ultrasound as a mechanism for powering the device, and then also getting data back. So now the question is, how\ndo you get the data back? The mechanism to which we landed on is what's called backscattering. This is actually something\nthat is very common and that we interface\non a day-to-day basis with our RFID cards, our\nradio frequency ID tag, where there's actually rarely,\nin your ID, a battery inside. There's an antenna and\nthere's some sort of coil that has your serial identification ID and then there's an external device called a reader that\nthen sends a wavefront, and then you reflect back that wavefront with some sort of modulation\nthat's unique to your ID. That's what's called\nbackscattering, fundamentally. So the tag itself actually doesn't have to consume that much energy. And that was a mechanism to which we were kind of thinking about\nsending the data back. So when you have an external\nultrasonic transducer that's sending ultrasonic\nwave to your implant, the neuro dust implant, and it records some information\nabout its environment, whether it's a neuron\nfiring or some other state of the tissue that it's interfacing with, and then it just amplitude modulates the wavefront that comes\nback to the source. - And the recording step\nwould be the only one that requires any energy? So what would require\nenergy in that low step? - Correct, so it is that initial\nkind of startup circuitry to get that recording, amplifying it, and then just modulating. And the mechanism that you can enable that is there is this specialized crystal called piezoelectric crystals\nthat are able to convert sound energy into electrical\nenergy and vice versa. So you can kind of have this interplay between the ultrasonic domain and electrical domain that\nis the biological tissue. - So on the theme of parking very small computational devices next\nto neurons, that's the dream, the vision of brain computer interfaces. Maybe before we talk about Neuralink, can you give a sense of the\nhistory of the field of BCI? What has been maybe the continued dream, and also some of the\nmilestones along the way with the different approaches and the amazing work\ndone at the various labs? - I think a good starting\npoint is going back to 1790s. (Lex laughs) - I did not expect that. - Where the concept of animal electricity or the fact that body is electric was first discovered by Luigi Galvani, where he had this famous experiment where he connected set\nof electrodes to frog leg and ran current through it,\nand then it started twitching, and he said, \"Oh my goodness,\nthe body's electric.\" So fast forward many, many years to 1920s where Hans Berger, who's\nGerman psychiatrist discovered EEG or electroencephalography, which is still around. There are these electrode\narrays that you wear outside the skull that gives you some sort of neural recording. That was a very, very big milestone that you can record\nsome sort of activities about the human mind. And then in the 1940s, there\nwere these group of scientists, Renshaw, Forbes, and\nMorrison that inserted these glass micro\nelectrodes into the cortex and recorded single neurons. The fact that there's signal that are a bit more high resolution and high fidelity as you get closer to the source, let's say. And in the 1950s, these two scientists, Hodgkin and Huxley showed up, and they built this\nbeautiful, beautiful models of the cell membrane\nand the ionic mechanism and had these like circuit diagram. And as someone who's an electric engineer, it's a beautiful model that's\nbuilt out of these partial differential equations,\ntalking about flow of ions, and how that really leads\nto how neurons communicate. And they won the Nobel Prize for that 10 years later in the 1960s. So in 1969, Eb Fetz from\nUniversity of Washington, published this beautiful paper called Operating Conditioning of\nCortical Unit Activity, where he was able to record a single unit neuron from a monkey and was able to have the monkey modulated based on its activity and reward system. So I would say this is the\nvery, very first example, as far as I'm aware, of as closed loop brain computer interface or BCI. - The abstract reads, \"The\nactivity of single neurons in precentral cortex\nof anesthetized monkeys was conditioned by reinforcing high rates of neuronal discharge with\ndelivery of a food pellet. Auditory and visual feedback\nof unit firing rates was usually provided in\naddition to food reinforcement.\" Cool, so they actually got it done. - They got it done. This is back in 1969. - \"After several training sessions, monkeys could increase the\nactivity of newly isolated cells by 50 to 500% above rates\nbefore reinforcement.\" Fascinating. - Brain is very plastic. (Lex laughs) - And so from here, the\nnumber of experiments grew. - Yeah, number of experiments\nas well as set of tools to interface with the\nbrain have just exploded. I think, and also, just\nunderstanding the neural code and how some of the cortical layers and the functions are organized. So the other paper that is pretty seminal, especially in the the motor decoding was this paper in the\n1980s from Georgopoulos that discovered that\nthere's this thing called motor tuning curve. So what are motor tuning curves? It's the fact that there are neurons in the motor cortex of\nmammals, including humans, that have a preferential direction\nthat causes them to fire. So what that means is\nthere are a set of neurons that would increase\ntheir spiking activities when you're thinking\nabout moving to the left, right, up, down, and any of those vectors. And based on that, you\ncould start to think, well, if you can't identify\nthose essential eigenvectors, you can do a lot, and you can actually use that information for actually decoding someone's intended\nmovement from the cortex. So that was a very, very seminal\nkind of paper that showed that there is some sort\nof code you can extract, especially in the motor cortex. - So there's signal there. And if you measure the\nelectrical signal from the brain, that you could actually figure\nout what the intention was. - Correct, yeah, not\nonly electrical signals, but electrical signals from\nthe right set of neurons that give you these\npreferential direction. - Hmm. Okay, so going slowly towards Neuralink, one interesting question\nis what do we understand on the BCI front on invasive\nversus non-invasive? From this line of work,\nhow important is it to park next to the neuron? What does that get you? - That answer fundamentally depends on what you want to do with it, right? There's actually incredible\namount of stuff that you can do with EEG and electrocardiograph, ECoG, which actually doesn't penetrate the cortical layer or parenchyma, but you place a set of electrodes on the surface of the brain. So the thing that I'm\npersonally very interested in is just actually understanding\nand being able to just really tap into the high\nresolution, high fidelity understanding of the activities that are happening at the local level. And we can get into biophysics, but just to kind of step\nback to kind of use analogy, 'cause analogy here can be useful, and sometimes, it's a little bit difficult to think about electricity. At the end of the day, we're\ndoing electrical recording that's mediated by ionic currents, movements of these charged particles, which is really, really hard for most people to think about. But turns out, a lot of the activities that are happening in the brain and the frequency bandwidth,\nwhich starts happening is actually very, very\nsimilar to sound waves, and in our normal\nconversation, audible range. So the analogy that typically\nis used in the field is, if you have a football stadium, there's game going on. If you stand outside the stadium, you maybe get a sense\nof how the game is going based on the cheers and the\nbooze of the home crowd, whether the team is winning or not. But you have absolutely\nno idea what the score is. You have absolutely no idea\nwhat individual audience or the players are talking\nor saying to each other, what the next play is,\nwhat the next goal is. So what you have to do is you\nhave to drop the microphone near into the stadium and\nthen get near the source, like into the individual chatter. In this specific example,\nyou would wanna have it right next to where\nthe huddle's happening. So I think that's kind\nof a good illustration of what we're trying to do when we say invasive or minimally invasive or implanted brain computer interfaces versus non-invasive or\nnon-implanted brain interfaces. It's basically talking about where do you put that microphone, and what can you do with that information. - So what is the biophysics\nof the read and write communication that we're\ntalking about here, as we now step into the\nefforts at Neuralink? - Yeah, so brain is made up of these specialized cells called neurons. There's billions of\nthem, tens of billions. Sometimes, people call\nit a hundred billion that are connected in this\ncomplex yet dynamic network that are constantly remodeling. They're changing their synaptic weights, and that's what we typically\ncall neuroplasticity. And the neurons are also bathed\nin this charged environment that is latent with\nmany charged molecules, like potassium ions,\nsodium ions, chlorine ions. And those actually facilitate\nthese through ionic current, communication between\nthese different networks. And when you look at a neuron as well, they have these membrane\nwith a beautiful, beautiful protein structure called the\nvoltage selective ion channels, which, in my opinion, is one\nof nature's best inventions. In many ways, if you\nthink about what they are, they're doing the job of\na modern day transistors. Transistors are nothing\nmore, at the end of the day, than a voltage-gated conduction channel. And nature found a way to have that very, very early on in its evolution. And as we all know, with the transistor, you can have many, many computation and a lot of amazing things\nthat we have access to today. So I think it's one of\nthose, just as a tangent, just a beautiful, beautiful invention that the nature came up with, these voltage-gated ion channels. - I mean, I suppose there's,\non the biological level, every level of the\ncomplexity of the hierarchy of the organism, there's\ngoing to be some mechanisms for storing information\nand for doing computation. And this is just one such way. But to do that with biological and chemical components is interesting. Plus like when neurons, I mean,\nit's not just electricity, it's chemical communication,\nit's also mechanical. I mean, these are like\nactual objects that vibrate. I mean, they move- - Yeah, they're actually, I mean, there's a lot of really,\nreally interesting physics that are involved in, you know, kind of going back to\nmy work on ultrasound during grad school, there are groups, and there were groups,\nand there are still groups looking at ways to cause\nneurons to actually fire an action potential using ultrasound wave. And the mechanism to\nwhich that's happening is still unclear as I understand. It may just be that you're imparting some sort of thermal energy\nand that causes cells to depolarize in some interesting ways. But there are also these ion channels or even membranes that\nactually just open up its pore as there are being\nmechanically shook, right? Vibrated. So there's just a lot of elements of these like move particles, which again, like that's governed by\ndiffusion physics, right? Movements of particles. And there's also a lot of kind\nof interesting physics there. - Also, not to mention, as\nRoger Penrose talks about, there might be some beautiful weirdness in the quantum mechanical\neffects of all of this. And he actually believes\nthat consciousness might emerge from the quantum\nmechanical effects there. So like there's physics, there's\nchemistry, there's biology, all of that is going on there. - Oh yeah, yeah. I mean, you can, yes, there's a lot of levels of\nphysics that you can dive into. But yeah, in the end,\nyou have these membranes with these voltage-gated ion\nchannels that selectively let these charge molecules that are in the extracellular\nmatrix like in and out. And these neurons generally have these like resting potential where there's a voltage difference between inside the cell\nand outside the cell. And when there's some sort\nof stimuli that changes the state such that they\nneed to send information to the downstream network,\nyou start to kind of see these like sort of orchestration of these different molecules going in and out of these channels. They also open up, like\nmore of them open up once it reaches some threshold to a point where you\nhave a depolarizing cell that sends action potential. So it's a just a very beautiful kind of orchestration of these molecules. And what we're trying to do\nwhen we place an electrode or parking it next to a neuron is that you're trying to measure these local changes in the potential. Again, mediated by the\nmovements of the ions. And what's interesting,\nas I mentioned earlier, there's a lot of physics involved. And the two dominant physics for this electrical recording domain is diffusion physics and electromagnetism. And where one dominates, where Maxwell's equation dominates versus fixed law dominates depends on where your electrode is. If it's close to the source,\nmostly electromagnetic-based, when you're farther away from\nit, it's more diffusion-based. So essentially, when you're\nable to park it next to it, you can listen in on\nthose individual chatter and those local changes in the potential, and the type of signal that you get are these canonical textbook\nneural spiking waveform. The moment you're further away, and based on some of the\nstudies that people have done, Christof Koch's lab and others, once you're away from that source by roughly around a hundred micron, which is about a width of a human hair, you no longer hear from that neuron. Or you're no longer able\nto kind of have the system sensitive enough to be able\nto record that particular local membrane potential\nchange in that neuron. And just to kind of give\nyou a sense of scale also, when you look at a hundred micron voxel, so a hundred micron by a hundred micron by a hundred micron box in a brain tissue, there's roughly around 40 neurons and whatever number of\nconnections that they have. So there's a lot in that volume of tissue. So the moment you're outside of that, there's just no hope that you'll be able to detect that change from\nthat one specific neuron that you may care about. - Yeah, but as you're\nmoving about this space, you'll be hearing other ones. So if you move another 100 micron, you'll be hearing chatter from another community.\n- Correct. - And so the whole\nsense is you wanna place as many as possible electrodes and then you're listening to the chatter. - Yeah, you wanna listen to the chatter. And at the end of the day, you also want to basically let the software\ndo the job of decoding. And just to kind of go to, why ECOG and EEG work at all, right? When you have these\nlocal changes, obviously, it's not just this one\nneuron that's activating. There's many, many other networks that are activating all the time. And you do see sort of a general change in the potential of this\nelectro, like this charge medium, and that's what you're recording\nwhen you're farther away. I mean, you still have\nsome reference electrode that's stable in the brain\nthat's just electroactive organ, and you're seeing some\ncombination aggregate action potential changes and\nthen you can pick it up, right? It's a much slower changing signals. But there are these like canonical kind of oscillations and waves, like gamma waves, beta waves. Like when you sleep, that can be detected, 'cause there's sort of a synchronized kind of global effect of the\nbrain that you can detect. And I mean, the physics of this go, I mean, if we really wanna\ngo down that rabbit hole, like there's a lot that\ngoes on in terms of like why diffusion physics\nat some point dominates when you're further away from the source. It's just a charged medium. So similar to how when you\nhave electromagnetic ways propagating in atmosphere or in a charged medium like a plasma, there's this weird shielding\nthat happens that actually further attenuates the signal\nas you move away from it. So yeah, you see, like if you do a really, really deep dive on kind of the signal\nattenuation over distance, you start to see kind\nof one of where square in the beginning, and\nthen exponential drop off. And that's the knee at which you go from electromagnet magnetism dominating to diffusion physics dominating. - But once again, with the\nelectrodes, the biophysics, you need to understand it's not as deep, because no matter where\nyou're placing that, you're listening to a small\ncrowd of local neurons. - Correct, yeah. So once you penetrate the brain, you're in the arena, so to speak. - And there's a lot of neurons. - [DJ] There are many, many of 'em. - But then again, there's a\nwhole field of neuroscience that's studying like how\nthe different groupings, the different sections of\nthe seating in the arena, what they usually are responsible for, which is where the metaphor\nprobably falls apart, 'cause the seating is not\nthat organized in an arena. - Also, most of them are silent. They don't really do much, or their activities are, you know, you have to hit it with just\nthe right set of stimulus. - So they're usually quiet. - They're usually very quiet. There's, I mean, similar to\ndark energy and dark matter, there's dark neurons. What are they all doing? When you place these electrode, again, like within this a hundred micron volume, you have 40 or so neurons. Like why do you not see 40 neurons? Why do you see only a handful? What is happening there? - Well, they're mostly quiet, but like when they speak, they\nsay profound shit, I think. That's the way I'd like to think about it. Anyway, before we zoom in\neven more, let's zoom out. So how does Neuralink work? From the surgery to the implant to the signal and the decoding process and the human being\nable to use the implant to actually affect the world outside? And all of this, I'm asking in the context of there's a gigantic historic\nmilestone in Neuralink just accomplished in January of this year, putting a Neuralink implant in the first human being, Noland. And there's been a lot to talk about there about his experience,\nbecause he's able to describe all the nuance and the beauty and the fascinating\ncomplexity of that experience of everything involved. But on the technical level,\nhow does Neuralink work? - Yeah, so there are\nthree major components to the technology that we're building. One is the device, the thing\nthat's actually recording these neural chatters. We call it N1 implant or The Link. And we have a surgical robot that's actually doing an implantation of these tiny, tiny wires\nthat we call threads that are smaller than human hair. And once everything is surgirized, you have these neural signals, these spiking neurons that\nare coming out of the brain and you need to have some\nsort of software to decode what the users intend to do with that. So there's what's called\nthe Neuralink application, or B1 app that's doing that translation, is running the very, very\nsimple machine learning model that decodes these inputs\nthat are neural signals and then convert it to a\nset of outputs that allows our participant, first participant Noland, to be able to control a cursor on this. - And this is done wirelessly? - And this is done wirelessly. So our implant is actually a two-part. The link has these flexible\ntiny wires called threads that have multiple\nelectrodes along its length. And they're only inserted\ninto the cortical layer, which is about three to five\nmillimeters in a human brain in the motor cortex region. That's where the kind of the\nintention for movement lies in. And we have 64 of these threads, each thread having 16 electrodes along the span of three to four millimeters, separated by 200 microns. So you can actually record along\nthe depth of the insertion. And based on that signal,\nthere's custom integrated circuit or ASIC that we built that\namplifies the neural signals that you're recording\nand then digitizing it and then has some mechanism for detecting whether there was an interesting event that is a spiking event\nand decide to send that, or not send that through\nBluetooth to an external device, whether it's a phone or a computer that's running this Neuralink application. - So there's onboard\nsignal processing already just to decide whether this is\nan interesting event or not. So there is some computational\npower on board inside in addition to the human brain? - Yeah, so it does the signal processing to kind of really compress\nthe amount of signal that you're recording. So we have a total of\na thousand electrodes sampling at just under 20\nkilohertz with 10 bit each. So that's 200 megabits. That's coming through to the chip, from thousand channel\nsimultaneous neural recording. And that's quite a bit of data. And there are technology available to send that off wirelessly, but being able to do that\nin a very, very thermally constrained environment that is a brain, so there has to be some amount\nof compression that happens to send off only the\ninteresting data that you need, which in this particular case, for motor decoding is\noccurrence of a spike or not. And then being able to use that to decode the intended cursor movement. So the implant itself processes it, figures out whether a\nspike happened or not with our spike detection algorithm, and then sends it off, packages it, sends it off through Bluetooth\nto an external device that then has the model to decode, okay, based on the spiking inputs, did Noland wish to go\nup, down, left, right, or click, or right click, or whatever? - All of this is really fascinating, but let's stick on the N1 implant itself, so the thing that's in the brain. So I'm looking at a picture\nof it, there's an enclosure, there's a charging call, so we didn't talk about the\ncharging, which is fascinating. The battery, the power\nelectronics, the antenna. Then there's the signal\nprocessing electronics. I wonder if there's more kinds of signal processing you can do. That's another question. And then there's the threads themselves with the enclosure on the bottom. So maybe to ask about the charging, so there's a external charging device. - Mm-hmm, yeah, there's an\nexternal charging device. So yeah, the second part of the implant, the threads are the ones, again, just the last three to five\nmillimeters are the ones that are actually penetrating the cortex. Rest of it is, actually, most of the volume\nis occupied by the battery, rechargeable battery. And it's about a size of a quarter. I actually have a device here, if you wanna take a look at it. This is the flexible\nthreat component of it. And then this is the implant. So it's about a size of a U.S. quarter. It's about nine millimeter thick. So basically, this implant, once you have the craniectomy\nand the directomy, threads are inserted, and the hole that you\ncreated, this craniectomy, gets replaced with that. So basically, that thing plugs that hole, and you can screw in these self-drilling cranial screws to hold it in place. And at the end of the day, once\nyou have the skin flap over, there's only about two\nto three millimeters. That's obviously transitioning off of the top of the implant\nto where the screws are. And that's the minor bump that you have. - Those threads look tiny. That's incredible. That is really incredible. That is really incredible. And also, you're right, most of the actual volume is the battery. Yeah, this is way smaller than I realized. - They are also, the threads\nthemselves are quite strong. - They look strong. - And the thread themselves\nalso has a very interesting feature at the end of it called the loop. And that's the mechanism\nto which the robot is able to interface and manipulate this tiny hair-like structure. - And they're tiny, so\nwhat's the width of a thread? - Yeah, so the width of a\nthread starts from 16 micron and then tapers out to about 84 micron. So average human hair is about\n80 to 100 micron in width. - This thing is amazing. This thing is amazing. - Yes, most of the volume\nis occupied by the battery, rechargeable lithium ion cell. And the charging is done\nthrough inductive charging, which is actually very commonly used. Most cell phones have that. The biggest difference is that for us, usually when you have a phone and you wanna charge it on a charging pad, you don't really care how hot it gets, whereas for us, it matters. There's a very strict regulation and good reasons to not actually increase the surrounding tissue temperature\nby two degrees Celsius. So there's actually a lot of innovation that is packed into this to allow charging of this implant without causing that temperature threshold to reach. And even small things like,\nyou see this charging coil and what's called the\nferrite shield, right? So without that ferrite shield, what you end up having\nwhen you have resonant inductive charging is\nthat the battery itself is a metallic can, and you\nform these Eddy currents from external charger\nand that causes heating and that actually contributes\nto inefficiency in charging. So this ferrite shield, what\nit does is that it actually concentrate that field\nline away from the battery and then around the coil that's\nactually wrapped around it. - There's a lot of really\nfascinating design here to make it, I mean, you're\nintegrating a computer into a complex biological system. - Yeah, there's a lot of innovation here. I would say that part of what enabled this was just the innovations in the wearable. There's a lot of really, really powerful, tiny, low power microcontrollers,\ntemperature sensors, or various different sensors\nand power electronics. A lot of innovation really came\nin the charging coil design, how this is packaged, and\nhow do you enable charging such that you don't really\nexceed that temperature limit, which is not a constraint\nfor other devices out there. - So let's talk about\nthe threads themselves, those tiny, tiny, tiny things. So how many of them are there? You mentioned a thousand electrodes. How many threads are there, and what did the electrodes\nhave to do with the threads? - Yeah, so the current instantiation of the device has 64 threads, and each thread has 16 electrodes for a total of 1,024 electrodes that are capable of both\nrecording and stimulating. And the thread is basically\nthis polymer insulated wire. The metal conductor is\nthe kind of tiramisu cake of ti, plat, gold, plate, ti. And they're very, very tiny wires. Two micron in width, so\n2/1000000th of meter. - It's crazy that that\nthing I'm looking at has the polymer installation,\nhas the conducting material, and has 16 electrodes at the end of it. - On each of those threads. - Yeah, on each of those threads. - Correct.\n- 16, each one of those. - You're not gonna be able\nto see it with naked eyes. - And I mean, to state the obvious, or maybe for people\nwho are just listening, they're flexible. - Yes, yes, that's also one element that was incredibly important for us. So each of these thread\nare, as I mentioned, 16 micron in width and then\nthey taper to 84 micron, but in thickness, they're\nless than five micron. And thickness is mostly\npolyamide at the bottom and this metal track and\nthen another polyamide. So two micron of polyamide, 400 nanometer of this metal stack, and two micron of polyamide\nsandwiched together to protect it from the environment that is 37 degrees C bag of salt water. - So what's some, maybe\ncan you speak to some interesting aspects of\nthe material design here? Like what does it take to\ndesign a thing like this and to be able to\nmanufacture a thing like this for people who don't know\nanything about this kind of thing? - Yeah, so the material\nselection that we have is not, I don't think it was particularly unique. There were other labs\nand there are other labs that are kind of looking\nat similar material stack. There's kind of a fundamental question and still needs to be\nanswered around the longevity and reliability of these\nmicro electrodes that we call, compared to some of the\nother more conventional, neural interfaces, devices\nthat are intracranial. So penetrating the cortex\nthat are more rigid, like the Utah array. There are these four by four millimeter kind of silicon shank that have exposed recording site at the end of it. And that's been kind of the innovation from Richard Normann back in 1997. It's called the Utah Array 'cause he was at University of Utah. - And what does the Utah array look like? So it's a rigid type of- - Yeah, so we can actually look it up. - Oh.\n- Yeah. (Lex laughing) Yeah, so it's a bed of needle. There's-\n- (laughs) Yeah. Okay, go ahead, I'm sorry. - So those are rigid-\n- Rigid, yeah. You weren't kidding. - And the size and the\nnumber of shanks vary, anywhere from 64 to 128. At the very tip of it\nis an exposed electrode that actually records neural signal. The other thing that's\ninteresting to note is that unlike Neuralink threads that\nhave recording electrodes that are actually exposed\niridium oxide recording sites along the depth, this is\nonly at a single depth. So these Utah array spokes can be anywhere between 0.5 millimeters to 1.5 millimeter. And they also have\ndesigns that are slanted. So you can have it inserted\nat different depth, but that's one of the\nother big differences. And then, I mean, the main key\ndifference is the fact that there's no active electronics. These are just electrodes, and then there's a bundle of\na wire that you're seeing, and then that actually\nthen exits the craniectomy that then has this port\nthat you can connect to for any external electronic devices. They are working on a or have\nthe wireless telemetry device, but it still requires\na through the skin port that actually is one of\nthe biggest failure modes for infection for the system. - What are some of the challenges associated with flexible threads? Like for example, on the robotic side, R1, implanting those threads,\nhow difficult does that task? - Yeah, so as you mentioned, they're very, very difficult\nto maneuver by hand. These Utah arrays that you saw earlier, they're actually inserted\nby a neurosurgeon actually positioning it near\nthe site that they want. And then there's a pneumatic hammer that actually pushes them in. So it's a pretty simple process, and they're easy to maneuver. But for these thin film arrays, they're very, very tiny and flexible. So they're very difficult to maneuver. So that's why we built an\nentire robot to do that. There are other reasons\nfor why we built a robot, and that is ultimately,\nwe want this to help millions and millions of people\nthat can benefit from this. And there just aren't that\nmany neurosurgeons out there. And robots can be something that we hope can actually do\nlarge parts of the surgery. But yeah, the robot is this\nentire other sort of category of product that we're working on. And it's essentially this\nmulti-axis gantry system that has the specialized robot head that has all of the optics and this kind of a needle\nretracting mechanism that maneuvers these threads\nvia this loop structure that you have on the thread. - So the thread already\nhas a loop structure by which you can grab it? - Correct, correct.\n- Okay. So this is fascinating. So you mentioned optics,\nso there's a robot - R1. So for now, there's a\nhuman that actually creates a hole in the skull.\n- Mm-hmm. - And then after that, there's\na computer vision component that's finding a way to\navoid the blood vessels. And then you're grabbing it by the loop, each individual thread and placing it in a particular location\nto avoid the blood vessels. And also choosing the depth of placement, all that?\n- Correct. So controlling every, like the 3D geometry of the placement? - Correct. So the aspect of this robot that is unique is that it's not surgeon-assisted\nor human-assisted. It's a semi-automatic or automatic robot. Obviously, there are\nhuman component to it, when you're placing targets. You can always move it away from kind of major vessels that you see. But I mean, we wanna get\nto a point where one click and it just does the\nsurgery within minutes. - So the computer vision\ncomponent finds great targets, candidates and the human\nkind of approves them and the robot does, does it do like one thread at a time or does it do one-\n- It does one thread at a time, and that's\nactually also one thing that we are looking at ways to do multiple threads at a time. There's nothing stopping from it. You can have multiple kind\nof engagement mechanisms, but right now, it's one by one. And we also still do quite a bit of just kind of verification to make sure that it got inserted. If so, how deep? Did it actually match\nwhat was programmed in and so on and so forth? - And the actual electrode\nis a place that vary at differing depths in the like, I mean, it's very small differences,\nbut differences. - [DJ] Yeah, yeah. - And so that there's some\nreasoning behind that, as you mentioned. Like it gets more varied signal. - Yeah, I mean, we try to place them all around three or four\nmillimeter from the surface, just 'cause the span of the electrode, those 16 electrodes that we\ncurrently have in this version spans roughly around three millimeters. So we wanna get all of those in the brain. - This is fascinating. Okay, so there's a million questions here. If we could zoom in\nspecifically on the electrodes, so what is your sense, how many neurons is each\nindividual electrode listening to? - Yeah, each electrode\ncan record from anywhere between 0 to 40, as I\nmentioned, right, earlier. But tactically speaking,\nwe only see about, at most, like two to three. And you can actually\ndistinguish which neuron it's coming from by the\nshape of the spikes. - [Lex] Oh, cool. - So I mentioned the spike\ndetection algorithm that we have. It's called BOSS algorithm, buffer online, spike sorter. - Nice. - It actually outputs\nat the end of the day six unique values, which\nare kind of the amplitude of these like negative\ngoing hump, middle hump, like positive going hump, and then also the time\nat which these happen. And from that, you can have a kind of a statistical probability\nestimation of, is that a spike? Is it not a spike? And then based on that,\nyou could also determine, oh, that spike looks\ndifferent than that spike. Must have come from a different neuron. - Okay, so that's a nice\nsignal processing step from which you can then\nmake much better predictions about if there's a spike.\n- Yeah. - Especially in this kind of context where there could be\nmultiple neurons screaming. And that also results in you being able to compress the data\nbetter in the (indistinct). Okay.\n- And just to be clear, I mean, the labs do what's\ncalled spike sorting. Usually, once you have\nthese like broadband, the fully digitized signals and then you run a bunch of different set of algorithms to kind of tease apart, it's just all of this for\nus is done on the device. - On the device.\n- In a very low power, custom built ASIC digital processing unit. - [Lex] Highly heat constrained? - Highly heat constrained, and the processing time\nfrom signal going in and giving you the output\nis less than a microsecond, which is a very, very\nshort amount of time. - Oh yeah, so the latency has to be super short.\n- Correct. - Oh wow. Oh, that's a pain in the ass. - Yeah, latency is this huge, huge thing that you have to deal with. Right now, the biggest source of latency comes from the Bluetooth, the way in which they're packetized and we bend them in 15 millisecond. - Oh, interesting, it says\ncommunication constraint. Is there some potential innovation there on the protocol used? - Absolutely.\n- Okay. - Yeah, Bluetooth is definitely not our final wireless communication protocol that we wanna get to. - Hence the N1 and the R1. I imagine that increases- - NxRx. - Yeah, that's the communication protocol, 'cause Bluetooth allows you to communicate I guess farther distances\nthan you need to, so you can go much shorter. - Yeah, well, the primary motivation for choosing Bluetooth is that, I mean, everything has Bluetooth. - All right, you can talk to any device. - Interoperability is\njust absolutely essential, especially in this early phase. And in many ways, if\nyou can access a phone or a computer, you can do anything. - Well, it'll be interesting to step back and actually look at, again, the same pipeline that\nyou mentioned for Noland. So what does this whole process look like? From finding and selecting a\nhuman being to the surgery, to the first time he's\nable to use this thing? - So we have what's called\nthe patient registry that people can sign up to\nhear more about the updates. And that was a route to\nwhich Noland applied. And the process is that once\nthe application comes in, it contains some medical records, and based on their medical eligibility, that there's a lot of different inclusion-exclusion\ncriteria for them to meet. And we go through a\nprescreening interview process with someone from Neuralink. And at some point, we\nalso go out to their homes to do a BCI home audit, 'cause one of the most\nkind of revolutionary part about having this N1 system\nthat is completely wireless is that you can use it at home. Like you don't actually\nhave to go to the lab and go to the clinic to get connecterized to these like specialized equipment that you can't take home with you. So that's one of the key elements of when we're designing the system that we wanted to keep in mind, like people hopefully would\nwanna be able to use this every day in the comfort of their homes. And so part of our engagement and what we're looking\nfor during BCI home audit is to just kind of\nunderstand their situation, what other assistive\ntechnology that they use. - And we should also step\nback and kind of say that the estimate is 180,000\npeople live with quadriplegia in the United States, and each year, an additional 18,000 suffer a\nparalyzing spinal cord injury. So these are folks who\nhave a lot of challenges, living a life in terms of accessibility, in terms of doing the\nthings that many of us just take for granted day to day. And one of the things, one\nof the goals of this initial study is to enable them to\nhave sort of digital autonomy, where they by themselves can\ninteract with a digital device using just their mind, something that you're calling telepathy. So digital telepathy, where a\nquadriplegic can communicate with a digital device in all the ways that\nwe've been talking about. Control the mouse cursor, enough to be able to\ndo all kinds of stuff, including play games and tweet\nand all that kind of stuff. And there's a lot of people for whom life, the basics of life are difficult, because of the things that\nhave happened to them. - Yeah, I mean, movement is so\nfundamental to our existence. I mean, even speaking involves movement of mouth, lip, larynx. And without that, it's\nextremely debilitating. And there are many, many\npeople that we can help. And I mean, like especially if you start to kind of look at other\nforms of movement disorders that are not just from spinal cord injury, but from ALS, MS, or even\nstroke and/or just aging, right? That leads you to lose\nsome of that mobility, that independence, it's\nextremely debilitating. - And all of these are\nopportunities to help people, to help alleviate suffering, to help improve the quality of life. But each of the things you mentioned is its own little puzzle that needs to have increasing\nlevels of capability from a device like a Neuralink device. And so the first one\nyou're focusing on is, it's just a beautiful word, telepathy. So being able to\ncommunicate using your mind wirelessly with a digital device. Can you just explain exactly\nwhat we're talking about? - Yeah, I mean, it's exactly that. I mean, I think if you are\nable to control a cursor and able to click and\nbe able to get access to computer or phone, I mean, the whole world opens up to you. And I mean, I guess the word telepathy, if you kind of think about that as just definitionally\nbeing able to transfer information from my brain to your brain without using some of the\nphysical faculties that we have, like voices. - But the interesting thing here is, I think the thing that's\nnot obviously clear is how exactly it works. So in order to move a cursor, there's at least a couple\nways of doing that. So one is you imagine\nyourself maybe moving a mouse with your hand, or you can\nthen, which Noland talked about, like imagine moving the\ncursor with your mind. But it's like there is\na cognitive step here that's fascinating, 'cause\nyou have to use the brain and you have to learn\nhow to use the brain. And you kind of have to\nfigure it out dynamically. Because you reward yourself if it works. I mean, there's a step that, this is just a fascinating step, 'cause you have to get the brain to start firing in the right way. And you do that by imagining. Like fake it till you make it. (laughs) And all of a sudden, it creates\nthe right kind of signal that if decoded correctly,\ncan create the kind of effect. And then there's like noise around that, you have to figure all of that out. But on the human side, imagine the cursor moving\nis what you have to do. - Yeah, he says using the force. - The force. I mean, isn't that just\nlike fascinating to you that it works? Like to me, it's like, holy\nshit, that actually works. Like you could move a\ncursor with your mind. - As much as you're\nlearning to use that thing, that thing's also learning about you. Like our model is constantly\nupdating the weights to say, \"Oh, if someone is thinking about this sophisticated forms\nof like spiking patterns, like that actually means\nto do this, right?\" - So the machine is\nlearning about the human and the human is learning\nabout the machine. So there is a adaptability to the signal processing,\nthe decoding step. And then there's the adaptation\nof Noland, the human being. Like the same way, if you give\nme a new mouse and I move it, I learn very quickly\nabout its sensitivity, so I learn to move it slower. And then there's other\nkinds of signal drift and all that kind of stuff\nthey have to adapt to. So both are adapting to each other. - [DJ] Correct. - That's a fascinating\nlike software challenge on both sides, the software on both, the human software and- - The organic and the inorganic. - The organic and the inorganic. Anyway, so sorry to rudely interrupt. So there's this selection that Noland has passed with flying colors. So everything including that the, it's a BCI-friendly home, all of that. So what is the process of\nthe surgery implantation, the first moment when he\ngets to use the system? - The end to end, we say\npatient end to patient out, is anywhere between two to four hours. In particular case for Noland, it was about three and a half hours. And there's many steps leading to the actual\nrobot insertion, right? So there's anesthesia induction, and we do intra-op CT imaging to make sure that we're drilling the hole in the right location. And this is also pre-planned beforehand. Someone like Nolan would go through fMRI, and then they can think\nabout wiggling their hand. And obviously, due to their injury, it's not gonna actually lead\nto any sort of intended output. But it's the same part of the brain that actually lights up\nwhen you're imagining moving your finger to\nactually moving your finger. And that's one of the ways\nin which we can actually know where to place our threads, 'cause we wanna go into what's called a hand knob area in the motor cortex. And as much as possible,\ndensely put our electro threads. So yeah, we do intra-op CT imaging to make sure and double check the location of the craniectomy. And surgeon comes in, does their thing in terms of like skin\nincision, craniectomy, so drilling of the skull, and then there's many\ndifferent layers of the brain. There's what's called a dura, which is a very, very thick\nlayer that surrounds the brain, that gets actually resected in\na process called atherectomy. And that then exposed the pia in the brain that you wanna insert. And by the time it's been around anywhere between one to\none and a half hours, robot comes in, does its thing,\nplacement of the targets, inserting of the thread. That takes anywhere\nbetween 20 to 40 minutes. In the particular case for Noland, it was just under or it\nwas just over 30 minutes. And then after that, the surgeon comes in. There's a couple other steps of like actually inserting\nthe dural substitute layer to protect the thread\nas well as the brain. And then yeah, screw in the\nimplant and then skin flap and then suture and then you're out. - So when Noland woke\nup, what was that like? What's the recovery like, and when was the first\ntime he was able to use it? - So he was actually\nimmediately, after the surgery, like an hour after the\nsurgery as he was waking up, we did turn on the device, make sure that we are\nrecording neural signals, and we actually did have a couple signals that we notice that he\ncan actually modulate. And what I mean by modulate is that he can think about crunching his fist, and you could see the\nspike disappear and appear. (Lex laughing) - That's awesome. - And that was immediate, right? Immediate after in the recovery room. - How cool is that? - Yeah.\n- That's a human being. I mean, what did that feel like for you? This device and a human being, a first step of a gigantic journey? I mean, it's a historic moment. Even just that spike, just\nto be able to modulate that. - Obviously, there have been\nother, as you mentioned, pioneers that have participated\nin these groundbreaking BCI investigational early feasibility studies. So we're obviously standing in the shoulders of the giants here. We're not the first ones to actually put electrodes in the human brain. But I mean, just leading\nup to the surgery, I definitely could not sleep. It's the first time that you're working in a completely new environment. We had a lot of confidence\nbased on our benchtop testing or preclinical R&D studies\nthat the mechanism, the threads, the insertion,\nall that stuff is very safe, and that it's obviously ready\nfor doing this in a human, but there's still a lot of unknown unknown about can the needle actually insert? I mean, we brought\nsomething like 40 needles just in case they break, and\nwe ended up using only one. But I mean, that was a level of just complete unknown, right? 'Cause it's a very, very\ndifferent environment. And I mean, that's why we do clinical trial in the first place to be able to test these things out. So extreme nervousness and just many, many sleepless night leading up to the surgery and definitely the day before the surgery, and it was an early morning surgery. Like we started at seven in the morning. And by the time, it was around 10:30. Everything was done. But I mean, first time seeing that, well, number one, just huge relief that this thing is doing\nwhat it's supposed to do. And two, I mean, just\nimmense amount of gratitude for Noland and his family, and then many others that have applied and that we've spoken to and will speak to are, I mean, true pioneers everywhere. And I sort of call them the neural astronauts or neuralnaut. These amazing, just\nlike in the '60s, right? Like these amazing just pioneers, right? Exploring the unknown outwards. In this case, it's inward. But incredible amount\nof gratitude for them to just participate and play a part. And it's a journey that\nwe're embarking on together. But also like, I think it was just, that was an very, very\nimportant milestone, but our work was just starting. So a lot of just kind of anticipation for, okay, what needs to happen next? What are set of sequences of events that needs to happen for\nus to make it worthwhile for both Noland as well as us? - Just to linger on that, just\na huge congratulations to you and the team for that milestone. I know there's a lot of work left, but that's really exciting to see. That's a source of hope. It's this first big step, opportunity to help hundreds\nof thousands of people and then maybe expand\nthe realm of the possible for the human mind for millions\nof people in the future. So it's really exciting. So like the opportunities\nare all ahead of us, and to do that safely and\nto do that effectively was really fun to see. As an engineer just watching\nother engineers come together and do an epic thing, that was awesome. Huge congrats. - Thank you, thank you. Yeah, could not have\ndone it without the team. And yeah, I mean, that's the other thing that I told the team as well, of just this immense sense\nof optimism for the future. I mean, it's a very important\nmoment for the company, needless to say, as well as hopefully for many others out\nthere that we can help. - So speaking of challenges, Neuralink published a blog post describing that some of\nthe threads retracted. And so the performance, as measured by bits per\nsecond dropped at first, but then eventually, it was regained. And that the whole story\nof how it was regained is super interesting. That's definitely something I'll talk to Bliss and to Noland about. But in general, can you speak\nto this whole experience? How was the performance regained and just the technical\naspects of the threads being retracted and moving? - The main takeaway is that in the end, the performance have come back and it's actually gotten\nbetter than it was before. He's actually just beat the\nworld record yet again last week to 8.5 BPS, so I mean, he's just cranking and\nhe's just improving. - [Lex] The previous one\nthat he set was eight. - Correct.\n- He said 8.5. - Yeah, the previous world\nrecord in human was 4.6. - Yeah.\n- So it's almost double. And his goal is to try to get to 10, which is roughly around kind of the median Neuralinker using a mouse with the hand. So it's getting there. - So yeah, so the\nperformance was regained. - Yeah, better than before. So that's a story on its own of what took the BCI team to recover that performance. It was actually mostly on\nkind of the signal processing. And so as I mentioned,\nwe were kind of looking at these spike outputs\nfrom our electrodes. And what happened is\nthat kind of four weeks into the surgery, we\nnoticed that the threads have solely come out of the brain. And the way in which we noticed\nthis, at first obviously, is that, well, I think Noland\nwas the first to notice that his performance was degrading. And I think at the time,\nwe were also trying to do a bunch of different experimentation, different algorithms,\ndifferent sort of UI/UX. So it was expected that\nthere will be variability in the performance, but we did\nsee kind of a steady decline. And then also, the way in\nwhich we measure the health of the electrodes or whether\nthey're in the brain or not, is by measuring impedance\nof the electrode. So we look at kind of the interfacial, kind of the Randall circuit, they say, the capacitance and the resistance between the electrosurface and the medium. And if that changes in some dramatic ways, we have some indication. Or if you're not seeing\nspikes on those channels, you have some indications that\nsomething's happening there. And what we notice is that\nlooking at those impedance plot and spike rate plots,\nand also because we have those electrodes\nrecording along the depth, you are seeing some sort of movement that indicated that the\nrest were being pulled out. And that obviously will\nhave an implication on the model side, because\nif the number of inputs that are going into the model is changing, 'cause you have less of them, that model needs to get updated, right? But there were still\nsignals, and as I mentioned, similar to how, even when\nyou place the signals on the surface of the\nbrain, or farther away, like outside the skull, you\nstill see some useful signals. What we started looking at is\nnot just the spike occurrence through this BOSS\nalgorithm that I mentioned, but we started looking at just the power of the frequency band that is interesting for Noland to be able to modulate. So once we kind of change the algorithm for the implant to not just\ngive you the BOSS output, but also these spike band power output, that helped us sort of, we find the model with the new set of inputs,\nand that was the thing that really ultimately gave\nus the performance back. In terms of, and obviously,\nlike the thing that we want ultimately, and the thing\nthat we are working towards, is figuring out ways in which we can keep those threads intact\nfor as long as possible so that we have many more\nchannels going into the model. That's by far the number one priority that the team is currently embarking on to understand how to\nprevent that from happening. The thing that I'll say also\nis that, as I mentioned, this is the first time ever that we're putting these\nthread in a human brain, and human brain, just for size reference, is 10 times out of the monkey\nbrain or the sheep brain. And it's just a very, very\ndifferent environment. It moves a lot more. It like actually moved a\nlot more than we expected when we did Noland's surgery. And it's just a very,\nvery different environment than what we're used to. And this is why we do\nclinical trial, right? We wanna uncover some of these issues and failure modes earlier than later. So in many ways, it's provided us with this enormous amount of data and information to be able to solve this. And this is something that\nNeuralink is extremely good at. Once we have set of clear objective and engineering problem,\nwe have enormous amount of talents across many, many disciplines to be able to come together and fix the problem very, very quickly. - But it sounds like one of\nthe fascinating challenges here is for the system and the decoding side to be adaptable across\ndifferent timescales. So whether it's movement of threads or different aspects of signal drift sort of on the software\nof the human brain, something changing,\nlike Noland talks about cursor drift that could be corrected, and there's a whole UX\nchallenge to how to do that. So it sounds like adaptability is like a fundamental property\nthat has to be engineered in. - It is, and I mean I think, I mean, as a company, we're\nextremely vertically integrated. We make these thin film\narrays in our own microfab. - Yeah, there's, like\nyou said, built in-house. This whole paragraph\nhere from this blog post is pretty gangster. \"Building the technology described above has been no small feat.\" And there's a bunch of links here that I recommend people click on. \"We constructed in-house\nmicro fabrication capabilities to rapidly produce various\niterations of thin film arrays that constitute our electrode threads. We created a custom femtosecond laser mill to manufacture components\nwith micro level precision.\" I think there's a tweet\nassociated with this. - That's a whole thing\nthat we can get into. - Yeah, okay, well, what\nare we looking at here? This thing?\n- Yeah. \"So in less than one minute,\nour custom-made femtosecond laser mill cuts this geometry\nin the tips of our needles.\" So we're looking at this\nweirdly-shaped needle. \"The tip is only 10 to\n12 microns in width, only slightly larger than the\ndiameter of a red blood cell. The small size allows\nthreats to be inserted with minimal damage to the cortex.\" Okay, so what's interesting\nabout this geometry? So we're looking at this\njust geometry of a needle. - This is the needle that's engaging with the loops in the thread. So they're the ones that thread the loop and then peel it from the silicon backing. And then this is the\nthing that gets inserted into the tissue, and then this pulls out, leaving the thread. And this kind of a\nnotch or the shark tooth that we used to call is the thing that actually is grasping the loop. And then it's designed in such way, such that when you pull\nout, leaps the loop. - And the robot is\ncontrolling this needle? - Correct, so this is\nactually housed in a cannula. And basically, the robot\nhas a lot of the optics that look for where the loop is. There's actually a 405 nanometer light that actually causes the\npolyamide to fluoresce so that you can locate\nthe location of the loop. - So the loop lights up? - Yeah, yeah, they do. It's a micron precision process. - What's interesting about the robot that it takes to do that? That's pretty crazy. That's pretty crazy that robot is able to get this kind of precision. - Yeah, our robot is quite heavy. Our current version of it. There's, I mean, it's\nlike a giant granite slab that weighs about a ton, 'cause it needs to be\nsensitive to vibration, environmental vibration. And then as the head is moving\nat the speed that is moving, there's a lot of kind of motion control to make sure that you can\nachieve that level of precision. A lot of optics that\nkind of zoom in on that. We're working on next\ngeneration of the robot that is lighter, easier to transport. I mean, it is a feat to move the robot. - And it's far superior to a human surgeon at this time for this particular task. - Absolutely, I mean, let alone\nyou try to actually thread a loop in a sewing kit,\nI mean this is like, we're talking like\nfractions of human hair. These things, it's not visible. - So continuing the paragraph, \"We developed novel hardware\nand software testing systems such as our accelerated\nlifetime testing racks and simulated surgery\nenvironment,\" which is pretty cool, \"to stress test and validate the robustness of our technologies. We performed many\nrehearsals of our surgeries to refine our procedures and\nmake them second nature.\" This is pretty cool. \"We practice surgeries on proxies with all the hardware and\ninstruments needed in our mock or in the engineering space. This helps us rapidly test and measure.\" So there's like proxies. - Yeah, this proxy's super cool actually. So there's a 3D printed\nskull from the images that is taken at Barrow, as well as this hydrogel mix, sort of synthetic polymer\nthing that actually mimics the mechanical properties of the brain. It also has vasculature of the person. So basically, what we're\ntalking about here, and there's a lot of work that has gone into making this set proxy that it's about like finding the right concentration of these different synthetic polymers to get the right set of consistency\nfor the needle dynamics, as they're being inserted. But we practice this\nsurgery with the person, Noland's basically physiology\nand brain many, many times prior to actually doing the surgery. - So every step, every step? - Every step, yeah. Like where does someone stand? I mean, what you're\nlooking at is the picture. This is in our office\nof this kind of corner of the robot engineering space that we have created this like mock or space that looks exactly like what they would experience, all the staff would experience\nduring their actual surgery. So I mean, it's just kind\nof like any dance rehearsal where you know exactly\nwhere you're gonna stand at what point and you just\npractice that over and over and over again with an exact anatomy of someone that you're going to surgerize. And it got to a point where\na lot of our engineers, when we created a\ncraniectomy, they're like, \"Oh, that looks very familiar. We've seen that before.\" - Yeah. Man, there's wisdom you can gain through doing the same thing\nover and over and over. It's like a Jira dreams\nof sushi kind of thing, because then it's like\nOlympic athletes visualize the Olympics, and then\nonce you actually show up, it feels easy. It feels like any other day. It feels almost boring\nwinning the gold medal, 'cause you visualized this so many times, you've practiced this so many times, and nothing bothers you. It's boring. You win the gold medal, it's boring. And the experience they talk\nabout is mostly just relief, probably that they don't\nhave to visualize it anymore. - Yeah, the power of\nthe mind to visualize, I mean, there's a whole field that studies where muscle memory lies in cerebellum. Yeah, it's incredible. - I think it is a good\nplace to actually ask sort of the big question\nthat people might have is, how do we know every aspect of this that you describe is safe? - At the end of the day, the gold standard is to look at the tissue. What sort of trauma did\nyou cause the tissue? And does that correlate\nto whatever behavioral anomalies that you may have seen? And that's the language to\nwhich we can communicate about the safety of inserting\nsomething into the brain and what type of trauma\nthat you can cause. So we actually have an entire department, department of pathology that\nlooks at these tissue slices. There are many steps that\nare involved in doing this. Once you have studies that are launched with particular endpoints in mind, at some point, you have\nto euthanize the animal and then you go through necropsy to kind of collect the\nbrain tissue samples. You fix them in formalin, and you like gross them, you section them, and you look at individual slices just to see what kind of\nreaction or lack thereof exists. So that's the kind of the\nlanguage to which FDA speaks and as well for us to kind\nof evaluate the safety of the insertion mechanism\nas well as the threads at various different time points. Both acute, so anywhere\nbetween zero to three months to beyond three months. - So those are kind of the details of an extremely high standard of safety that has to be reached. - Correct.\n- FDA supervises this, but this, in general,\njust a very high standard. And every aspect of this,\nincluding the surgery, I think Matthew MacDougall has mentioned that like the standard is, let's say, how to put it politely? Higher than maybe some other operations that we take for granted. So the standard for all\nthe surgical stuff here is extremely high. - Very high. I mean, it's a highly,\nhighly regulated environment with the governing\nagencies that scrutinize every medical device that gets marketed. And I think it's a good thing. It's good to have those high standards, and we try to hold\nextremely high standards to kind of understand what\nsort of damage, if any, these innovative emerging technologies and new technologies\nthat we're building are. And so far, we have\nbeen extremely impressed by lack of immune response\nfrom these threads. - Speaking of which, you talk to me with\nexcitement about the histology and some of the images\nthat you're able to share. Can you explain to me\nwhat we're looking at? - Yeah, so what you're looking\nat is a stained tissue image. So this is a sectioned tissue slice from an animal that was\nimplanted for seven months. So kind of a chronic time point. And you're seeing all\nthese different colors, and each color indicates\nspecific types of cell types. So purple and pink are astrocytes\nand microglia respectably. They're types of glial cells. And yet the other thing that\npeople may not be aware of is your brain is not just made up of soup of neurons and axons. There are other cells, like glial cells, that actually kind of is\nthe glue and also react if there are any trauma\nor damage to the tissue. - The brown are the neurons here? - The brown are the neurons.\n- The modern neurons. - So what you're seeing is,\nin this kind of macro image, you're seeing these like\ncircle highlighted in white, the insertion sites. And when you zoom into one of\nthose, you see the threads. And then in this particular case, I think we're seeing about the 16 wires that are going into the page. And the incredible thing here is the fact that you have the neurons that\nare these brown structures or brown circular or elliptical thing that are actually touching\nand abutting the thread. So what this is saying is that\nthere's basically zero trauma that's caused during this insertion. And with these neural interfaces, these micro electrodes that you insert, that is one of the most\ncommon mode of failure. So when you insert these\nthreads, like the Utah array, it causes neuronal death around the site, because you're inserting\na foreign object, right? And that kind of elicit\nthese like immune response through microglia and astrocytes. They form this like\nprotective layer around it. Oh, not only are you\nkilling the neuron cells, but you're also creating\nthis protective layer that then basically prevents you from recording neural signals, 'cause you're getting\nfurther and further away from the neurons that\nyou're trying to record. And that is the biggest mode of failure. And in this particular\nexample, in that inside, it's about 50 micron with that scale bar. The neurons just seem\nto be attracted to it. (Lex laughing) - And so there's certainly no trauma. That's such a beautiful image, by the way. So the brown are the neurons. And for some reason, I can't look away. It's really cool.\n- Yeah, and the way that these things like, I mean, your tissues generally don't\nhave these beautiful colors. This is multiplex stain that\nuses these different proteins that are staining these\nat different colors. We use very standard set\nof staining techniques, with HG, EB1 and new N and GFAP. So if you go to the next image, this is also kind of\nillustrates the second point, 'cause you can make an\nargument, and initially, when we saw the previous image, we said, \"Oh, like are the threads just floating? Like what is happening here? Like are we actually\nlooking at the right thing?\" So what we did is we did another stain, and this is all done in-house, of this batons, trichrome stain, which is in blue that shows\nthese collagen layers. So the blue basically, like you don't want the blue\naround the implant threads, 'cause that means that there's some sort of scarring that's happen. And what you're seeing, if you\nlook at individual threads, is that you don't see any of the blue, which means that there\nhas been absolutely, or very, very minimal to a\npoint where it's not detectable amount of trauma in\nthese inserted threads. - So that presumably is\none of the big benefits of having this kind of flexible thread. - Yeah, so we think this is\nprimarily due to the size, as well as the flexibility of the threads. Also the fact that R1\nis avoiding vasculature, so we're not disrupting or we're not causing damage to the vessels and not breaking any of\nthe blood brain barrier has basically caused the\nimmune response to be muted. - But this is also a nice illustration of the size of things. So this is the tip of the thread. - Yeah, those are neurons. - And they're neurons. And this is the thread listening. And the electrodes are positioned how? - Yeah, so this is, what you're looking at is\nnot electrode themselves. Those are the conductive wires. So each of those should\nprobably be two micron in width. So what we're looking at is we're looking at the coronal slice. So we're looking at some\nslice of the tissue. So as you go deeper, you'll obviously have less and less of the\ntapering of the thread. But yeah, the point basically being that there's just kind of\ncells around the inserter site, which is just an incredible thing to see. I've just never seen anything like this. - How easy and safe is\nit to remove the implant? - Yeah, so it depends on when. In the first three months\nor so after the surgery, there's a lot of kind of tissue\nmodeling that's happening. Similar to when you got a cut, you obviously start\nover first couple weeks, or depending on the size of the wound, scar tissue forming, right? There are these like\ncontractive, and then in the end, they turn into scab and\nyou can scab it off. The same thing happens in the brain, and it's a very dynamic environment. And before the scar\ntissue or the neomembrane or the neomembrane that forms, it's quite easy to just pull 'em out. And there's minimal trauma\nthat's caused during that. Once the scar tissue forms,\nand with Noland as well, we believe that that's the thing that's currently anchoring the thread. So we haven't seen any\nmore movements since then. So they're quite stable. It gets harder to actually\ncompletely extract the threads. So our current method\nfor removing the device is cutting the thread,\nleaving the tissue intact, and then unscrewing and\ntaking the implant out. And that hole is now gonna be plugged with either another Neuralink or just with kind of a\npeak-based, plastic-based cap. - Is it okay to leave the\nthreads in there forever? - Yeah, we think so. We've done studies where\nwe left them there, and one of the biggest\nconcerns that we had is like, do they migrate and do they get to a point where they should not be? We haven't seen that. Again, once the scar tissue forms, they get anchored in place. And I should also say\nthat when we say upgrades, like we're not just\ntalking in theory here. Like we've actually\nupgraded many, many times. Most of our monkeys or non-human primates, NHP have been upgraded. Pager, who you saw playing mind pong, has the latest version of\ndevice since two years ago and is seemingly very\nhappy and healthy and fat. - So what's designed for the\nfuture, the upgrade procedure? So maybe for Noland. What would the upgrade look like? It was essentially what you're mentioning. Is there a way to upgrade\nsort of the device internally, where you take it apart and\nsort of keep the capsule and upgrade the internals? - Yeah, so there are a\ncouple different things here. So for Noland, if we were to upgrade, what we would have to do\nis either cut the threads or extract the threads depending on kind of the situation there in terms of how they're\nanchored or scarred in. If you were to remove them\nwith the dural substitute, you have an intact brain\nso you can reinsert different threads with the\nupdated implant package. There are a couple different other ways that we're thinking about, the future of what the\nupgradable system looks like. One is, at the moment, we\ncurrently remove the dura, this kind of thick layer\nthat protects the brain, but that actually is the thing that actually proliferates\nthe scar tissue formation. So typically, general good rule of thumb is you wanna leave the nature as is and not disrupt it as much. So looking at ways to insert\nthe threads through the dura, which comes with different\nset of challenges, such as it's a pretty thick layer, so how do you actually penetrate that without breaking the needle? So we're looking at different\nneedle design for that, as well as the kind of\nthe loop engagement. The other biggest challenges\nare it's quite opaque, optically, and with\nwhite light illumination. So how do you avoid still\nthis biggest advantage that we have of avoiding vasculature? How do you image through that? How do you actually still mediate that? So there are other imaging techniques that we're looking at to enable that. But our hypothesis is that, and based on some of the\nearly evidence that we have, doing through the dura insertion\nwill cause minimal scarring that causes them to be much\neasier to extract over time. And the other thing that\nwe're also looking at, this is gonna be a fundamental change in the implant architecture, is at the moment, it's a\nmonolithic single implant that comes with a thread\nthat's bonded together. So you can't actually\nseparate the thing out, but you can imagine\nhaving two part implant. Bottom part, that is the\nthread that are inserted that has the chips and maybe\na radio and some power source. And then you have another implant that has more of the\ncomputational heavy load and the bigger battery. And then one can be under the dura, one can be above the dura, like being the plug for the skull. They can talk to each other, but the thing that you wanna upgrade, the computer and not the thread. If you wanna upgrade that,\nyou just go in there, remove the screws and then\nput in the next version. It's a very, very easy surgery too. Like you do a skin incision,\nslip this in, screw. Probably be able to do this in 10 minutes. - So that would allow you to\nreuse the thread, sort of. - [DJ] Correct. - So I mean, this leads to\nthe natural question of, what is the pathway to scaling that increase in the number of threads? Is that a priority? What's the technical challenge there? - Yeah, that is a priority. So for next versions of the implant, the key metrics that\nwe're looking to improve are number of channels, just recording from more and more neurons. We have a pathway to actually\ngo from currently 1,000 to hopefully 3,000 if not\n6,000 by end of this year. And then end of next year, we wanna get to even more, 16,000. - Wow. - There's a couple limitations to that. One is obviously being able to photo lithographically\nprint those wires. As I mentioned, it's two\nmicron in width and spacing. Obviously, there are chips\nthat are much more advanced than those types of resolution, and we have some of the tools that we have brought in\nhouse to be able to do that. So traces will be narrower\njust so that you have to have more of the wires coming up into the chip. Chips also cannot linearly\nconsume more energy, as you have more and more channels. So there's a lot of\ninnovations in the circuit and architecture as well as\nthe circuit design topology to make them lower power. You need to also think about, if you have all of these spikes, how do you send that off\nto the end application? So you need to think about\nbandwidth limitation there and potentially innovations\nand signal processing. Physically, one of the biggest challenge is gonna be the interface. It's always the interface that breaks. Bonding this thin film\narray to the electronics. It starts to become very, very\nhighly dense interconnects. So how do you characterize that? There's a lot of innovations in kind of the 3D integrations\nin the recent years that we can take advantage of. One of the biggest\nchallenges that we do have is forming this hermetic barrier, right? That this is an extremely\nharsh environment that we're in - the brain. So how do you protect it from, yeah, like the brain trying\nto kill your electronics to also your electronics leaking things that you don't want into the brain. And that forming that hermetic barrier is gonna be a very, very big challenge that we I think are actually\nwell-suited to tackle. - How do you test that? Like what's the development environment to simulate that kind of harshness? - Yeah, so this is where\nthe accelerated life tester essentially is a brain in a vat. It literally is a vessel\nthat is made up of, and again, for all intents and purpose for this particular type of test, your brain is a salt water. And you can also put some\nother set of chemicals like reactive oxygen species that get at kind of these interfaces and trying to cause a\nreaction to pull it apart. But you could also increase the rate at which these interfaces are aging by just increasing temperature. So every 10 degrees\nCelsius that you increase, you're basically accelerating time by 2x. And there's limit as to how much temperature you wanna\nincrease, 'cause at some point, there's some other non-linear dynamics that causes you to have\nother nasty gases to form that just is not realistic\nin an environment. So what we do is we\nincrease in our ALT chamber by 20 degrees Celsius that increases the aging by four times. So essentially one day in ALT chamber, it's four day in calendar year. And we look at whether the\nimplants still are intact, including the threads and- - And operation and all of that? - And operation and all of that. It obviously is not an exact\nsame environment as a brain, 'cause brain has mechanical, other more biological\ngroups that attack at it. But it is a good test environment, testing environment for\nat least the enclosure and the strength of the enclosure. And I mean, we've had implants, the current version of the implant that has been in there for, I mean, close to two and a half years, which is equivalent to a decade. And they seem to be fine. - So it's interesting that the burn, so basically, close\napproximation is warm salt water, hot salt water is a good\ntesting environment. Yeah, by the way, I'm drinking LMNT, which is basically salt water,\nwhich is making me kinda, it doesn't have computational\npower the way the brain does, but maybe in terms of\nother characteristics, it's quite similar and I'm consuming it. - Yeah, you have to get it\nin the right pH too. (laughs) - And then consciousness will emerge. Yeah, no. - By the way, the other thing\nthat also is interesting about our enclosure is, if\nyou look at our implant, it's not your common-looking\nmedical implant that usually is encased in a titanium can that's laser welded. We use this polymer called PCTFE, polychlorotrifluoroethylene, which is actually commonly\nused in blister packs. So when you have a pill\nand you try to pop a pill, there's like kind of\nthat plastic membrane. That's what this is. No one's actually ever\nused this except us. And the reason we wanted to do this is 'cause it's\nelectromagnetically transparent. So when we talked about\nthe electromagnetic inductive charging, with titanium can, usually, if you wanna\ndo something like that, you have to have a sapphire window, and it's a very, very\ntough process to scale. - So you're doing a lot of iteration here in every aspect of this. The materials, the software- - The whole, whole shebang. - So, okay. So you mentioned scaling. Is it possible to have\nmultiple Neuralink devices as one of the ways of scaling? To have multiple Neuralink\ndevices implanted? - That's the goal, that's the goal. Yeah, we've had, I mean, our monkeys have had two Neuralinks, one in each hemisphere. And then we're also looking\nat potential of having one in motor cortex, one in visual cortex, and one in wherever other cortex. - So focusing on a particular function, one Neuralink device.\n- Correct. - I mean, I wonder if there's\nsome level of customization that can be done on the compute side. So for the motor cortex- - Absolutely. That's the goal. And we talk about at Neuralink\nbuilding a generalized neural interface to the brain. And that also is strategically how we're approaching this with marketing. And also, with regulatory, which is, hey look, we have the robot, and the robot can access\nany part of the cortex. Right now, we're focused on motor cortex with current version of the N1 that's specialized for\nmotor decoding tasks. But also, at the end of the day, there's kind of a general\ncompute available there. But typically, if you\nwanna really get down to kind of hyperoptimizing\nfor power and efficiency, you do need to get to some\nspecialized function, right? But what we're saying is that, hey, you are now used to this\nrobotic insertion techniques, which took many, many\nyears of showing data and conversation with the FDA. And also, internally convincing\nourselves that this is safe. And now the difference is that if we go to other parts of\nthe brain, like visual cortex, which we're interested\nin as our second product, obviously, it's a completely\ndifferent environment. The cortex is laid out\nvery, very differently. It's gonna be more stimulation focus rather than recording, just kind of creating visual percepts. But in the end, we're using the same thin film array technology. We're using the same robot\ninsertion technology. We're using the same packaging technology. Now, more the conversation\nis focused around what are the differences\nand what are the implication of those differences\nin safety and efficacy? - The way you said second product is both hilarious and awesome to me. That product being restoring\nsight for blind people. So can you speak to\nstimulating the visual cortex? I mean, the possibilities\nthere are just incredible to be able to give that\ngift back to people who don't have sight or\neven any aspect of that. Can you just speak to the challenges of, there's several challenges here. - Oh, many.\n- One of which is, like you said, from\nrecording to stimulation. Just any aspect of that that you're both excited\nand see the challenges of. - Yeah, I guess I'll start by saying that we actually have been capable of stimulating through our thin film array as well as other electronics for years. We have actually demonstrated\nsome of that capabilities for reanimating the\nlimb in the spinal cord. Obviously, for the current EFS study, we've hardware disabled that, so that's something\nthat we wanted to embark as a separate, separate journey. And obviously, there are\nmany, many different ways to write information into the brain. The way in which we're doing\nthat is through electrical, passing electrical current,\nand kind of causing that to really change the local environment so that you can sort of artificially cause kind of the neurons to\ndepolarize in nearby areas. For vision specifically, the\nway our visual system works, it's both well-understood. I mean, anything with kind of brain, there are aspects of it\nthat's well-understood. But in the end, like we\ndon't really know anything. But the way visual system works is that you have photon hitting your eye, and in your eyes, there\nare these specialized cells called photoreceptor cells that convert the photon energy\ninto electrical signals. That then gets projected\nto your back of your head, your visual cortex. It goes through actually thalamic system called LGN that then projects it out. And then in the visual cortex, there's visual area one or V1, and then there's bunch\nof other higher level processing layers like V2, V3. And there there are actually\nkind of interesting parallels. And when you study the behaviors of these convolutional neural networks, like what the different layers\nof the network is detecting, first, they're detecting like these edges, and they're then detecting\nsome more natural curves, and then they start to\ndetect like objects, right? Kind of similar thing\nhappens in the brain. And a lot of that has been inspired, and also, it's been kinda exciting to see some of the correlations there. But things like from there,\nwhere those cognition arise and where's color encoded, there's just not a lot of understanding, fundamental understanding there. So in terms of kind of bringing sight back to those that are blind, there are many different\nforms of blindness. There's actually million people, one million people in the\nU.S. that are legally blind. That means like certain, like score below in kind of the visual tests. I think it's something like, if you can see something\nat 20 feet distance, that normal people can\nsee at 200 feet distance, like if you're worse than\nthat, you're legally blind. - So that means you can't\nfunction effectively. - Correct.\n- Using sight in the world. - Yeah, like to navigate your environment. And yeah, there are\ndifferent forms of blindness. There are forms of blindness\nwhere there's some degeneration of your retina, these photoreceptor cells, and rest of your visual processing that I described is intact. And for those types of individuals, you may not need to maybe stick electrodes into the visual cortex. You can actually build\nretinal prosthetic devices that actually just replaces the function of that retinal cells\nthat are degenerated. And there are many companies\nthat are working on that. But that's a very small slice. Albeit significance, those smaller slice of folks that are legally blind. If there's any damage\nalong that circuitry, whether it's in the optic nerve or just the LGN circuitry or any break in that circuit, that's not gonna work for you. And the source of where\nyou need to actually cause that visual percept to happen, because your biological\nmechanism not doing that is by placing electrodes\nin the visual cortex in the back of your head. And the way in which this would work is that you would have an external camera, whether it's something as\nunsophisticated as a GoPro or some sort of wearable\nRayBan type glasses that Meta's working on that\ncaptures a scene, right? And that scene is then converted to set of electrical impulses\nor stimulation pulses that you would activate\nin your visual cortex through these thin film arrays. And by playing in a\nconcerted kind of orchestra of these stimulation patterns, you can create what's called phosphenes, which are these kind\nof white yellowish dots that you can also create\nby just pressing your eyes. You can actually create those percepts by stimulating in the visual cortex. And the name of the game is\nreally have many of those and have those percepts be, the phosphenes be as small as possible so that you can start to tell apart, like they're the individual\npixels of the screen, right? So if you have many, many of those, potentially, you'll be able to, in the long term, be able to actually get naturalistic vision. But in the like short\nterm to maybe midterm, being able to at least be able to have object detection algorithms\nrun on your glasses, the pre-op processing units, and then being able to at\nleast see the edges of things so you don't bump into stuff. - It's incredible. This is really incredible. So you basically would be adding pixels, and your brain would start to figure out what those pixels mean. Yeah, and like with\ndifferent kinds of assistant on the signal processing on all fronts. - Yeah. The thing that actually, so a couple things. One is, obviously, if\nyou're blind from birth, the way brain works,\nespecially in the early age, neuroplasticity is really nothing other than kind of your brain and different parts of your brain fighting for the limited territory. - [Lex] (laughs) Yeah. - And I mean, very, very quickly, you see cases where you\nknow people that are, I mean, you also hear\nabout people who are blind that have heightened sense of\nhearing or some other senses. And the reason for that is that cortex that's not used just gets taken over by these different parts of the cortex. So for those types of individuals, I mean, I guess they're\ngoing to have to now map some other parts of their senses\ninto what they call vision. But it's gonna be obviously a very, very different\nconscious experience before. So I think that's a interesting caveat. The other thing that also\nis important to highlight is that we're currently\nlimited by our biology in terms of the wavelength\nthat we can see. There's a very, very small wavelength that is a visible light wavelength that we can see with our eyes. But when you have an external\ncamera with this BCI system, you're not limited to that. You can have infrared, you can have UV, you can have whatever other\nspectrum that you want to see. And whether that gets matched to some sort of weird conscious\nexperience, I've no idea. But oftentimes, I talk to people about the goal of Neuralink being going beyond the\nlimits of our biology. That's sort of what I mean. - And if you're able to\ncontrol the kind of raw signal, when we use our sight,\nwe're getting the photons and there's not much processing on it. If you're being able\nto control that signal, maybe you can do some kind of processing. Maybe you do object\ndetection ahead of time. - [DJ] Yeah. - You're doing some\nkind of pre-processing, and there's a lot of\npossibilities to explore that. So it's not just increasing\nsort of thermal imaging, that kind of stuff, but it's\nalso just doing some kind of interesting processing.\n- Correct, yeah. I mean, my theory of how\nlike visual system works also is that, I mean, there's\njust so many things happening in the world, and\nthere's a lot of photons that are going into your\neye, and it's unclear exactly where some of the pre-processing\nsteps are happening. But I mean, I actually think that just from a fundamental perspective,\nthere's just so much, the reality that we're\nin, if it's a reality, is so there's so much data. And I think humans are\njust unable to actually like eat enough actually to\nprocess all that information. So there's some sort of\nfiltering that does happen, whether that happens in the retina, whether that happens in different layers of the visual cortex. Unclear. But like the analogy that\nI sometimes think about is, if your brain is a CCD camera, and all of the information\nin the world is a sun, and when you try to\nactually look at the sun with the CCD camera, it's just gonna saturate\nthe sensors, right? 'Cause it's enormous amount of energy. So what you do is you end up\nadding these filters, right? To just kind of narrow the information that's coming to you and being captured. And I think things like our experiences or our like drugs like propofol, that like anesthetic drug or psychedelics, what they're doing is\nthey're kind of swapping out these filters and putting in\nnew ones or removing older ones and kind of controlling\nour conscious experience. - Yeah, man, not to\ndistract from the topic, but I just took a very\nhigh dose of ayahuasca in the Amazon jungle. So yes, it's a nice way to think about it. You're swapping out different experiences, and with Neuralink being\nable to control that, primarily at first, to improve function, not for entertainment purposes\nor enjoyment purposes, but- - Yeah, giving back lost functions. - Well, giving back lost functions. And there, especially when the\nfunction is completely lost, anything is a huge help. Would you implant a Neuralink\ndevice in your own brain? - Absolutely. I mean, maybe not right\nnow, but absolutely. - What kind of capability, once reached, you would start getting real curious and almost get a little antsy, like jealous of people that get it as you watch them get implanted? - Yeah, I mean I think, I mean, even with our early participants, if they start to do\nthings that I can't do, which I think is in the\nrealm of possibility for them to be able to get, 15, 20, if not like 100 BPS right? There's nothing that\nfundamentally stops us from being able to achieve\nthat type of performance. I mean, I would certainly get\njealous that they can do that. - I should say that watching Noland, I get a little jealous 'cause\nhe's having so much fun, and it seems like such a\nchill way to play video games. - Yeah. So I mean, the thing that also is hard to appreciate sometimes is that, he's doing these things while talking. I mean, it's multitasking, right? So it's clearly, it's obviously cognitively intensive, but similar to how, when we talk, we move our hands, like these\nthings like are multitasking. I mean, he's able to do that. And you won't be able to do that with other assistive\ntechnology as far as I'm aware. If you're obviously using\nlike an eye tracking device, you're very much fixated on that thing that you're trying to do. And if you're using voice control, I mean, like if you say some other stuff, yeah, you don't get to use that. - Yeah, the multitasking aspect of that is really interesting. So it's not just the BPS\nfor the primary task. It's the parallelization\nof multiple tasks. If you measure the BPS for the entirety of the human organism, so if you're talking and\ndoing a thing with your mind and looking around also, I mean, there's just a lot of paralyzation\nthat can be happening. - Yeah. But I mean, I think at some point for him, like if he wants to really\nachieve those high level BPS, it does require like\nfull attention, right? And that's a separate circuitry\nthat is a big mystery. Like how attention works and, you know? - Yeah, attention, like cognitive load, I've read a lot of literature\non people doing two tasks. Like you have your primary\ntask and a secondary task. And the secondary task is\na source of distraction. And how does that affect the performance of the primary task? And depending on the tasks,\nthere's a lot of interesting, I mean, this is an interesting\ncomputational device, right? And I think-\n- To say the least. - A lot of novel insights that can be gained from everything. I mean, I personally am surprised that Noland's able to do\nsuch incredible control of the cursor while talking and also being nervous at the same time, 'cause he's talking like all of us are, if you're talking in front of the camera, you get nervous. So all of those are coming into play, and he is able to still\nachieve high performance. Surprising. I mean, all of this is really amazing. And I think just after\nresearching this really in depth, I kind of want Neuralink. - (laughs) Get in line. - And also, the safety get in line. Well, we should say the registry is for people who have quadriplegia and all that kind of stuff so-\n- Correct. - There'll be a separate line for people. They're just curious, like myself. So now that Noland, patient P1, is part of the ongoing prime study, what's the high level\nvision for P2, P3, P4, P5? And just the expansion\ninto other human beings that are getting to\nexperience this implant? - Yeah, I mean, the primary goal is, for our study in the first place is to achieve safety endpoints. Just understand safety of this device, as well as the implantation process. And also, at the same time,\nunderstand the efficacy and the impact that it could have on the potential users' lives. And just because you have, you're living with tetraplegia, it doesn't mean your situation is same as another person\nliving with tetraplegia. It's wildly, wildly varying. It's something that we're\nhoping to also understand how our technology can serve not just a very small\nslice of those individuals, but broader group of individuals and being able to get the\nfeedback to just really build just the best product for them. There's obviously also goals that we have, and the primary purpose of\nthe early feasibility study is to learn from each\nand every participant to improve the device, improve the surgery before we embark on what's\ncalled a pivotal study that then is much larger trial that starts to look at\nstatistical significance of your endpoints, and that's required before you can then market the device. And that's how it works in the U.S. and just generally around the world. That's the process you follow. So our goal is to really just understand from people like Noland,\nP2, P3, future participants, what aspects of our\ndevice needs to improve. If it turns out that people are like, \"I really don't like the fact\nthat it lasts only six hours. I wanna be able to use this\ncomputer for like 24 hours.\" I mean, that is a user\nneeds and user requirements, which we can only find out from just being able to engage with them. - So before the pivotal study, there's kind of like a rapid innovation based on individual experiences. You're learning from individual\npeople how they use it, like the high resolution details in terms of like cursor control and signal and all that kind of stuff\nto like life experience. - Yeah, yeah, so there's hardware changes but also just firmware updates. So even when we had that sort\nof recovery event for Noland, he now has the new firmware\nthat he has been updated with. And it's similar to how like your phones get updated all the time with new firmware for security patches, whatever new functionality UI, right? And that's something that is\npossible with our implant. It's not a static one-time device that can only do the thing\nthat it said it can do. I mean, it's similar to Tesla. You can do over the air firmware updates, and now you have completely\nnew user interface. And all this bells and\nwhistles and improvements on everything like the latest, right? When we say generalized platform, that's what we're talking about. - Yeah, it's really cool how\nthe app that Noland is using, there's like calibration,\nall that kind of stuff. And then there's update. You just click and get an update. What other future capabilities\nare you kinda looking to? You said vision. That's a fascinating one. What about sort of\naccelerated typing or speech, this kind of stuff? And what else is there? - Yeah, those are still in\nthe realm of movement program. So largely speaking, we have two programs. We have the movement program and we have the vision program. The movement program\ncurrently is focused around the digital freedom. As you can easily guess,\nif you can control 2D cursor in the digital space, you could move anything\nin the physical space. So robotic arms, wheelchair,\nyour environment, or even really like, whether\nit's through the phone or just like directly to those interfaces, so like to those machines. So we're looking at ways to kind of expand those types of capability,\neven for Noland. That requires conversation with the FDA and kind of showing safety data for, if there's a robotic arm or a wheelchair that we can guarantee that\nthey're not gonna hurt themselves accidentally, right? It's very different if you're moving stuff in the digital domain versus\nlike in the physical space, you can actually potentially\ncause harm to the participants. So we're working through that right now. Speech does involve\ndifferent areas of the brain. Speech prosthetic is\nvery, very fascinating, and there's actually been a\nlot of really amazing work that's been happening in academia. Sergei Stavisky at UC\nDavis, Jaimie Henderson, and late Krishna Shenoy at Stanford doing just some incredible amount of work in improving speech neuroprosthetics. And those are actually looking more at parts of the motor cortex that are controlling\nthese focal articulators. And being able to like, even by mouthing the\nword or imagine speech, you can pick up those signals. The more sophisticated higher\nlevel processing areas, like the Broca's area or Wernicke's area, those are still very, very big mystery in terms of the underlying mechanism of how all that stuff works. But yeah, I mean, I think\nNeuralink's event goal is to kind of understand those things and be able to provide\na platform and tools to be able to understand\nthat and study that. - This is where I get to\nthe pothead questions. Do you think we can start getting insight into things like thought? So speech is, there's a muscular\ncomponent, like you said. There's like the act of producing sounds. But then what about the\ninternal things like cognition? Like low level thoughts\nand high level thoughts. Do you think we'll start noticing kind of signals that could be picked up? They could be understood,\nthey could be maybe used in order to interact\nwith the outside world. - In some ways, like I guess this starts to kind of get into the heart\nproblem of consciousness. And I mean, on one hand, all of these are, at some point, set of electrical signals, that from there, maybe it in itself is giving you the\ncognition or the meaning, or somehow, human mind\nis incredibly amazing storytelling machine. So we're telling ourselves\nand fooling ourselves that there's some\ninteresting meaning here. But I mean, I certainly think that BCI and really BCI at the end of the day is a set of tools that\nhelp you kind of study the underlying mechanisms\nin both like local but also broader sense. And whether there's some\ninteresting patterns of like electrical signal, that means like you're\nthinking this versus, and you can either like learn from like many, many sets of\ndata to correlate some of that and be able to do mind reading or not. I'm not sure. I certainly would not\nkind of rule that out as a possibility, but I think BCI alone probably can't do that. There's probably additional\nset of tools and framework. And also, like just heart\nproblem of consciousness at the end of the day is rooted in this philosophical question of like, what's the meaning of it all? What's the nature of our existence? Where's the mind emerge\nfrom this complex network? - Yeah, how does the\nsubjective experience emerge from just a bunch of\nspikes, electrical spikes? - Yeah, yeah, I mean, we\ndo really think about BCI and what we're building as a tool for understanding the mind, the brain. The only question that matters. There's actually, there actually is some\nbiological existence proof of like what it would take\nto kind of start to form some of these experiences\nthat may be unique. If you actually look at\nevery one of our brains, there are two hemispheres. There's a left-sided brain,\nthere's a right-sided brain. And I mean, unless you\nhave some other conditions, you normally don't feel like\nleft legs or right legs. Like you just feel like one legs, right? So what is happening there, right? If you actually look\nat the two hemispheres, there's a structure that\nkind of characterize the two called the corpus callosum\nthat is supposed to have around 200 to 300 million\nconnections or axons. So whether that means that's\nthe number of interface and electrodes that we need to create some sort of\nmind meld, or from that, like whatever new conscious experience that you can experience. But yeah, I do think that there's like kind of an interesting existence\nproof that we all have. - And that threshold is\nunknown at this time. - Oh yeah, these things, everything in this domain\nis speculation, right? - And then there would be, you'd be continuously\npleasantly surprised. Do you see a world where\nthere's millions of people, like tens of millions,\nhundreds of millions of people walking around with a Neuralink device, or multiple Neuralink\ndevices in their brain? - I do. First of all, there are, like if you look at\nworldwide, people suffering from movement disorders\nand visual deficits. I mean, that's in the tens if not hundreds of millions of people. So that alone, I think,\nthere's a lot of benefit and potential good that we can do with this type of technology. And once you start to\nget into kind of neuro, like psychiatric application, depression, anxiety, hunger, or obesity, right? Like mood, control of appetite, I mean, that starts to\nbecome very real to everyone. - Not to mention that most people on earth have a smartphone. And once BCI starts\ncompeting with a smartphone as a preferred methodology of interacting with the digital world, that also becomes an interesting thing. - Oh yeah, I mean, yeah. This is even before going to that, right? I mean, there's like almost, I mean, the entire world that could benefit from\nthese types of thing. And then, yeah, like\nif we're talking about kind of next generation\nof how we interface with machines or even ourselves, in many ways, I think BCI\ncan play a role in that. And some of the things\nthat I also talk about is I do think that there\nis a real possibility that you could see eight billion people walking around with Neuralink. - Well, thank you so\nmuch for pushing ahead. And I look forward to\nthat exciting feature. - Thanks for having me. - Thanks for listening to\nthis conversation with DJ Seo. And now, dear friends,\nhere's Matthew MacDougall, the head neurosurgeon at Neuralink. When did you first become\nfascinated with the human brain? - Since forever. As far back as I can remember, I've been interested in the human brain. I mean, I was a thoughtful kid and a bit of an outsider. And you sit there thinking about what the most important\nthings in the world are in your little tiny adolescent brain. And the answer that I came to, that I converged on was\nthat all of the things you can possibly conceive of,\nas things that are important for human beings to care about are literally contained in the skull. Both the perception of them\nand their relative values. And the solutions to all our problems and all of our problems are\nall contained in the skull. And if we knew more about how that worked, how the brain encodes information and generates desires and\ngenerates agony and suffering, we could do more about it. You think about all the\nreally great triumphs in human history. You think about all the\nreally horrific tragedies. You think about the holocaust, you think about any prison\nfull of human stories, and all of those problems\nboil down to neurochemistry. So if you get a little\nbit of control over that, you provide people the\noption to do better. And in the way I read history, the way people have dealt\nwith having better tools is that they most often\nin the end do better, with huge asterisk. But I think it's an interesting, a worthy and noble pursuit to give people more options, more tools. - Yeah, that's a fascinating\nway to look at human history. You just imagine all these\nneurobiological mechanisms, Stalin, Hitler, all of these, Gengis Khan, all of them just had like a brain, just a bunch of neurons, like a few tons of billions of neurons gaining a bunch of information\nover a period of time. They have a set of module that does language and\nmemory and all that. And from there, in the\ncase of those people, they're able to murder millions of people. - [Matthew] Yeah. - All that coming from, there's not some glorified\nnotion of a dictator of this enormous mind\nor something like this. It's just the brain. - Yeah, yeah. I mean, a lot of that\nhas to do with how well people like that can\norganize those around them. - Other brains. - Yeah, and so I always\nfind it interesting to look to primatology, look to our closest non-human relatives for clues as to how\nhumans are going to behave and what particular humans\nare able to achieve. And so you look at chimpanzees and bonobos and they're similar but different in their social structures particularly. And I went to Emory in Atlanta and studied under Frans,\nthe great Frans de Waal, who was kind of the leading\nprimatologist who recently died, and his work at looking\nat chimps through the lens of how you would watch\nan episode of \"Friends\" and understand the motivations of the characters\ninteracting with each other. He would look at a chimp colony and basically apply that lens. I'm massively oversimplifying it. If you do that, instead of just saying, subject 473 through his\nfeces at subject 471, you talk about them in terms\nof their human struggles, accord them the dignity\nof themselves as actors with understandable goals and drives, what they want out of life. And primarily, it's the\nthings we want out of life: food, sex, companionship, power. You can understand chimp\nand bonobo behavior in the same lights much more easily. And I think doing so gives\nyou the tools you need to reduce human behavior from\nthe kind of false complexity that we layer onto it with language and look at it in terms of, oh, well, these humans are\nlooking for companionship, sex, food, power. And I think that's a pretty powerful tool to have in understanding human behavior. - And I just went to the\nAmazon jungle for a few weeks, and it's a very visceral reminder that a lot of life on earth\nis just trying to get laid. They're all screaming at each other. Like I saw a lot of monkeys, and they're just trying\nto impress each other, or maybe if there's a battle for power, but a lot of the battle for power has to do with them getting laid. - Right. Breeding rights often\ngo with alpha status. And so if you can get a piece of that, then you're gonna do okay. - And would like to think that we're somehow\nfundamentally different, but especially when it comes to primates, we're really aren't, you know. We can use fancier poetic language, but maybe some of the underlying drives that motivate us are similar. - Yeah, I think that's true. - And all of that is coming\nfrom this, the brain. - Yeah.\n- So when did you first start studying the brain as I guess as a biological mechanism? - Basically, the moment I got to college, I started looking around for labs that I could do neuroscience work in. I originally approached\nthat from the angle of looking at interactions\nbetween the brain and the immune system, which\nisn't the most obvious place to start, but I had this idea at the time that the contents of your\nthoughts would have an impact, a direct impact, maybe a powerful one on non-conscious systems in your body. The systems we think of as homeostatic, automatic mechanisms like\nfighting off a virus, like repairing a wound. And sure enough, there are big\ncrossovers between the two. I mean, it gets to kind of a key point that I think goes under recognized. One of the things people don't recognize or appreciate about\nthe human brain enough, and that is that it basically controls or has a huge role in almost\neverything that your body does. Like you try to name an example of something in your body\nthat isn't directly controlled or massively influenced by the\nbrain, and it's pretty hard. I mean, you might say like\nbone healing or something, but even those systems, the\nhypothalamus and pituitary end up playing a role in\ncoordinating the endocrine system that does have a direct influence on, say, the calcium level in your blood\nthat goes to bone healing. So non-obvious connections\nbetween those things implicate the brain as really a potent prime mover in all of health. - One of the things I realized\nin the other direction too, how most of the systems in the body integrated with the human brain, like they affect the brain\nalso, like the immune system. I think there's just people\nwho study Alzheimer's and those kinds of things. It's just surprising how\nmuch you can understand of that from the immune\nsystem, from the other systems that don't obviously seem\nto have anything to do with sort of the nervous system. They all play together. - Yeah, you could\nunderstand how that would be driven by evolution too,\njust in some simple examples. If you get sick, if you\nget a communicable disease, you get the flu, it's pretty advantageous for your immune system to tell your brain, \"Hey, now be antisocial for a few days. Don't go be the life of the party tonight. In fact, maybe just\ncuddle up somewhere warm under a blanket and just\nstay there for a day or two.\" And sure enough, that\ntends to be the behavior that you see both in\nanimals and in humans. If you get sick, elevated\nlevels of interleukins in your blood and TNF alpha in your blood ask the brain to cut\nback on social activity. And even moving around, you have lower locomotor activity in animals that are infected with viruses. - So from there, the early days\nin neuroscience to surgery, when did that step happen? - Yeah.\n- This is a leap. - It was sort of an evolution of thought. I wanted to study the brain. I started studying the brain in undergrad in this neuroimmunology lab. I, from there, realized at some point that I didn't wanna\njust generate knowledge. I wanted to effect real\nchanges in the actual world, in actual people's lives. And so after having not\nreally thought about going into medical school, I was on a track to go into a PhD program. I said, \"Well, I'd like that option. I'd like to actually potentially help tangible people in front of me.\" And doing a little digging\nfound that there exists these MD-PhD programs where\nyou can choose not to choose between them and do both. And so I went to USC for medical school and had a joint PhD program with Caltech, where I actually chose\nthat program particularly because of a researcher at\nCaltech named Richard Andersen, who's one of the godfathers\nof primate neuroscience and has a MACAC lab where Utah arrays and other electrodes were being inserted into the brains of monkeys\nto try to understand how intentions were being\nencoded in the brain. So I ended up there with the idea that maybe I would be a neurologist and study the brain on the side, and then discovered that neurology, again, I'm gonna make\nenemies by saying this, but neurology predominantly and distressingly to me is the practice of diagnosing a thing and then saying, \"Good luck with that. There's not much we can do.\" And neurosurgery, very differently, it's a powerful lever on taking people that are headed in a bad direction\nand changing their course in the sense of brain tumors that are potentially treatable\nor curable with surgery. Even aneurysms in the brain, blood vessels that are gonna rupture, you can save lives really is at the end of the\nday, what mattered to me. And so I was at USC, as I mentioned, that happens to be one of the\ngreat neurosurgery programs. And so I met these truly\nepic neurosurgeons, Alex Khalessi and Mike\nApuzzo and Steve Giannotta and Marty Weiss, these sort of epic people that were just human\nbeings in front of me. And so it kind of changed my thinking from neurosurgeons are distant gods that live on another planet and occasionally come and visit us to these are humans that\nhave problems and are people. And there's nothing\nfundamentally preventing me from being one of them. And so at the last\nminute in medical school, I changed gears from going\ninto a different specialty and switched into neurosurgery,\nwhich cost me a year. I had to do another year of research, because I was so far along in the process to switch into neurosurgery. The deadlines had already passed. So it was a decision that cost time, but absolutely worth it. - What was the hardest\npart of the training on the neurosurgeon track? - Yeah, two things. I think that residency in neurosurgery is sort of a competition of pain, of like how much pain\ncan you eat and smile. - Yeah.\n- And so there's workout restrictions that are not really, they're viewed at, I think, internally among the residents as weakness. And so most neurosurgery residents try to work as hard as they can. And that I think necessarily\nmeans working long hours, and sometimes, over the work hour limits. And we care about being compliant with whatever regulations\nare in front of us. But I think more important than that, people wanna give their all in becoming a better neurosurgeon, because the stakes are so high. And so it's a real fight to get residents to say go home at the end of their shift and not stay and do more surgery. - Are you seriously saying\nlike one of the hardest things is literally like\nforcing them to get sleep and rest and all this kind of stuff? - Historically, that was the case. I think the next generation, I think the next generation\nis more compliant and more selfcare-\n- Weaker is what you mean. All right, I'm just\nkidding, I'm just kidding. - I didn't say it. - Now I'm making enemies. Okay, I get it. Wow, that's fascinating. So what was the second thing? - The personalities. And maybe the two are connected, but- - Was it pretty competitive? - It's competitive and it's also, as we touched on earlier,\nprimates like power, and I think neurosurgery\nhas long had this aura of mystique and excellence\nand whatever about it. And so it's an invitation\nI think for people that are cloaked in that authority. A board certified\nneurosurgeon is basically a walking fallacious\nappeal to authority, right? You have license to walk into any room and act like you're an expert on whatever. And fighting that tendency is not something that most\nneurosurgeons do well. Humility isn't the forte. - Yeah, so I have friends who know you, and whenever they speak about you, that you have the surprising quality for a neurosurgeon of humility, which I think indicates\nthat it's not as common as perhaps in other professions, 'cause there is a kind of gigantic sort of heroic aspect to neurosurgery, and I think it gets to\npeople's head a little bit. - Yeah. Well, I think that allows me to play well at an Elon company. Because Elon, one of his strengths, I think, is to just instantly see through fallacy from authority. So nobody walks into a\nroom that he's in and says, \"Well, goddamn it, you have to trust me. I'm the guy that built the\nlast 10 rockets or something.\" And he says, \"Well, you did it wrong and we can do it better.\" Or, \"I'm the guy that kept Ford alive for the last 50 years. You listen to me on how to build cars.\" And he says no. And so you don't walk\ninto a room that he's in and say, \"Well, I'm a neurosurgeon. Let me tell you how to do it.\" He's gonna say, \"Well, I'm a\nhuman being that has a brain. I can think from first principles myself, thank you very much. And here's how I think\nit ought to be done. Let's go try it and see who's right.\" And that's proven I think\nover and over in his case to be a very powerful approach. - If we just take that tangent, there's a fascinating\ninterdisciplinary team at Neuralink that you get to interact\nwith, including Elon. What do you think is the\nsecret to a successful team? What have you learned from just getting to observe these folks? World experts in different\ndisciplines work together. - Yeah, there's a sweet\nspot where people disagree and forcefully speak their mind and passionately defend their position, and yet are still able to\naccept information from others and change their ideas when they're wrong. And so I like the analogy of sort of how you polish rocks. You put hard things in a\nhard container and spin it. People bash against each other and outcome's a more refined product. And so to make a good team at Neuralink, we've tried to find\npeople that are not afraid to defend their ideas passionately. And occasionally, strongly\ndisagree with people that they're working with and have the best idea come out on top. It's not an easy balance, again, to refer back to the primate brain. It's not something that is inherently built into the primate brain to say, \"I passionately put all\nmy chips on this position and now I'm just gonna walk away from it. Admit you were right.\" Part of our brains tell us\nthat that is a power loss. That is a loss of face, a loss of standing in the community. And now, you're a zeta chimp, 'cause your idea got trounced. And you just have to recognize\nthat that little voice in the back of your head is maladaptive and it's not helping the team win. - Yeah, you have to have the confidence to be able to walk away from\nan idea that you hold onto. Yeah.\n- Yeah. - And if you do that often enough, you're actually going to become the best in the world at your thing. I mean, that kind of that rapid iteration. - Yeah, you'll at least be\na member of a winning team. - Ride the wave. What did you learn? You mentioned there's a lot of\namazing neurosurgeons at USC. What lessons about surgery and life have you learned from those folks? - Yeah, I think working\nyour ass off, working hard while functioning as a member of a team, getting a job done that\nis incredibly difficult, working incredibly long\nhours, being up all night, taking care of someone that you think probably won't\nsurvive no matter what you do. Working hard to make people\nthat you passionately dislike look good the next morning. These folks were\nrelentless in their pursuit of excellent neurosurgical\ntechnique decade over decade. And I think we're well-recognized\nfor that excellence. Especially Marty Weiss,\nSteve Giannotta, Mike Apuzzo, they made huge contributions\nnot only to surgical technique, but they built training\nprograms that trained dozens or hundreds of\namazing neurosurgeons. I was just lucky to kind\nof be in their wake. - What's that like, you\nmentioned doing a surgery where the person is likely not to survive. Does that wear on you? - Yeah. It's especially challenging when you, with all respect to our elders, it doesn't hit so much when you're taking care of an 80-year-old and something was going to\nget them pretty soon anyway. And so you lose a patient like that, and it was part of the natural course of what is expected of them in\nthe coming years, regardless. Taking care of a father of\ntwo or three, four young kids, someone in their 30s that\ndidn't have it coming, and they show up in your ER having their first seizure of their life, and lo and behold, they've\ngot a huge, malignant, inoperable or incurable brain tumor. You can only do that, I\nthink, a handful of times before it really starts\neating away at your armor. Or a young mother that shows up that has a giant hemorrhage in her brain that she's not gonna survive from. And they bring her four-year-old daughter to say goodbye one last time before they turn the ventilator off. The great Henry Marsh is\na English neurosurgeon who said it best. I think he says that every neurosurgeon carries with them a private graveyard, and I definitely feel that, especially with young parents. That kills me. They had a lot more to give. The loss of those people specifically has a knock on effect that's\ngoing to make the world worse for people for a long time. And it's just hard to feel\npowerless in the face of that. And that's where I think you\nhave to be borderline evil to fight against a company like Neuralink or to constantly be taking pot shots at us because what we're doing is\nto try to fix that stuff. We're trying to give people options, to reduce suffering. We're trying to take the pain out of life that broken brains brings in. And yeah, this is just our little way that we're fighting back\nagainst entropy, I guess. - Yeah, the amount of\nsuffering that's endured when some of the things\nthat we take for granted that our brain is able to\ndo is taken away is immense. And to be able to restore some of that functionality is a real gift. - Yeah, we're just starting. We're gonna do so much more. - Well, can you take me\nthrough the full procedure of implanting, say, the\nN1 chip in Neuralink? - Yeah, it's a really simple, really simple, straightforward procedure. The human part of the surgery\nthat I do is dead simple. It's one of the most basic neurosurgery procedures imaginable. And I think there's evidence\nthat some version of it has been done for thousands of years. That there are examples I\nthink from ancient Egypt of healed or partially healed\ntrephinations from Peru or ancient times in South America where these protosurgeons would drill holes in people's skulls, presumably to let out the evil spirits, but maybe to drain blood clots. And there's evidence of bone\nhealing around the edge, meaning the people at least survive some months after a procedure. And so what we're doing is that. We are making a cut in the\nskin on the top of the head over the area of the brain\nthat is the most potent representation of hand intentions. And so if you are an\nexpert concert pianist, this part of your brain is lighting up the entire time you're playing. We call it the hand knob. - The hand knob.\n- Yeah. - So it's all like the finger movement. All of that is just firing away. - Yep, there's a little squiggle\nin the cortex right there. One of the folds in the brain is kind of doubly folded\nright on that spot. So you can look at it on an MRI and say, \"That's the hand knob.\" And then you do a functional test and a special kind of MRI called\nan a functional MRI, fMRI. And this part of the brain\nlights up when people, even quadriplegic people whose brains aren't connected to their\nfinger movements anymore. They imagine finger movements, and this part of the\nbrain still lights up. So we can ID that part of the brain in anyone who's preparing\nto enter our trial and say, \"Okay, that part of the brain, we confirm, is your hand intention area.\" And so I'll make a little cut in the skin, we'll flap the skin open, just like kind of opening\nthe hood of a car, only a lot smaller. Make a perfectly round one inch\ndiameter hole in the skull, remove that bit of skull, open the lining of the brain,\nthe covering of the brain. It's like a little bag of\nwater that the brain floats in. And then show that part\nof the brain to our robot. And then this is where the robot shines. It can come in and take these tiny, much smaller than human hair electrodes and precisely insert them into the cortex, into the surface of the brain\nto a very precise depth, in a very precise spot that\navoids all the blood vessels that are coating the surface of the brain. And after the robot's done with its part, then the human comes back\nin and puts the implant into that hole in the\nskull and covers it up, screwing it down to the skull and sewing the skin back together. So the whole thing is a few hours long. It's extremely low risk compared\nto the average neurosurgery involving the brain that might say, open up a deep part of the brain or manipulate blood vessels in the brain. This opening on the surface of the brain, with only cortical micro insertions, carries significantly less risk than a lot of the tumor\nor aneurysm surgeries that are routinely done. - So cortical micro\ninsertions that are via robot and computer vision are designed\nto avoid the blood vessels. - Exactly. - So I know you're a bit biased here, but let's compare human and machine. - Sure.\n- So what are human surgeons able to do well, and what are robot\nsurgeons able to do well at this stage of our human\ncivilization development? - Yeah, yeah, that's a good question. Humans are general purpose machines. We're able to adapt to unusual situations. We're able to change the plan on the fly. I remember well a surgery that\nI was doing many years ago down in San Diego, where the\nplan was to open a small hole behind the ear and go\nreposition a blood vessel that had come to lay on the facial nerve, the trigeminal nerve, the\nnerve that goes to the face. When that blood vessel lays on the nerve, it can cause just intolerable,\nhorrific shooting pain that people describe like being\nzapped with a cattle prod. And so the beautiful, elegant surgery is to go move this blood\nvessel off the nerve. The surgery team, we went in there and started moving this blood vessel and then found that there\nwas a giant aneurysm on that blood vessel that\nwas not easily visible on the pre-op scans. And so the plan had to dynamically change, and the human surgeons\nhad no problem with that. We're trained for all those things. Robots wouldn't do so\nwell in that situation, at least in their current incarnation, fully robotic surgery, like the electrode insertion portion of the Neuralink surgery. It goes according to a set plan. And so the humans can interrupt the flow and change the plan, but the robot can't really change the\nplan midway through. It operates according\nto how it was programmed and how it was asked to run. It does its job very precisely, but not with a wide degree of latitude and how to react to changing conditions. - So there could be just a\nvery large number of ways that you could be surprised as a surgeon when you enter a situation\nthat could be subtle things that you have to dynamically adjust to. - Correct.\n- And robots are not good at that. - Currently.\n- Currently. I think we are at the\ndawn of a new era with AI of the parameters for robot responsiveness to be dramatically broadened, right? I mean, you can't look\nat a self-driving car and say that it's operating\nunder very narrow parameters. If a chicken runs across the road, it wasn't necessarily\nprogrammed to deal with that, specifically, but a Waymo\nor a self-driving Tesla would have no problem reacting\nto that appropriately. And so surgical robots aren't\nthere yet, but give it time. - And then there could be a lot of, sort of into like\nsemi-autonomous possibilities of maybe a robotic surgeon could say, this situation is perfectly familiar, or the situation is not familiar. And in the not familiar case,\na human could take over. But basically like be very\nconservative in saying, \"Okay, this for sure has\nno issues, no surprises, and let the humans deal\nwith the surprises, with the edge cases, all that.\" That's one possibility. So you think eventually,\nyou'll be out of the job? Well, you being neurosurgeon, your job being neurosurgeon. Humans, there will not\nbe many neurosurgeons left on this earth. - I'm not worried about my job in the course of my professional life. I think I would tell my\nmy kids not necessarily to go in this line of work depending on how things look in 20 years. - It's so fascinating, 'cause I mean, if I have a line of work, I\nwould say it's programming. And if you ask me like for the\nlast, I don't know, 20 years, what I would recommend for people, I would tell 'em, yeah, go. You will always have a job\nif you're a programmer, 'cause there's more and more computers and all this kind of\nstuff and it pays well. But then you realize these\nlarge language models come along and they're really\ndamn good at generating code. So overnight, you could be surprised like, \"Wow, like what is the\ncontribution of the human really?\" But then you start to think, \"Okay, it does seem like\nhumans have ability, like you said, to deal\nwith novel situations.\" In the case of programming,\nit's the ability to kinda come up with novel\nideas to solve problems. It seems like machines aren't\nquite yet able to do that. And when the stakes are very high, when it's life critical,\nas it is in surgery, especially in neurosurgery, the stakes are very high for a robot to actually replace a human. But it's fascinating that\nin this case of Neuralink, there's a human-robot collaboration. - Yeah, yeah. I do the parts it can't do, and it does the parts I can't do. And we are friends. (Lex laughing) - I saw that there's a\nlot of practice going on. So I mean, everything in Neuralink is tested extremely rigorously. But one of the things I\nsaw, that there's a proxy on which the surgeries are performed. - Yeah.\n- So this is both for the robot and for the human, for everybody involved\nin the entire pipeline. What's that like practicing the surgery? - It's pretty intense. So there's no analog to\nthis in human surgery. Human surgery is sort\nof this artisanal craft that's handed down directly from master to pupil over the generations. I mean, literally the way you learn to be a surgeon on humans is\nby doing surgery on humans. I mean, first, you watch your professors do a bunch of surgery, and then finally, they\nput the trivial parts of the surgery into your hands, and then the more complex parts. And as your understanding of the point and the purposes of the surgery increases, you get more responsibility\nin the perfect condition. Doesn't always go well. In Neuralink's case, the\napproach is a bit different. We, of course, practiced as\nfar as we could on animals. We did hundreds of animal surgeries. And when it came time\nto do the first human, we had just amazing\nteam of engineers build incredibly lifelike models. One of the engineers, Fran\nRomano, in particular built a pulsating brain in a\ncustom 3D printed skull that matches exactly\nthe patient's anatomy, including their face and\nscalp characteristics. And so when I was able to practice that, I mean, it's as close as it\nreally reasonably should get to being the real thing\nand all the details, including the having a mannequin body attached to this custom head. And so when we were doing\nthe practice surgeries, we'd wheel that body into the CT scanner and take a mock CT scan\nand wheel it back in and conduct all the normal\nsafety checks verbally. \"Stop, this patient, we're\nconfirming his identification, is mannequin number blah, blah, blah.\" And then opening the brain\nin exactly the right spot using standard operative\nneuronavigation equipment, standard surgical drills in the same OR that we do all of our practice\nsurgeries in at Neuralink. And having the skull open\nand have the brain pulse, which adds a degree of\ndifficulty for the robot to perfectly precisely plan\nand insert those electrodes to the right depth and location. And so yeah, we kind of broke new ground on how extensively we\npracticed for this surgery. - So there was a historic moment, a big milestone for Neuralink in part for humanity with the first human getting a Neuralink implant\nin January of this year. Take me through the surgery on Noland. What did it feel like to be part of this? - Yeah. Well, we're lucky to have\njust incredible partners at the Barrow Neurologic Institute. They are, I think, the premier neurosurgical hospital in the world. They made everything as easy as possible for the trial to get going\nand helped us immensely with their expertise on\nhow to arrange the details. It was a much more high\npressure surgery in some ways. I mean, even though the\noutcome wasn't particularly in question in terms of\nour participant safety, the number of observers,\nthe number of people, there's conference rooms full of people watching live streams in the hospital, rooting for this to go perfectly, and that just adds pressure\nthat is not typical for even the most intense\nproduction neurosurgery. Say, removing a tumor or placing deep brain stimulation electrodes. And it had never been\ndone on a human before. There were unknown unknowns. And so definitely, a\nmoderate pucker factor there for the whole team, not knowing if we were going to encounter, say, a degree of brain\nmovement that was unanticipated or a degree of brain\nsag that took the brain far away from the skull and\nmade it difficult to insert or some other unknown unknown problem. Fortunately, everything went well and that surgery is one of the smoothest outcomes we could have imagined. - Were you nervous? I mean, you're a bit quarterback in the Super Bowl kind of situation. - Extremely nervous. Extremely. I was very pleased when it went well and when it was over. Looking forward to number two.\n- Yeah. Even with all that practice, all of that, you've never been in a situation that's still high stakes in\nterms of people watching. And we should also probably mention, given how the media\nworks, a lot of people, maybe in a dark kind of way,\nhoping it doesn't go well. - Well, I think wealth is easy to hate or envy or whatever. And I think there's a whole\nindustry around driving clicks, and bad news is great for clicks. And so any way to take an\nevent and turn it into bad news is gonna be really good for clicks. - It just sucks because I think\nit puts pressure on people. It discourages people from trying to solve really hard problems, because\nto solve hard problems, you have to go into the unknown. You have to do things that\nhaven't been done before, and you have to take risks. - Yeah.\n- Calculated risks. You have to do all kinds\nof safety precautions, but risks nevertheless. And I just wish there would\nbe more celebration of that, of the risk taking versus\nlike people just waiting on the sidelines, like\nwaiting for failure, and then pointing out the failure. Yeah, it sucks. But in this case, it's\nreally great that everything went just flawlessly, but it's unnecessary\npressure, I would say. - Now that there's a human\nwith literal skin in the game, there's a participant whose wellbeing rides on this doing well, you have to be a pretty bad person to be rooting for that to go wrong. And so hopefully, people\nlook in the mirror and realize that at some point. - So did you get to\nactually front row seat like watch the robot work? You get to see the whole thing? - Yeah, I mean, because an\nMD needs to be in charge of all of the medical decision making throughout the process, I\nunscrubbed from the surgery after exposing the brain and\npresenting it to the robot and placed the targets on the robot software interface that tells the robot where it's going to insert\neach thread that was done with my hand on the mouse,\nfor whatever that's worth. - So you were the one placing the targets? - Yeah.\n- Oh, cool. So like the robot with a computer vision provides a bunch of candidates and you kinda finalize the decision. - Right. The software engineers\nare amazing on this team. And so they actually provided an interface where you can essentially use a lasso tool and select a prime area\nof brain real estate, and it will automatically\navoid the blood vessels in that region and automatically\nplace a bunch of targets. So that allows the human robot operator to select really good areas of brain and make dense applications\nof targets in those regions, the regions we think are gonna have the most high fidelity representations of finger movements and\narm movement intentions. - I've seen like images of this. And for me, with OCD, it's for some reason a really pleasant, I think there's a subreddit\ncalled oddly satisfying. - [Matthew] Yeah, love that subreddit. - It's oddly satisfying to\nsee the different target sites avoiding the blood vessels\nand also maximizing like the usefulness of those\nlocations for the signal. It just feels good. It's like, ah. - As a person who has a visceral reaction to the brain bleeding, I can tell you it's extremely satisfying watching the electrodes\nthemselves go into the brain and not cause bleeding. - Yeah, yeah. So you said the feeling was of relief when everything went perfectly. - Yeah. - How deep in the brain\ncan you currently go and eventually go, let's\nsay, on the Neuralink side. It seems the deeper you go in the brain, the more challenging it becomes. - Yeah, so talking broadly\nabout neurosurgery, we can get anywhere. It's routine for me to put deep\nbrain stimulating electrodes near the very bottom of the brain, entering from the top and passing about a two millimeter wire all the way into the bottom of the brain. And that's not revolutionary. A lot of people do that. And we can do that with\nvery high precision. I use a robot from\nGlobus to do that surgery several times a month. It's pretty routine. - What are your eyes in that situation? What are you seeing? What kind of technology\ncan you use to visualize where you are to light your way? - Yeah, so it's a cool\nprocess on the software side. You take a preoperative MRI that's extremely high resolution\ndata of the entire brain. You put the patient to sleep, put their head in a frame that holds the skull very rigidly, and then you take a CT scan of their head while they're asleep with that frame on, and then merge the MRI\nand the CT in software. You have a plan based on the MRI where you can see these\nnuclei deep in the brain. You can't see them on CT, but if you trust the\nmerging of the two images, then you indirectly know\non the CT where that is. And therefore, indirectly know where in reference to the titanium frame screwed to their head those targets are. And so this is '60s\ntechnology to manually compute trajectories, given the\nentry point and target and dial in some goofy\nlooking titanium actuators with a manual actuators with\nlittle tick marks on them. The modern version of\nthat is to use a robot. Just like a little KUKA arm, you might see it building\ncars at the Tesla factory. This small robot arm can\nshow you the trajectory that you intended from the pre-op MRI and establish a very rigid holder through which you can drill\na small hole in the skull and pass a small rigid wire\ndeep into that area of the brain that's hollow and put your electrode through that hollow wire and then remove all of\nthat except the electrode. So you end up with the electrode very, very precisely placed\nfar from the skull surface. Now that's standard\ntechnology that's already, been out in the world for a while. Neuralink right now is focused\nentirely on cortical targets, surface targets because\nthere's no trivial way to get, say, hundreds of wires\ndeep inside the brain without doing a lot of damage. So your question, what do you see? Well, I see an MRI on a screen. I can't see everything\nthat that DBS electrode is passing through on its\nway to that deep target. And so it's accepted with this approach that there's gonna be about\nwhat one in a hundred patients who have a bleed somewhere in the brain as a result of passing that wire blindly into the deep part of the brain. That's not an acceptable\nsafety profile for Neuralink. We start from the position that we want this to be dramatically maybe two or three orders of\nmagnitude safer than that. Safe enough really that you or I, without a profound medical problem, might on our lunch break someday say, \"Yeah, sure, I'll get that. I'd be meaning to upgrade\nto the latest version.\" And so the safety constraints\ngiven that are high. And so we haven't settled\non a final solution for arbitrarily approaching\ndeep targets in the brain. - It's interesting 'cause\nlike you have to avoid blood vessels somehow. Maybe there's creative ways\nof doing the same thing, like mapping out high resolution\ngeometry of blood vessels and then you can go in blind. But how do you map out that in a way that's like super stable? There's a lot of interesting\nchallenges there, right? - [Matthew] Yeah. - But there's a lot to do on the surface. Luckily.\n- Exactly. So we've got vision on the surface. We actually have made a\nhuge amount of progress sewing electrodes into the spinal cord as a potential workaround\nfor a spinal cord injury that would allow a brain-mounted implant to translate motor intentions\nto a spine-mounted implant that can effect muscle contractions in previously paralyzed arms and legs. - That's mind-blowing. That's just incredible. So like the effort there\nis to try to bridge the brain to the spinal cord to the peripheral nervous system. So how hard is that to do? - We have that working in\nvery crude forms in animals. - That's amazing.\n- Yeah, we've done- - So similar to like with Noland, where he's able to\ndigitally move the cursor, here you're doing the\nsame kind of communication but with the actual factors that you have. - Yeah. - [Lex] That's fascinating. - Yeah, so we have anesthetized\nanimals doing grasp and moving their legs in\na sort of walking pattern. Again, early days, but the future is bright\nfor this kind of thing. And people with paralysis should look forward to that bright future. They're gonna have options. - Yeah, and there's a lot\nof sort of intermediate or extra options where you\ntake like an Optimus robot, like the arm, and to be\nable to control the arm. The fingers and hands of\nthe arm as a prosthetic. - So skeletons are getting better too. - So skeletons. Yeah, so that goes hand in hand. Although I didn't quite understand until thinking about it deeply and doing more research about Neuralink, how much you can do on the digital side. So this digital telepathy, I didn't quite understand that you can really map the intention, as you described in the hand knob area, that you can map the intention. Just imagine it, think about it. That intention can be mapped to actual action in the digital world. And now more and more, so much can be done in the digital world\nthat it can reconnect you to the outside world. It can allow you to have freedom, have independence if\nyou're a quadriplegic. That's really powerful. Like you can go really far with that. - Yeah, our first\nparticipant, he's incredible. He's breaking world\nrecords left and right. - And he is having fun\nwith it, it's great. Just going back to the\nsurgery, your whole journey, you mentioned to me offline,\nyou have surgery on Monday. So you're like doing surgery all the time. - Yeah. - Maybe the ridiculous question, what does it take to get good at surgery? - Practice, repetitions. Same with anything else. There's a million ways of\npeople saying the same thing and selling books saying it, but you call it 10,000\nhours, you call it, you know, spend some chunk of your life, some percentage of your\nlife focusing on this, obsessing about getting better at it. Repetitions, humility, recognizing\nthat you aren't perfect at any stage along the way. Recognizing you've got improvements to make in your technique. Being open to feedback and coaching from people with a different\nperspective on how to do it. And then just the constant\nwill to do better. That fortunately, if\nyou're not a sociopath, I think your patients bring that with them to the office visits every day. They force you to wanna\ndo better all the time. - Yeah, to step up. I mean, it's a real human being, a real human being that you can help. - Yeah.\n- So every surgery, even if it's the same exact surgery, is there a lot of variability between that surgery\nand a different person? - Yeah, a fair bit. I mean, a good example for\nus is the angle of the skull relative to the normal\nplane of the body axis, of the skull over hand knob\nis pretty wide variation. I mean, some people\nhave really flat skulls, and some people have really steeply angled skulls over that area. And that has consequences for\nhow their head can be fixed in sort of the frame that we use and how the robot has\nto approach the skull. Yeah, people's bodies\nare built as differently as the people you see\nwalking down the street, as much variability in body\nshape and size as you see there. We see in brain anatomy and skull anatomy, there are some people who\nwe've had to kind of exclude from our trial for having skulls that are too thick or too thin or scalp that's too thick or too thin. I think we have like the\nmiddle 97% or so of people, but you can't account for all\nhuman anatomy variability. - How much like mushiness\nand messes there, 'cause taking biology classes, the diagrams are always\nreally clean and crisp. Neuroscience, the pictures of neurons are always really nice and vary. But whenever I look at\npictures of like real brains, I don't know what is going on. - Yeah.\n- So how much are biological systems in reality? Like how hard is it to\nfigure out what's going on? - Not too bad. Once you really get used to this, that's where experience\nand skill and education really come into play is if\nyou stare at a thousand brains, it becomes easier to kind\nof mentally peel back the, say, for instance, blood vessels that are obscuring the sulci and gyri, kind of the wrinkle pattern\nof the surface of the brain. Occasionally, when you're\nfirst starting to do this and you open the skull, it doesn't match what you thought you were\ngonna see based on the MRI. And with more experience, you\nlearn to kind of peel back that layer of blood vessels and see the underlying pattern\nof wrinkles in the brain and use that as a landmark\nfor where you are. - [Lex] The wrinkles are a landmark? So like-\n- Yeah. So I was describing hand knob earlier. That's a pattern of the\nwrinkles in the brain. It's sort of this sort of Greek letter, omega-shaped area of the brain. - So you could recognize\nthe hand knob area. Like if I show you a thousand brains and give you like one minute with each, you'd be like, \"Yep, that's that?\" - Sure.\n- And so there is some uniqueness to that area of the brain, like in terms of the geometry,\nthe topology of the thing. - Yeah. - Where is it about in the- - So you have this strip of\nbrain running down the top called the primary motor area. And I'm sure you've seen this\npicture of the homunculus laid over the surface of the brain, the weird little guy with\nhuge lips and giant hands. That guy sort of lays with his legs up at the top of the brain and\nface, arm, areas farther down and then some kind of mouth,\nlip, tongue areas farther down. And so the hand is right in there. And then the areas that control speech, at least on the left side of the brain in most people are just below that. And so any muscle that you\nvoluntarily move in your body, the vast majority of that\nreferences that strip or those intentions come\nfrom that strip of brain. And the wrinkle for hand knob is right in the middle of that. - And vision is back here.\n- Back, yep. - Also, close to the surface? - Vision's a little deeper. And so this gets to your question about how deep can you get to do vision. We can't just do the surface of the brain. We have to be able to go in, not as deep as we have to go for DBS, but maybe a centimeter deeper than we're used to for hand insertions. And so that's work in progress. That's a new set of\nchallenges to overcome. - By the way, you\nmentioned the Utah array. And I just saw a picture of that, and that thing looks terrifying. - [Matthew] Yeah, bed of nails. - It's because it's rigid. And then if you look at the\nthreads, they're flexible. What can you say that's interesting to you about the flexible, that kind of approach of the flexible threads to deliver the electrodes\nnext to the neurons? - Yeah, I mean, the goal\nthere comes from experience. I mean, we stand on\nthe shoulders of people that made Utah arrays and\nused Utah arrays for decades before we ever even came along. Neuralink arose, partly this approach to technology arose out\nof a need recognized after Utah arrays would fail routinely because the rigid electrodes, those spikes that are literally hammered using an air hammer into the brain, those spikes generate\na bad immune response that encapsulates the electrode spikes in scar tissue essentially. And so one of the projects\nthat was being worked on in the Andersen lab at\nCaltech when I got there, was to see if you could use chemotherapy to prevent the formation of scar. Things are pretty bad when\nyou're jamming a bed of nails into the brain and then\ntreating that with chemotherapy to try to prevent scar tissue. It's like, maybe we've\ngotten off track here, guys. Maybe there's a fundamental\nredesign necessary. And so Neuralink's approach\nof using highly flexible, tiny electrodes avoids\na lot of the bleeding, avoids a lot of the immune\nresponse that ends up happening when rigid electrodes are\npounded into the brain. And so what we see is\nour electrode longevity and functionality and the\nhealth of the brain tissue immediately surrounding\nthe electrode is excellent. I mean, it goes on for years\nnow in our animal models. - What do most people not understand about the biology of the brain? We mention the vasculature. That's really interesting. - I think the most interesting\nmaybe underappreciated fact is that it really does\ncontrol almost everything. I mean, I don't know, for\nout of the blue example, imagine you want a lever on fertility, you wanna be able to turn\nfertility on and off. I mean, there are legitimate\ntargets in the brain itself to modulate fertility, say blood pressure. You wanna modulate blood pressure. There are legitimate targets\nin the brain for doing that. Things that aren't immediately\nobvious as brain problems are potentially solvable in the brain. And so I think it's an under-explored area for primary treatments of all the things that bother people. - That's a really fascinating\nway to look at it. Like there's a lot of\nconditions we might think have nothing to do with the brain, but they might just be symptoms of something that actually\nstarted in the brain. The actual source of the problem. The primary source is\nsomething in the brain. - Yeah, not always. I mean, kidney disease is real. But there are levers you\ncan pull in the brain that affect all of these systems. - There's knobs.\n- Yeah. - On-off switches and knobs in the brain, from which this all originates. Would you have a Neuralink\nchip implanted in your brain? - Yeah. I think use case right\nnow is use a mouse, right? I can already do that. And so there's no value proposition. On safety grounds alone, sure. I'll do it tomorrow. - You say the use case of the mouse, is it after like researching all this and part of it is just watching\nNoland have so much fun? If you can get that bits per second look really high with a mouse, like being able to interact, 'cause if you think about it, the way, on the smartphone, the way you swipe, that was transformational how\nyou interact with a thing. It's subtle. You don't realize it, but\nto able to touch a phone and to scroll with your\nfinger, that's like, that changed everything. People were sure you\nneed a keyboard to type. There's a lot of HCI aspects to that that changed how we\ninteract with computers. So there could be a certain\nrate of speed with the mouse that would change everything. - Yeah.\n- It's like you might be able to just click around a\nscreen extremely fast. And that, I can see myself getting a Neuralink for much more rapid interaction\nwith the digital devices. - Yeah, I think recording\nspeech intentions from the brain might change things as well. The value proposition\nfor the average person, a keyboard is a pretty\nclunky human interface, requires a lot of training. It's highly variable in\nthe maximum performance that the average person can achieve. I think taking that out of the equation and just having a natural word to computer interface might change things for a lot of people. - It'd be hilarious if that\nis the reason people do it. Even if you have speech to text, that's extremely accurate,\nit currently isn't, but say it gotten super accurate, it'd be hilarious if\npeople went for Neuralink just so you avoid the\nembarrassing aspect of speaking, like looking like a douche bag speaking to your phone in public, which is a real, like that's a real constraint. - Yeah. I mean, with a bone conducting case, that can be an invisible headphone, say, and the ability to think\nwords into software and have it respond to you, that starts to sound sort of like embedded super intelligence. If you can silently ask\nfor the Wikipedia article on any subject and have it read to you, without any observable change happening in the outside world, for one thing, standardized\ntesting is obsolete. (laughs) - Yeah. If it's done well in the\nUX side, it could change. I don't know if it transforms society, but it really can create a kind of shift in the way we interact\nwith digital devices in the way that a smartphone did. Just having to look into the\nsafety of everything involved, I would totally try it\nso it doesn't have to go to some like incredible\nthing where you have, it connects your vision or to some other, like it connects all over your brain. That could be like just\nconnecting to the hand knob. You might have a lot of\ninteresting interaction, human-computer interaction possibilities. That's really interesting. - Yeah, and the technology\non the academic side is progressing at light speed here. I think there was a really\namazing paper out of UC Davis, Sergey Stavisky's lab that\nbasically made a initial solve of speech decode. It was something like 125,000 words that they we're getting with\nvery high accuracy, which is- - So you're just thinking the word? - Yeah.\n- Thinking the word and you're able to get it? - Yeah.\n- Oh boy. Like you have to have the\nintention of speaking it. - Right. - So like do the inner voice. Man, it's so amazing to me\nthat you can do the intention, the signal mapping. All you have to do is just\nimagine yourself doing it. And if you get the feedback\nthat it actually worked, you can get really good at that. Like your brain will first of all adjust and you develop it like any other skill. Like touch typing, you develop\nin that same kind of way. To me, it's just really fascinating to be able to even to play with that. Honestly, like I would get a Neuralink just to be able to play with that. Just to play with the capacity, the capability of my\nmind to learn this skill. It's like learning the skill of typing and learning the skill of moving a mouse. It's another skill of moving the mouse, not with my physical\nbody, but with my mind. - I can't wait to see\nwhat people do with it. I feel like we're cavemen right now. We're like banging rocks with a stick and thinking that we're making music. At some point, when these\nare more widespread, there's gonna be the equivalent of a piano that someone can make art with their brain in a way that we didn't even anticipate. I'm looking forward to it. - Give it to like a teenager. Like anytime I think\nI'm good at something, I'll always go to like, I don't know. Even with the bit per second\nof playing a video game, you realize you give it to a teenager, you've given your link to a teenager, just the large number of them, the kind of stuff, they get good at stuff. They're gonna get like\nhundreds of bits per second. Even just with the current technology. - Probably. Probably. - 'Cause it's also addicting, the number go up aspect of it of like improving and training, 'cause it is almost like a skill. And plus, there's the\nsoftware on the other end that adapts to you. And especially if the\nadapting procedure algorithm becomes better and better and better, you're like learning together. - Yeah, we're scratching the\nsurface on that right now. There's so much more to do. - So on the complete other side of it, you have an RFID chip implanted in you. - Yeah.\n- So I hear, nice. - Little subtle thing. - It's a passive device that you use for unlocking like a\nsafe with top secrets, or what do you use it for? What's the story behind it? - I'm not the first one. There's this whole community\nof weirdo biohackers that have done this stuff, and I think one of the\nearly use cases was storing private crypto wallet keys and whatever. I dabbled in that a bit\nand had some fun with it. - You have some bitcoin\nimplanted in your body somewhere. You can't tell where, yeah. - Yeah, actually, yeah. (Lex laughing) It was the modern day equivalent of finding change in the sofa cushions after I put some orphan crypto on there that I thought was worthless and forgot about it for a few years. Went back and found that some\ncommunity of people loved it and had propped up the value of it. And so it had gone up 50 fold. - Wow.\n- So there was a lot of change in those cushions. (Lex laughing) - That's hilarious. - But the primary use case was mostly as a tech demonstrator. It has my business card on it. You can scan that in by\ntouching it to your phone. It opens the front door to my house, whatever simple stuff. - It's a cool step. It's a cool leap to implant\nsomething in your body. I mean, perhaps, it's a\nsimilar leap to a Neuralink because for a lot of\npeople, that kind of notion of putting something inside\nyour body, something electronic inside a biological system is a big leap. - Yeah, we have a kind\nof a mysticism around the barrier of our skin. We're completely fine\nwith knee replacements, hip replacements, dental implants. But there's a mysticism still around the inviable barrier that\nthe skull represents. And I think that needs to be treated like any other pragmatic barrier. The question isn't, how incredible\nis it to open the skull? The question is, what\nbenefit can we provide? - So from all the surgeries you done, from everything you understand the brain, how much does neuroplasticity\ncome into play? How adaptable is the brain, for example, just even in the case\nof healing from surgery or adapting to the post-surgery situation. - The answer that is sad for me and other people of my demographic is that plasticity decreases with age. Healing decreases with age. I have too much gray hair\nto be optimistic about that. There are theoretical ways\nto increase plasticity using electrical stimulation. Nothing that is totally proven out as a robust enough mechanism\nto offer widely to people. But yeah, I think there's\ncause for optimism that we might find something\nuseful in terms of, say, an implanted electrode\nthat improves learning. Certainly, there's been\nsome really amazing work recently from Nicholas\nSchiff, Jonathan Baker, and others who have a cohort of patients with moderate traumatic brain injury who have had electrodes placed in the deep nucleus in the brain called the centromedian nucleus or just near central media nucleus. And when they apply small\namounts of electricity to that part of the brain, it's almost like electronic caffeine. They're able to improve\npeople's attention and focus. They're able to improve how\nwell people can perform a task. I think in one case, someone\nwho was unable to work after the device was turned on,\nthey were able to get a job. And that's sort of one of the holy grails for me with Neuralink and\nother technologies like this is from a purely utilitarian standpoint, can we make people able\nto take care of themselves and their families economically again? Can we make it so someone\nwho's fully dependent and even maybe requires a\nlot of caregiver resources, can we put them in a position\nto be fully independent, taking care of themselves,\ngiving back to their communities? I think that's a very\ncompelling proposition, and what motivates a lot of what I do and what a lot of the people\nat Neuralink are working for. - It's just a cool possibility that if you put a Neuralink in\nthere, that the brain adapts, like the other part of\nthe brain adapts too. - Yeah.\n- And integrates it. The capacity of the brain to\ndo that is really interesting. Probably unknown to the degree\nto which you can do that, but you're now connecting\nan external thing to it, especially once it's doing stimulation, like the biological brain\nand the electronic brain outside of it working together. Like the possibilities there\nare really interesting. It's still unknown but interesting. It feels like the brain is really good at adapting to whatever. - Yeah.\n- But of course, it is a system that by itself is already, like everything serves a purpose and so you don't wanna\nmess with it too much. - Yeah, it's like, eliminating a species from an ecology. You don't know what the\ndelicate interconnections and dependencies are. The brain is certainly a\ndelicate, complex beast. And we don't know every\npotential downstream consequence of a single change that we make. - Do you see yourself\ndoing, so you mentioned P1, surgeries of P2, P3, P4, P5? Just more and more and more humans. - I think it's a certain\nkind of brittleness or a failure on the company's side if we need me to do all the surgeries. I think something that I would very much like to work towards is a\nprocess that is so simple and so robust on the surgery side that literally anyone could do it. We wanna get away from\nrequiring intense expertise or intense experience to\nhave this successfully done and make it as simple and\ntranslatable as possible. I mean, I would love it if\nevery neurosurgeon on the planet had no problem doing this. I think we're probably far\nfrom a regulatory environment that would allow people\nthat aren't neurosurgeons to do this, but not impossible. - All right, I'll sign up for that. Did you ever anthropomorphize\nthe robot R1? Like do you give it a name? Do you see it as like a friend, as like working together with you? - I mean, to a certain degree it's- - Or anatomy who's gonna get the gap. - To a certain degree, yeah,\nit's complex relationship. - All the good relationships are. - It's funny when, in the\nmiddle of the surgery, there's a part of it where I stand basically shoulder to\nshoulder with the robot. And so if you're in the room\nreading the body language, that's my brother in arms there. We're working together\non the same problem. Yeah, I'm not threatened by it. - Keep telling yourself that. (laughs) How have all the surgeries that\nyou've done over the years, the people you've helped and the stakes, the high stakes that you've mentioned, how has that changed your\nunderstanding of life and death? - Yeah. It gives you a very visceral sense, and this makes sound trite, but it gives you a very visceral sense that death is inevitable. On one hand, you are, as a neurosurgeon, you're deeply involved in these like just hard to fathom tragedies: young parents dying, leaving\na four-year-old behind say. And on the other hand, it takes the sting out of it a bit because you see how just mind\nnumbingly universal death is. There's zero chance that\nI'm going to avoid it. I know techno optimists right now and longevity buffs right now would disagree on that 0.0% estimate. But I don't see any chance that our generation is going to avoid it. Entropy is a powerful force, and we are very ornate,\ndelicate, brittle DNA machines that aren't up to the\ncosmic ray bombardment that we're subjected to. So on the one hand, every human that has ever\nlived died or will die. On the other hand, it's just\none of the hardest things to imagine inflicting on anyone that you love is having them gone. I'm sure you've had friends\nthat aren't living anymore and it's hard to even think about them. And so I wish I had arrived\nat the point of nirvana where death doesn't have a sting. I'm not worried about it,\nbut I can at least say that I'm comfortable\nwith the certainty of it. If not, having found out how\nto take the tragedy out of it when I think about my\nkids either not having me or me not having them or my wife. - Maybe I have come to accepting intellectual certainty of\nit, but it may be the pain that comes with losing\nthe people you love, I don't think I've come to understand the existential aspect of it. Like that this is gonna end. And I don't mean like in some trite way. I mean like, it certainly feels\nlike it's not going to end. Like you live life like\nit's not going to end. - [Matthew] Right. - And the fact that this light that's shining this consciousness is going to no longer be,\nin one moment, maybe today, it fills me when I really am able to load all that in with\nErnest Becker's terror. Like it's a real fear. I think people aren't always honest with how terrifying it is. I think the more you are able to really think through it,\nthe more terrifying it is. It's not such a simple thing. Oh well, it's the way life is. If you really can load that in, it's hard. But I think that's why the stoics did it, because it like helps you\nget your shit together and be like, well, the moment, every single moment you're\nalive is just beautiful. And it's terrifying that it's gonna end, like almost like you're\nshivering in the cold a child helpless, this kind of feeling. And then it makes you,\nwhen you have warmth, when you have the safety, when you have the love\nto really appreciate it. I feel like sometimes, in your position, when you mentioned\narmor, just to see death, it might make you not be able to see that, the finiteness of life, because if you kept looking\nat that, it might break you. So it's good to know that you're kind of still struggling with that, that there's the neurosurgeon\nand then there's a human. And the human is still\nable to struggle with that and feel the fear of that\nand the pain of that. - Yeah, it definitely\nmakes you ask the question of how long, how many\nof these can you see? And not say, \"I can't do this anymore.\" But I mean, you said it well. I think it gives you an\nopportunity to just appreciate that you're alive today. And I've got three kids\nand an amazing wife and I'm really happy. Things are good. I get to help on a project\nthat I think matters. I think it moves us forward. I'm a very lucky person. - It's the early steps of a potentially gigantic leap for humanity. It's a really interesting one. And it's cool 'cause like you, you read about all this stuff in history where it's like the early days. I've been reading, before\ngoing to the Amazon, I would read about explorers\nthat would go and explore even the Amazon jungle for the first time. Those are the early steps. Or early steps into space, early steps in any discipline, in physics and mathematics. And it's cool 'cause this\nis like, on the grand scale, these are the early steps into delving deep into the human brain. So not just observing the brain, but be able to interact\nwith the human brain. It's gonna help a lot of people, but it also might help us understand what the hell's going on in there. - Yeah, I think ultimately,\nwe wanna give people more levers that they can pull, right? Like you wanna give people options. If you can give someone\na dial that they can turn on how happy they are, I think that makes people\nreally uncomfortable. But now, talk about major\ndepressive disorder. Talk about people that\nare committing suicide at an alarming rate in this country. And try to justify that queasiness in that light of you\ncan give people a knob to take away suicidal\nideation, suicidal intention. I would give them that knob. I don't know how you\njustify not doing that. - Yeah, you can think about like all the suffering that's\ngoing on in the world. Like every single human being that's suffering right now, it'll be a glowing red dot. The more suffering, the more it's glowing. And you just see the\nmap of human suffering, and any technology that\nallows you to dim that light of suffering on a grand\nscale is pretty exciting, because there's a lot of people suffering and most of them suffer quietly. We look away too often, and we should remember\nthose that are suffering, 'cause once again, most of\nthem are suffering quietly. - Well, and on a grander\nscale, the fabric of society, people have a lot of complaints about how our social fabric is working or not working, how our politics\nis working or not working. Those things are made\nof neurochemistry too, in aggregate, right? Like our politics is\ncomposed of individuals with human brains and, the\nway it works or doesn't work is potentially tunable in\nthe sense that, I don't know, say remove our addictive behaviors or tune our addictive\nbehaviors for social media or our addiction to outrage,\nour addiction to sharing the most angry political\ntweet we can find. I don't think that leads\nto a functional society. And if you had options for people to moderate\nthat maladaptive behavior, there could be huge benefits to society. Maybe we could all work together a little more harmoniously\ntoward useful ends. - There's a sweet spot,\nlike you mentioned, you don't wanna completely\nremove all the dark sides of human nature 'cause those\nkind of are somehow necessary to make the whole thing work. But there's a sweet spot. - Yeah, I agree. We gotta suffer a little, just not so much that you lose hope. - Yeah. We knew all the surgeries you've done. Have you seen consciousness in there ever? Was there like a glowing light? - I have this sense that I never found it. Never removed it, like a\ndementor in Harry Potter. I have this sense that\nconsciousness is a lot less magical than our instincts wanna claim it is. It seems to me like a useful\nanalog for thinking about what consciousness is in the brain, is that we have a really\ngood intuitive understanding of what it means to, say, touch your skin and know what's being touched. I think consciousness is just that level of sensory mapping applied to the thought processes\nin the brain itself. So what I'm saying is consciousness is the sensation of some part\nof your brain being active. So you feel it working. You feel the part of your brain that thinks of red things\nor winged creatures or the taste of coffee. You feel those parts of\nyour brain being active the way that I'm feeling my\npalm being touched, right? And that sensory system\nthat feels the brain working is consciousness. - That's so brilliant. It's the same way, it's\nthe sensation of touch when you're touching a thing. Consciousness is the sensation of you feeling your brain working,\nyour brain thinking, your brain perceiving. - Which isn't like a\nwarping of space and time or some quantum field effect, right? It's nothing magical. People always wanna\nascribe to consciousness something truly different. And there's this awesome long history of people looking at whatever the latest discovery in physics is\nto explain consciousness, because it's the most magical, the most out there thing\nthat you can think of. And people always wanna do\nthat with consciousness. I don't think that's necessary. It's just a very useful and gratifying way of\nfeeling your brain work. - And as we said, it's\none heck of a brain. - [Matthew] Yeah. - Everything we see around\nus, everything we love, everything that's beautiful, it came from brains like these. - It's all electrical activity\nhappening inside your skull. - And I, for one, am grateful\nthat there's people like you that are exploring all\nthe ways that it works and all the ways it can be made better. - Thanks, Lex. - Thank you so much for talking today. - It's been a joy. Thanks for listening to this conversation with Matthew MacDougall. And now, dear friends,\nhere's Bliss Chapman, Brain Interface Software\nlead at Neuralink. You told me that you've\nmet hundreds of people with spinal cord injuries or with ALS and that your motivation\nfor helping at Neuralink is grounded in wanting to help them. Can you describe this motivation? - Yeah. First, just a thank you to all the people I've gotten a chance to speak with, for sharing their stories with me. I don't think there's any\nworld really in which I can share their stories as\npowerful way as they can. But just I think to summarize\nat a very high level what I hear over and over\nagain is that people with ALS or severe spinal cord injury in a place where they basically can't\nmove physically anymore, really at the end of the day\nare looking for independence. And that can mean different\nthings for different people. For some folks, it can mean\nthe ability just to be able to communicate again\nindependently without needing to wear something on their face,\nwithout needing a caretaker to be able to put\nsomething in their mouth. For some folks, it can mean\nindependence to be able to work again, to be able to\nnavigate a computer digitally, efficiently enough to\nbe able to get a job, to be able to support themself,\nto be able to move out and ultimately be able to support themself after their family maybe\nisn't there anymore to take care of them. And for some folks, it's as simple as just being able to respond\nto their kid in time before they run away or get\ninterested in something else. And these are deeply personal and sort of very human problems. And what strikes me again and again when talking with these folks is that this is actually an engineering problem. This is a problem that\nwith the right resources, with the right team, we can\nmake a lot of progress on. And at the end of the day, I think that's a deeply inspiring message and something that makes me\nexcited to get up every day. - So it's both an engineering problem in terms of a BCI, for example, that can give them capabilities where they can interact with the world. But also on the other side,\nit's an engineering problem for the rest of the world\nto make it more accessible for people living with quadriplegia. - Yeah, and actually,\nI'll take a broad view sort of lens on this for a second. I think I'm very in favor of anyone working in this problem space. So beyond BCI, I'm happy and excited and willing to support any way I can folks working on eye tracking systems, working on speech to text systems, working on head trackers or\nmouse sticks or quad sticks. And I've met many engineers and folks in the community\nthat do exactly those things. And I think for the people\nwe're trying to help, it doesn't matter what the\ncomplexity of the solution is as long as the problem is solved. And I wanna emphasize that there can be many solutions out there that can help with these problems. And BCI is one of a\ncollection of such solutions. So BCI, in particular, I think offers several advantages here. And I think the folks that\nrecognize this immediately are usually the people who\nhave spinal cord injury or some form of paralysis. Usually, you don't have to explain to them why this might be something\nthat could be helpful. It's usually pretty self-evident. But for the rest of us\nfolks that don't live with severe spinal cord injury or who don't know somebody with ALS, it's not often obvious why\nyou would want a brain implant to be able to connect\nand navigate a computer. And it's surprisingly nuanced, and to the degree that\nI've learned a huge amount just working with Noland in the first Neuralink clinical trial\nand understanding from him in his words why this\ndevice is impactful for him. And it's a nuanced topic. It can be the case that\neven if you can achieve the same thing, for\nexample, with a mouse stick when navigating a computer,\nhe doesn't have access to that mouse stick every\nsingle minute of the day. He only has access when\nsomeone's available to put it in front of him. And so a BCI can really offer a level of independence and autonomy that if it wasn't literally\nphysically part of your body, it'd be hard to achieve in any other way. - So there's a lot of fascinating aspects to what it takes to get Noland to be able to control a cursor on\nthe screen with his mind. You texted me something that I just love. You said, \"I was part of the team that interviewed and selected P1. I was in the operating room during the first human surgery monitoring live signals\ncoming out of the brain. I work with the user basically every day to develop new UX paradigm's\ndecoding strategies. And I was part of the\nteam that figured out how to recover useful BCI to new world record levels when the signal quality degraded.\" We'll talk about I think\nevery aspect of that, but just zooming out, what was it like to be part of that team\nand part of that historic, I would say, historic first? - Yeah, I think for me, this is something I've been excited about for close to 10 years now. And so to be able to be\neven just some small part of making it a reality\nis extremely exciting. A couple maybe special moments\nduring that whole process that I'll never really truly forget, one of them is during the actual surgery, at that point in time, I\nknow Noland quite well. I know his family. And so I think the initial\nreaction when Noland is rolled into the operating room is just a \"oh shit\" kind of reaction. But at that point, muscle memory kicks in and you sort of go into, you let your body just do all the talking. And I have the lucky job in\nthat particular procedure to just be in charge of\nmonitoring the implant. So my job is to sit there, to look at the signals\ncoming off the implant, to look at the live brain\ndata streaming off the device as threads are being\ninserted into the brain and just to basically observe and make sure that nothing is going wrong or that there's no red\nflags or fault conditions that we need to go and investigate or pause the surgery to debug. And because I had that\nsort of spectator view of the surgery, I had a\nslightly removed perspective than I think most folks in the room. I got to sit there and think to myself, \"Wow, that brain is moving a lot.\" When you look into the\nside look craniectomy, that we stick the threads in, one thing that most people don't\nrealize is the brain moves. The brain moves a lot when you breathe, when your heart beats, and\nyou can see it visibly. So that's something that I\nthink was a surprise to me and very, very exciting to be\nable to see someone's brain who you physically know and\nhave talked with at length actually pulsing and\nmoving inside their skull. - And they use that brain\nto talk to you previously, and now it's right there moving. - Yep.\n- Actually, I didn't realize that in terms of the thread sending, so the Neuralink implant\nis active during surgery, and one thread at a time, you're able to start seeing the signal? - Yeah.\n- So that's part of the way you test that the thing is working? - Yeah, so actually in the operating room, right after we sort of finished\nall the thread insertions, I started collecting what's\ncalled broadband data. So broadband is basically\nthe most raw form of signal you can collect\nfrom a Neuralink electrode. It's essentially a measurement of the local field potential or the, yeah, the voltage essentially measured by that electrode. And we have a certain\nmode in our application that allows us to visualize\nwhere detected spikes are. So it visualizes sort of where, in the broadband symbol, and it's very, very raw form of the data a neuron is actually spiking. And so one of these moments\nthat I'll never forget as part of this whole clinical trial is seeing live in the operating room, while he's still under anesthesia, beautiful spikes being\nshown in the application, just streaming live to a\ndevice I'm holding in my hand. - So this is no signal\nprocessing the raw data and then the signals\nprocessings on top of it, you're seeing the spikes detected? - [Bliss] Right, yeah. - And that's a UX too because-\n- Yes. - That looks beautiful as well. - During that procedure, there was actually a lot\nof cameramen in the room. So they also were curious\nand wanted to see. There's several neurosurgeons in the room who are all just excited to\nsee robots taking their job and they're all crowded\naround a small little iPhone watching this live brain\ndata stream out of his brain. - What was that like seeing the robot do some of the surgery? So the computer vision\naspect where it detects all the spots that avoid the blood vessels and then obviously with\nthe human supervision, then actually doing the\nreally high precision connection of the threads to the brain. - That's a good question. My answer's gonna be pretty\nlame here, but it was boring. I've seen it so many times. Yeah, that's exactly how\nyou want surgery to be. You want it to be boring, because I've seen it so many times. I've seen the robot do the surgery literally hundreds of times, and so it was just one more time. - Yeah, all the practice\nsurgeries and the proxies and this is just another day. - Yep. - So what about when Noland woke up? Do you remember a moment where he was able to move the\ncursor, not move the cursor, but get signal from the brain such that it was able to show\nthat there's a connection? - Yeah, yeah. So we are quite excited to\nmove as quickly as we can, and Noland was really, really\nexcited to get started. He wanted to get started\nactually the day of surgery, but we waited till the next\nmorning very patiently. So a long night. And the next morning in the\nICU, where he was recovering, he wanted to get started and\nactually start to understand what kind of signal we can\nmeasure from his brain. And maybe for folks who are not familiar with the Neuralink system, we\nimplant the Neuralink system or the Neuralink implant\nin the motor cortex. So the motor cortex is responsible for representing things like motor intent, sort of if you imagine\nclosing and opening your hand, that kind of signal representation would be present in the motor cortex. If you imagine moving\nyour arm back and forth or wiggling a pinky, this sort of signal can be present in the motor cortex. So one of the ways we\nstart to sort of map out, what kind of signal do we\nactually have access to in any particular individual's brain is through this task called body mapping. And body mapping is where\nyou essentially present a visual to the user and you say, \"Hey, imagine doing this.\" And that visual is a 3D\nhand opening and closing, or index finger modulating up and down. And you ask the user to imagine that, and obviously, you can't see them do this, 'cause they're paralyzed\nso you can't see them actually move their arm,\nbut while they do this task, you can record neural activity, and you can basically\noffline model and check, can I predict or can I\ndetect the modulation corresponding with\nthose different actions? And so we did that task and we realized, hey, there's actually some modulation associated with some of his hand motion, which was the first indication that, okay, we can potentially use that modulation to do useful things in the world. For example, control a computer cursor. And he started playing with it, the first time we showed him it, and we actually just\ntook the same live view of his brain activity and\nput it in front of him. And we said, \"Hey, you\ntell us what's going on. We're not you. You're able to imagine different things, and we know that it's\nmodulating some of these neurons so you figure out for us what that is actually representing.\" And so he played with it for a bit. He was like, \"I don't quite get it yet.\" He played for a bit longer. And he said, \"Oh, when I move this finger, I see this particular\nneuron start to fire more.\" And I said, okay, \"Prove it, do it again.\" And so he said, \"Okay,\nthree, two, one, boom.\" And the minute he moved, you\ncan see like instantaneously this neuron is firing - single neuron. I can tell you the exact channel number if you're interested. It's stuck in my brain now forever. But that single channel firing\nwas a beautiful indication that it was behaved really\nmodulated neural activity that could then be used\nfor downstream tasks like decoding a computer cursor. - And when you say single channel, is that associated with\na single electrode? - Yeah, channel electrode\nare interchangeable. - And there's 1,024 of those? - 1,024, yeah. - That's incredible that that works. When I was learning about all\nthis and like loading it in, it was just blowing my\nmind that the intention, you can visualize yourself\nmoving the finger, that can turn into a signal, and the fact that you\ncan then skip that step and visualize the cursor moving or have the intention of the cursor moving and that leading to a signal that can then be used to move the cursor, there is so many exciting things there to learn about the brain,\nabout the way the brain works, the very fact of their existing signal that can be used is really powerful. But it feels like that's\njust like the beginning of figuring out how that\nsignal could be used really, really effectively. I should also just, there's so many fascinating details here, but you mentioned the body mapping step. At least in the version I saw\nthat Noland was showing off, there's like a super nice interface, like a graphical interface. But like it just felt like\nI was like in the future 'cause it like, you know, I guess it visualizes you moving the hand. And there's very like a\nsexy, polished interface. Hello. I don't know if there's a voice component, but it just felt like when you wake up in a really nice video\ngame and this is a tutorial at the beginning of that video game. \"This is what you're supposed to do.\" It's cool. - No, I mean, the future\nshould feel like the future. - But it's not easy to pull that off. I mean, it needs to be\nsimple but not too simple. - Yeah, and I think the\nUX design component here is underrated for BCI\ndevelopment in general. There's a whole interaction effect between the ways in which you visualize an instruction to the user and the kinds of signal you can get back. And that quality of sort of\nyour behavioral alignment to the neural signal is a function of how good you are at expressing to the user what you want them to do. And so yeah, we spend a lot\nof time thinking about the UX, of how we build our applications, of how the decoder actually functions, the control surfaces it\nprovides to the user. All these little details matter a lot. - So maybe it'd be nice to get\ninto a little bit more detail of what the signal looks like and what the decoding looks like. So there's a N1 implant\nthat has, like we mentioned, 1,024 electrodes and that's\ncollecting raw data, raw signal. What does that signal look like, and what are the different\nsteps along the way before it's transmitted,\nand what is transmitted, all that kind of stuff? - Yeah, yep. This is gonna be a fun one. Let's go. So maybe before diving into what we do, it's worth understanding\nwhat we're trying to measure, because that dictates a\nlot of the requirements for the system that we build. And what we're trying to measure is really individual neurons\nproducing action potentials. And action potential\nis, you can think of it like a little electrical impulse that you can detect if\nyou're close enough. And by being close enough,\nI mean, like within let's say 100 microns of that cell. And 100 microns is a\nvery, very tiny distance. And so the number of neurons\nthat you're gonna pick up with any given electrode is just a small radius\naround that electrode. And the other thing worth understanding about the underlying biology here is that when neurons\nproduce an action potential, the width of that action potential\nis about one millisecond. So from the start of the\nspike to the end of the spike, that whole width of that sort\nof characteristic feature of a neuron firing is\none millisecond wide. And if you want to detect\nthat an individual spike is occurring or not, you\nneed to sample that signal or sample the local full\npotential nearby that neuron much more frequently\nthan once a millisecond. You need to sample many,\nmany times per millisecond to be able to detect that this is actually the characteristic waveform of a neuron producing an action potential. And so we sample across\nall 1,024 electrodes about 20,000 times a second. 20,000 times a second means we've already given\none millisecond window. We have about 20 samples that tell us what that exact shape of that\naction potential looks like. And once we've sort of\nsampled at super high rate the underlying electrical\nfield nearby these cells, we can process that signal into just where do we detect a\nspike or where do we not, sort of a binary signal one or zero. Do we detect a spike in\nthis one millisecond or not? And we do that because the\nactual information carrying sort of subspace of neural activity is just when are spikes occurring. Essentially, everything that we care about for decoding can be\ncaptured or represented in the frequency\ncharacteristics of spike trains, meaning how often are spikes firing in any given window of time. And so that allows us to\ndo sort of a crazy amount of compression from this very\nrich, high density signal to something that's much, much more sparse and compressible that can be\nsent out over a wireless radio, like a Bluetooth\ncommunication, for example. - Quick tangents here. You mentioned electrode neuron. There's a local neighborhood\nof neurons nearby. How difficult is it to like isolate from where the spike came from? - Yeah, so there's a whole\nfield of sort of academic neuroscience work on exactly this problem, of basically given a single electrode or given a set of electrodes\nmeasuring a set of neurons, how can you sort of sort, spike sort which spikes are\ncoming from what neuron? And this is a problem that's\npursued in academic work because you care about it for\nunderstanding what's going on in the underlying sort of\nneuroscience of the brain. If you care about understanding how the brain's representing information, how that's evolving through time, then that's a very, very\nimportant question to understand. For sort of the\nengineering side of things, at least at the current scale, if the number of neurons per\nelectrode is relatively small, you can get away with basically ignoring that problem completely. You can think of it like\nsort of a random projection of neurons to electrodes, and there may be in some cases more than one neuron per electrode. But if that number is small enough, those signals can be thought of as sort of a union of the two. And for many applications,\nthat's a totally reasonable trade off to make and can\nsimplify the problem a lot. And as you sort of\nscale out channel count, the relevance of distinguishing\nindividual neurons becomes less important, because\nyou have more overall signal and you can start to rely\non sort of correlations or covariance structure in\nthe data to help understand when that channel's firing, what does that actually represent? 'Cause you know that when\nthat channel's firing in concert with these other 50 channels, that means move left. But when that same channel's\nfiring with concert with these other 10 channels,\nthat means move right. - Okay, so you have to do\nthis kind of spike detection on board, and you have to\ndo that super efficiently, so fast and not use too much power, 'cause you don't wanna be\ngenerating too much heat. So it has to be a super\nsimple signal processing step. - [Bliss] Yeah. - Is there some wisdom you can share about what it takes to overcome that challenge? - Yeah, so we've tried\nmany different versions of basically turning this raw signal into sort of a feature that you\nmight wanna send off the device. And I'll say that I don't think we're at the final step of this process. This is a long journey. We have something that\nworks clearly today, but there can be many\napproaches that we find in the future that are much better than what we do right now. So some versions of what we do right now, and there's a lot of academic\nheritage to these ideas, so I don't wanna claim\nthat these are original Neuralink ideas or anything like that. But one of these ideas\nis basically to build a sort of like a convolutional\nfilter almost, if you will, that slides across the signal and looks for a certain\ntemplate to be matched. And that template consists\nof sort of how deep the spike modulates, how much it recovers, and what the duration\nand window of time is that the whole process takes. And if you can see in the\nsignal that that template is matched within certain bounds, then you can say, \"Okay, that's a spike.\" One reason that approach\nis super convenient is that you can actually implement that extremely efficiently in hardware, which means that you\ncan run it in low power across 1,024 channels all at once. Another approach that we've\nrecently started exploring, and this can be combined with\nthe spike detection approach, something called spike band power. And the benefits of that approach are that you may be able to\npick up some signal from neurons that are maybe too far away\nto be detected as a spike, because the farther away\nyou are from an electrode, the weaker that actual spike waveform will look like on that electrode. So you might be able to pick up population level activity\nof things that are maybe slightly outside the\nnormal recording radius, what neuroscientists\nsometimes refer to as the hash of activity, the other\nstuff that's going on, and you can look at sort\nof across many channels how that sort of background\nnoise is behaving and you might be able to get more juice out of the signal that way. But it comes at a cost. That signal is now a floating\npoint representation, which means it's more expensive\nto send out over a power. It means you have to find\ndifferent ways to compress it that are different than what you can apply to binary signals. So there's a lot of different challenges associated with these\ndifferent modalities. - So also, in terms of communication, you're limited by the\namount of data you can send. - [Bliss] Yeah. - And also, because you're currently using the Bluetooth protocol, you\nhave to batch stuff together. But you have to also do this\nkeeping the latency crazy low. Like crazy low. Anything to say about the latency? - Yeah, this is a passion project of mine, so I wanna build the\nbest mouse in the world. I don't wanna build like the, you know, the Chevrolet Spark or\nwhatever of electric cars. I wanna build like the Tesla\nRoadster version of a mouse. And I really do think it's quite possible that within 5 to 10 years, that most eSports\ncompetitions are dominated by people with paralysis. This is like a very real\npossibility for number of reasons. One is that they'll have\naccess to the best technology to play video games effectively. The second is they have the time to do so. So those two factors together are particularly potent\nfor eSport competitors. - Unless people without paralysis are also allowed to implant. - (laughs) Right. - Which is, it is another way to interact with a digital device. And there's something to that, if it's a fundamentally\ndifferent experience, more efficient experience. Even if it's not like some kinda full on high bandwidth communication, if it's just the ability to\nmove the mouse 10x faster, like the bits per second, if I can achieve a bits per second, that 10x, what I can do with the mouse, that's a really interesting\npossibility of what that can do, especially as you get really\ngood at it with training. - It's definitely the case that you have a higher\nceiling performance, because you don't have\nto buffer your intention through your arm, through your muscle. You get just, by nature of\nhaving a brain implant at all, like 75 millisecond\nlead time on any action that you're actually trying to take. And there's some nuance to this, like there's evidence\nthat the motor cortex, you can sort of plan\nout sequences of action so you may not get that\nwhole benefit all the time. But for sort of like\nreaction time style games where you just wanna, somebody's over here, snipe\n'em, that kind of thing. You actually do have just\nan inherent advantage 'cause you don't need\nto go through muscle. So the question is, just how\nmuch faster can you make it? And we're already than\nyou what you would do if you're going through muscle\nfrom a latency point of view, and we're in the early stage of that. I think we can push it\nsort of our end-to-end latency right now from brain\nspike to cursor movement, it's about 22 milliseconds. If you think about the\nbest mice in the world, the best gaming mice, that's\nabout five milliseconds-ish of latency, depending on how you measure. Depending how fast your screen refreshes, there's a lot of characteristics\nthat matter there. But yeah, and the rough\ntime for like a neuron in the brain to actually impact your command of your hand\nis about 75 millisecond. So if you look at those numbers, you can see that we're\nalready like competitive and slightly faster than what you'd get by actually moving your hand. And this is something that,\nif you ask Noland about it, when he moved the cursor\nfor the first time, we asked him about this. This was something I\nwas super curious about, like what does it feel like\nwhen you're modulating, a click intention or when\nyou're trying to just move the cursor to the right. He said it moves before he is\nlike actually intending it to, which is kind of a surreal\nthing and something that I would love to experience myself one day. What is that like to have that\nthing just be so immediate, so fluid that it feels like it's happening before you're actually\nintending it to move. - Yeah, I suppose we've\ngotten used to that latency, that natural latency that happens. So is the currently the\nbottleneck, the communication, so like the Bluetooth communication, what's the actual bottleneck? I mean, there's always\ngonna be a bottleneck. What's the current bottleneck? - Yeah, a couple things. So kind of hilariously,\nBluetooth low energy protocol has some restrictions on how\nfast you can communicate. So the protocol itself\nestablishes a standard of the most frequent sort\nof updates you can send are on the order of 7.5 milliseconds. And as we push latency down to the level of sort of individual\nspikes impacting control, that level of resolution,\nthat kind of protocol is gonna become a limiting\nfactor at some scale. Another sort of important nuance to this is that it's not just the Neuralink itself that's part of this equation. If you start pushing latency\nsort of below the level of how fast screens refresh,\nthen you have another problem. Like you need your whole system\nto be able to be as reactive as the sort of limits of what\nthe technology can offer. Like you need the screen like 120 hertz just doesn't work anymore if you're trying to have something respond at something that's at the\nlevel of one millisecond. - That's a really cool challenge. I also like that for a T-shirt, the best mouse in the world. Tell me on the receiving\nend, so the decoding step, now we figured out what the spikes are, we got them all together, now we're sending that over to the app. What's the decoding step look like? - Yeah, so maybe first, what is decoding? I think there's probably\na lot of folks listening that just have no clue what it means to decode brain activity. - Actually, even if we\nzoom out beyond that, what is the app? So there's an implant that's\nwirelessly communicating with any digital device\nthat has an app installed. So maybe can you tell me a\nhigh level what the app is, what the software is outside of the brain? - Yeah, so maybe working\nbackwards from the goal, the goal is to help\nsomeone with paralysis, in this case Noland, be able to navigate his\ncomputer independently. And we think the best way to do that is to offer them the\nsame tools that we have to navigate our software\nbecause we don't wanna have to rebuild an entire software\necosystem for the brain. At least not yet. Maybe someday you can imagine there's UXs that are built natively for BCI. But in terms of what's\nuseful for people today, I think most people would prefer\nto be able to just control mouse and keyboard inputs\nto all the applications that they wanna use for their daily jobs, for communicating with\ntheir friends, et cetera. And so the job of the application is really to translate\nthis wireless stream of brain data coming off the implant into control of the computer. And we do that by essentially building a mapping from brain activity\nto sort of the HID inputs to the actual hardware. So HID is just the protocol for communicating like\ninput device events. So for example, move\nmouse to this position or press this key down. And so that mapping is fundamentally what the app is responsible for. But there's a lot of nuance\nof how that mapping works that we spend a lot of\ntime to try to get right and we're still in the early\nstages of a long journey to figure out how to do that optimally. So one part of that process is decoding. So decoding is this process of taking the statistical\npatterns of brain data that's being channeled across\nthis Bluetooth connection to the application and turning it into, for example, a mouse movement. And that decoding step,\nyou can think of it in a couple different parts. So similar to any\nmachine learning problem, there's a training step and\nthere's an inference step. The training step in our\ncase is a very intricate behavioral process where the user has to imagine doing different actions. So for example, they'll\nbe presented a screen with a cursor on it and they'll be asked to push that cursor to the right. Then imagine pushing that\ncursor to the left, push it up, push it down, and we can\nbasically build up a pattern or using any sort of modern ML method, a mapping of given this brain data and that imagined behavior\nmap one to the other. And then at test time, you take that same\npattern matching system. In our case, it's a deep neural network, and you run it and you\ntake the live stream of brain data coming off\ntheir implant, you decode it by pattern matching to what\nyou saw at calibration time, and you use that for a\ncontrol of the computer. Now, a couple like sort of rabbit holes that are I think are quite interesting, one of them has to do with how you build that best template matching system because there's a variety\nof behavioral challenges and also debugging challenges when you're working with\nsomeone who's paralyzed. Because again, fundamentally, you don't observe what\nthey're trying to do. You can't see them attempt\nto move their hand. And so you have to figure out\na way to instruct the user to do something and validate\nthat they're doing it correctly such that then you can\ndownstream, build with confidence the mapping between the neural spikes and the intended action. And by doing the action correctly, what I really mean is at\nthis level of resolution of what neurons are doing. So if in ideal world,\nyou could get a signal of behavioral intent that\nis ground truth accurate at the scale of sort of\none millisecond resolution, then with high confidence,\nI could build a mapping from my neuro spikes to\nthat behavioral intention. But the challenge is, again, that you don't observe what\nthey're actually doing. And so there's a lot of nuance to how you build user experiences that give you more than\njust sort of a course on average correct representation of what the user's intending to do. If you want to build\nthe world's best mouse, you really want it to be\nas responsive as possible. You want it to be able to do exactly what the user's intending\nat every sort of step along the way, not just\non average be correct when you're trying to move\nit from left to right. And building a behavioral\nsort of calibration game or sort of software experience\nthat gives you that level of resolution is what we\nspend a lot of time working. - So the calibration process, the interface has to encourage precision, meaning like whatever it does, it should be super intuitive\nthat the next thing the human is going to likely\ndo is exactly that intention that you need and only that intention. And you don't have any feedback except that may be\nspeaking to you afterwards what they actually did. You can't, \"Oh yeah.\"\n- Right. - So that's fundamentally, that is a really exciting UX challenge, 'cause that's all on the UX. It's not just about being\nfriendly or nice or usable. - Yeah.\n- It's like- - User experience is how it works. - It's how it works.\n- Yeah. - For the calibration, and calibration, at least\nat this stage of Neuralink, is like fundamental to\nthe operation of the thing and not just calibration but continued calibration essentially. - [Bliss] Yeah. - Wow, yeah.\n- You said something that I think is worth\nexploring there a little bit. You said it's primarily a UX challenge, and I think a large component of it is, but there is also a very interesting machine learning challenge here, which is given some data set, including some on average correct behavior of asking the user to\nmove up or move down, move right, move left, and given a data set of neural spikes, is there a way to infer in\nsome kind of semi-supervised or entirely unsupervised way what that high resolution\nversion of their intention is? And if you think about\nit, like there probably is because there are enough\ndata points in the dataset, enough constraints on your\nmodel that there should be a way with the right sort of formulation to let the model figure out itself. For example, at this millisecond, this is exactly how hard\nthey're pushing upwards. And at this millisecond, this is how hard they're trying to push upwards. - It's really important to\nhave very clean labels, yes. So like the problem becomes much harder from the machine learning perspective if the labels are noisy. - [Bliss] That's correct. - And then to get the clean\nlabels, that's a UX challenge. - Correct, although clean labels, I think maybe it's worth\nexploring what that exactly means. I think any given labeling strategy will have some number\nof assumptions it makes about what the user's attempting to do. Those assumptions can be\nformulated in a loss function, or they can be formulated\nin terms of heuristics that you might use to just try to estimate or guesstimate what the\nuser's trying to do. And what really matters is how accurate are those assumptions. For example, you might say, \"Hey, user, push upwards and follow the speed of this cursor,\" and your heuristic might be\nthat they're trying to do it exactly what that cursor's trying to do. Another competing heuristic might be they're actually trying\nto go slightly faster at the beginning of the movement and slightly slower at the end. And those competing heuristics\nmay or may not be accurate reflections of what the\nuser's trying to do. Another version of the task might be, \"Hey, user, imagine moving\nthis cursor a fixed offset. So rather than follow the cursor, just try to move it exactly\n200 pixels to the right.\" So here's the cursor, here's the target. Okay, cursor disappears. Try to move that now invisible cursor 200 pixels to the right. And the assumption in that\ncase would be that the user can actually modulate\ncorrectly that position offset, but that position offset assumption might be a weaker\nassumption, and therefore, potentially you can make it more accurate than these heuristics that\nare trying to guesstimate at each millisecond what\nthe user's trying to do. So you can imagine different tasks that make different\nassumptions about the nature of the user intention and\nthose assumptions being correct is what I would think of as a clean label. - For that step, what are we\nsupposed to be visualizing? There's a cursor and you\nwanna move that cursor to the right or the left or up and down or maybe move them by a certain offset. So that's one way, is that the\nbest way to do calibration? So for example, an alternative crazy way that probably is playing a role here is a game like Webgrid, where you're just getting a\nvery large amount of data, the person playing a game, where if they're in a state of flow, maybe you can get clean\nsignal as a side effect. - [Bliss] Yep. - Is that not an effective\nway for initial calibration? - Yeah, great question. There's a lot to unpack there. So the first thing I\nwould draw a distinction between a sort of open\nloop, first closed loop. So open loop, what I mean by that is the user is sort of\ngoing from zero to one. They have no model at all, and they're trying to get to the place where they have some\nlevel of control at all. In that setup, you really\nneed to have some task that gives the user a hint\nof what you want them to do such that you can build this mapping again from brain data to output. Then once they have a model,\nyou could imagine them using that model and\nactually adapting to it and figuring out the right\nway to use it themself and then retraining on that data to give you sort of a\nboost in performance. There's a lot of challenges associated with both of these techniques\nand we can sort of rabbit hole into both of 'em, if you're interested. But the sort of challenge\nwith the open loop task is that the user themself doesn't get proprioceptive feedback\nabout what they're doing. They don't necessarily perceive themself or feel the mouse under their hand when they're using an open, when they're trying to do\nan open loop calibration. They're being asked to perform something. Like imagine if you sort of\nhad your whole right arm numbed and you stuck it in a box\nand you couldn't see it. So you had no visual feedback and you had no proprioceptive feedback about what the position or\nactivity of your arm was. And now you're asked, okay,\ngiven this thing on the screen that's moving from left to\nright, match that speed. And you basically can\ntry your best to invoke whatever that imagined\naction is in your brain that's moving the cursor\nfrom left to right. But in any situation,\nyou're gonna be inaccurate and maybe inconsistent\nin how you do that task. And so that's sort of the fundamental challenge of open loop. The challenge with closed loop is that, once the user's given a model, and they're able to start\nmoving the mouse on their own, they're going to very\nnaturally adapt to that model. And that co-adaptation\nbetween the model learning, what they're doing, and the user learning how to use the model may\nnot find you the best sort of global minima. And maybe that your first\nmodel was noisy in some ways or maybe just had some like quirk, like if there's some like\npart of the data distribution that didn't cover super well,\nand the user now figures out because they're a\nbrilliant user like Noland. They figured out the right\nsequence of imagined motions or the right angle they\nhave to hold their hand at to get it to work. And they'll get it to work great, but then the next day, they\ncome back to their device and maybe they don't remember exactly all the tricks that they\nused the previous day. And so there's a complicated\nsort of feedback cycle here that can emerge and can make it a very, very difficult debugging process. - Okay, there's a lot of really\nfascinating things there. Yeah, actually, just to\nstay on the closed loop, I've seen situations,\nthis actually happened watching psychology grad students. They use piece of software\nwhen they don't know how to program themselves. They use piece of software\nthat somebody else wrote, and it has a bunch of bugs. And they figure out like, and they've been using it for years. They figured out ways to work around it. Oh, that just happens. Like nobody like considers\nmaybe we should fix this. They just adapt. And that's a really interesting notion, that we were really good at\nadapting, but you need to still, that might not be the optimal. Okay, so how do you solve that problem? Do you have to restart from scratch every once in a while kind of thing? - Yeah, it's a good question. First and foremost, I would say this is not a solved problem. And for anyone who's listening in academia who works on BCIs, I would\nalso say this is not a problem that's solved by simply\nscaling channel account. Maybe that can help and you can get sort of richer covariance structures that you can use to exploit when trying to come up with\ngood labeling strategies. But if you're interested in problems, that aren't gonna be solved inherently by scaling channel account,\nthis is one of them. Yeah, so how do you solve it? It's not a solved problem. That's the first thing I\nwanna make sure gets across. The second thing is, any solution\nthat involves closed loop is going to become a very\ndifficult debugging problem. And one of my sort of general\nheuristics for choosing what prompts to tackle is\nthat you wanna choose the one that's gonna be the easiest to debug, 'cause if you can do that, even if the ceiling is\nlower, you're gonna be able to move faster because you have a tighter iteration loop debugging the problem. And in the open loop setting, there's not a feedback cycle debug with the user in the loop. And so there's some reason to think that that should be an\neasier debugging problem. The other thing that's worth understanding is that even in a closed loop setting, there's no special software\nmagic of how to infer what the user is truly attempting to do. In the closed loop setting,\nalthough they're moving the cursor on the screen, they\nmay be attempting something different than what your\nmodel is outputting. So what the model is\noutputting is not a signal that you can use to retrain\nif you want to be able to improve the model further. You still have this very\ncomplicated guesstimation or unsupervised problem of figuring out what is the true user intention\nunderlying that signal. And so the open loop problem\nhas the nice property of being easy to debug. And the second nice property of, it has all the same\ninformation and content as the closed loop scenario. Another thing I wanna mention and call out is that this problem\ndoesn't need to be solved in order to give useful control to people. Even today with the solutions we have now and that academia has\nbuilt up over decades, the level of control that can be given to a user today is quite useful. It doesn't need to be solved to get to that level of control. But again, I wanna build\nthe world's best mouse. I wanna make it so good that it's not even a\nquestion that you want it. And to build the world's best\nmouse, the superhuman version, you really need to nail that problem. And a couple maybe details\nof previous studies that we've done internally that\nI think are very interesting to understand when thinking\nabout how to solve this problem, the first is that even when\nyou have ground truth data of what the user's trying to do, and you can get this with\nan able-bodied monkey, a monkey that has a\nNeuralink device implanted and moving a mouse to control a computer, even with that ground truth dataset, it turns out that the\noptimal thing to predict to produce high performance BCI is not just the direct\ncontrol of the mouse. You can imagine building dataset of what's going on in the brain and what is the mouse\nexactly doing on the table. And it turns out that\nif you build the mapping from neuro spikes to predict exactly what the mouse is doing,\nthat model will perform worse than a model that\nis trained to predict sort of higher level assumptions about what the user might be trying to do. For example, assuming\nthat the monkey is trying to go in a straight line to the target, it turns out that making those assumptions is actually more effective in producing a model\nthan actually predicting the underlying hand movement. - So the intention, not like the physical\nmovement or whatever. - Yeah.\n- There's obviously a really strong correlation\nbetween the two, but the intention is a more\npowerful thing to be chasing. - [Bliss] Right. - Well, that's also super interesting. I mean, the intention\nitself is fascinating, because yes, with the BCI here, in this case, with a digital telepathy, you're acting on the\nintention, not the action, which is why there's an experience of like feeling like it's happening before you meant for it to happen. That is so cool. And that is why you could achieve like superhuman performance problem in terms of the control of the mouse. So for open loop, just to clarify, so whenever the person is tasked to like move the mouse to the right, you said there's not feedback so they don't get to get that satisfaction of like actually getting\nit to move, right? - You could imagine giving\nthe user feedback on a screen, but it's difficult, because at this point, you don't know what\nthey're attempting to do. So what can you show them\nthat would basically give them a signal of I'm doing this\ncorrectly or not correctly. So let's take this very specific example. Like maybe your calibration\ntask looks like you're trying to move the cursor a\ncertain position offset. So your instructions to the user are, \"Hey, the cursor's here. Now, when the cursor disappears, imagine moving it 200\npixels from where it was to the right to be over this target.\" In that kind of scenario,\nyou could imagine coming up with some sort of consistency\nmetric that you could display to the user of, \"Okay, I\nknow what the spike train looks like on average when you\ndo this action to the right. Maybe I can produce some sort\nof probabilistic estimate of how likely is that to\nbe the action you took given the latest trial or\ntrajectory that you imagined.\" And that could give the\nuser some sort of feedback of how consistent are they\nacross different trials. You could also imagine that\nif the user is prompted with that kind of consistency metric, that maybe they just become more behaviorally engaged to begin with because the task is kind of boring when you don't have any feedback at all. And so there may be benefits\nto the user experience of showing something on the screen, even if it's not accurate,\njust because it keeps the user motivated to try to increase that number or push it upwards. - So there's a psychology element here. - Yeah, absolutely. - And again, all of that is UX challenge. How much signal drift is there,\nhour to hour, day to day, week to week, month to month? How often do you have to recalibrate because of the signal drift? - Yeah, so this is a\nproblem we've worked on, both with NHP, non-human primates,\nbefore our clinical trial and then also with Noland\nduring the clinical trial. Maybe the first thing that's worth stating is what the goal is here. So the goal is really to enable the user to have a plug and play experience where I guess they don't\nhave to plug anything in, but a play experience where they can use the device whenever they want\nto, however they want to. And that's really what we're aiming for. And so there can be a set of solutions that get to that state without considering this non-stationary problem. So maybe the first solution\nhere that's important is that they can recalibrate\nwhenever they want. This is something that Noland\nhas the ability to do today. So he can recalibrate the system at 2:00 AM in the middle of the night, without his caretaker or\nparents or friends around to help push a button for him. The other important part\nof the solution is that when you have a good model calibrated, that you can continue using that without needing to recalibrate it. So how often he has to do\nthis recalibration today depends really on his\nappetite for performance. We observe sort of a\ndegradation through time of how well any individual model works, but this can be mitigated behaviorally by the user adapting\ntheir control strategy. It can also be mitigated\nthrough a combination of sort of software features\nthat we provide to the user. For example, we let\nthe user adjust exactly how fast the cursor is moving. We call that the gain, for example, the gain of how fast the cursor reacts to any given input intention. They can also adjust the smoothing, how smooth the output of that\ncursor intention actually is. They can also adjust the friction, which is how easy is it\nto stop and hold still. And all these software tools\nallow the user a great deal of flexibility and\ntroubleshooting mechanisms to be able to solve this problem for themselves.\n- By the way, all of this is done by looking to the right side of the screen, selecting the mixer,\nand the mixer you have- - It's like DJ mode. DJ mode for your VCI. - So I mean, it's a really\nwell done interface. It's really, really well done. And so yeah, there's that bias\nthat there's a cursor drift that Noland talked about in a stream, although he said that you\nguys were just playing around with it with him and then\nconstantly improving. So that could have been just a snapshot of that particular\nmoment, a particular day. But he said that there\nwas this cursor drift and this bias that\ncould be removed by him, I guess looking to the\nright side of the screen, the left side of the screen\nto kind of adjust the bias. - Yeah, yeah.\n- That's one interface action I guess to adjust the bias. - Yeah, so this is actually an idea that comes out of academia. There are some prior work\nwith sort of BrainGate clinical trial participants\nwhere they pioneered this idea of bias correction. The way we've done it I think is, yeah, it's very prototized, very beautiful user experience where the user can essentially flash the cursor over to\nthe side of the screen and it opens up a window\nwhere they can actually sort of adjust or tune exactly\nthe bias of the cursor. So bias maybe, for people\nwho aren't familiar, is just sort of what is the default motion of the cursor if you're imagining nothing. And it turns out that\nthat's one of the first sort of qualia of the\ncursor control experience that's impacted by neuro non-stationarity. - Quality off the cursor experience. - I don't know how else to describe it. I'm not the guy moving- - It's very poetic, I love it. The quality of the cursor experience. Yeah, I mean, it sounds poetic but it is deeply true. There is an experience,\nwhen it works well, it is a joyful, a really\npleasant experience. And when it doesn't work well, it's a very frustrating experience. That's actually the art of UX. It's like you have the\npossibility to frustrate people or the possibility to give them joy, - And at the end of the day,\nit really is truly the case that UX is how the thing works. And so it's not just like\nwhat's showing on the screen, it's also what control surfaces does a decode provide the user? Like we want them to feel\nlike they're in the F1 card, not like some like minivan, right? And that really truly is\nhow we think about it. Noland himself is an F1 fan, so we refer to ourself as a pit crew. He really is truly the F1 driver, and there's different control surfaces that different kinds of cars\nand airplanes provide the user. And we take a lot of inspiration\nfrom that when designing how the cursor should behave.. And what maybe one nuance of this is, even details like when you move a mouse on a MacBook track pad,\nthe sort of response curve of how that input that\nyou give the track pad translates to cursor movement is different than how it works with a mouse. When you move on the track pad, there's a different response function, a different curve to how much\na movement translates to input to the computer than when you\ndo it physically with a mouse. And that's because somebody\nsat down a long time ago when they're designed\nthe initial input systems to any computer and they\nthought through exactly how it feels to use\nthese different systems. And now we're designing\nsort of the next generation of this input system to a\ncomputer, which is entirely done via the brain, and there's\nno proprioceptive feedback. Again, you don't feel\nthe mouse in your hand, you don't feel the keys\nunder your fingertips, and you want a control surface\nthat still makes it easy and intuitive for the user to understand the state of the system and how to achieve what\nthey wanna achieve. And ultimately, the end goal\nis that that UX is completely, it fades into the background. It becomes something that's\nso natural and intuitive that it's subconscious to the user, and they just should feel\nlike they have basically direct control over the cursor. It just does what they want it to do. They're not thinking\nabout the implementation of how to make it do\nwhat they want it to do. It's just doing what they want it to do. - Is there some kind of things along the lines of like Fitts' law where you should move the\nmouse in a certain kind of way that maximizes your\nchance to hit the target? I don't even know what I'm asking, but I'm hoping the\nintention of my question will land on a profound answer. No, is there some kind of understanding of the laws of UX when\nit comes to the context of somebody using their\nbrain to control it? Like that's different\nthan actual with a mouse? - I think we're in the early stages of discovering those\nlaws, so I wouldn't claim to have solved that problem yet. But there's definitely\nsome things we've learned that make it easier for\nthe user to get stuff done. And it's pretty straightforward\nwhen you verbalize it, but it takes a while to\nactually get to that point when you're in the process of debugging the stuff in the trenches. One of those things is that\nany machine learning system you build has some number of errors, and it matters how those errors translate to the downstream user experience. For example, if you're\ndeveloping a search algorithm in your photos, if you\nsearch for your friend Joe and it pulls up a photo\nof your friend Josephine, maybe that's not a big deal\nbecause the cost of an error is not that high. In a different scenario where you're trying to\ndetect insurance fraud or something like this and\nyou're directly sending someone to court because of some\nmachine learning model output, then the errors make a lot\nmore sense to be careful about. You wanna be very thoughtful about how those errors translate\nto downstream effects. The same is true in BCI. So for example, if you're building a model that's decoding a velocity\noutput from the brain versus an output where\nyou're trying to modulate the left click, for example. These have sort of different trade-offs of how precise you need to be before it becomes useful to the end user. For velocity, it's okay\nto be on average correct, because the output of the model\nis integrated through time. So if the user's trying\nto click at position A, and they're currently in position B, they're trying to navigate over time to get between those two points. And as long as the output of the model is on average correct,\nthey can sort of steer it through time with the user\ncontrol loop in the mix. They can get to the\npoint they wanna get to. The same is not true of a click. For a click, you're\nperforming it almost instantly at the scale of neurons firing. And so you want to be very\nsure that that click is correct because a false click can be\nvery destructive to the user. They might accidentally close\nthe tab that they're trying to do something and\nlose all their progress. They might accidentally like hit some Send button on some text that it's only like half-composed\nand reads funny after. So there's different\nsort of cost functions associated with errors in this space. And part of the UX design is understanding how to build a solution\nthat is when it's wrong, still useful to the end user. - It's so fascinating that assigning cost to every action when an error occurs. So every action, if an error\noccurs, has a certain cost, and incorporating that into how\nyou interpret the intention, mapping it to the action\nis really important. I didn't quite until you said it realize there's a cost to like\nsending the text early. It's like very expensive cost.\n- Yeah. It's super annoying if you accidentally, like if you're a cursor,\nimagine if your cursor misclick every once in a while,\nthat's like super obnoxious. And the worst part of it is, usually, when the user's trying to click,\nthey're also holding still because they're over the\ntarget they wanna hit and they're getting ready to click, which means that in the data\nsets that we build, on average, it's the case that sort of low speeds or desire to hold still. It's correlated with when the\nuser's attempting to click. - Wow, that is really fascinating. - It's also not the case. People think that, \"Oh, a\nclick is a binary signal. This must be super easy to decode.\" Well, yes it is, but the bar is so much higher for it to become a useful thing for the user, and there's ways to solve this. I mean, you can sort of take\nthe compound approach of, well, let's just give the like, let's take five seconds to click. Let's take a huge window of time so it can be very\nconfident about the answer. But again, world's best mouse. The world's best mouse\ndoesn't take a second to click or 500 milliseconds to click. It takes five milliseconds\nto click or less. And so if you're aiming\nfor that kind of high bar, then you really wanna solve\nthe underlying problem. - So maybe this is a\ngood place to ask about how to measure performance,\nthis whole bits per second. Can you like explain\nwhat you mean by that? Maybe a good place to start is to talk about Webgrid as a game, as a good illustration of the\nmeasurement of performance. - Yeah, maybe I'll take\none zoom out step there, which is just explaining why\nwe care to measure this at all. So again, our goal is to\nprovide the user the ability to control the computer as well as I can and hopefully better. And that means that they can do it at the same speed as what I can do. It means that they have access to all the same functionality that I have, including all those little\ndetails like command tab, command space, all this stuff. They need to be able to\ndo it with their brain and with the same level of reliability as what I can do with my muscles. And that's a high bar. And so we intend to measure and quantify every aspect of that to understand how we're progressing towards that goal. There's many ways to\nmeasure BPS, by the way. This isn't the only way,\nbut we present the user a creative targets, and basically, we compute a score which\nis dependent on how fast and accurate they can select, and then how small are the targets. And the more targets\nthat are on the screen, the smaller they are, the more information\nyou present per click. And so if you think about\nit from information theory point of view, you can communicate across different information\ntheoretic channels. And one such channel is a typing interface you could imagine that's\nbuilt out of a grid, just like a software\nkeyboard on the screen. And bits per second is a\nmeasure that's computed by taking the log of the number\nof targets on the screen. You can subtract one if you\ncare to model a keyboard because you have to subtract one for the Delete key on the keyboard, but log of the number\nof targets on the screen times the number of correct\nselections minus incorrect, divided by some time window. For example, 60 seconds. And that's sort of the standard way to measure a cursor\ncontrol task in academia. And all credit in the world\ngoes to this great professor, Dr. Shenoy of Stanford who\ncame up with that task. And he's also one of my\ninspirations for being in the field. So all the credit in the world to him for coming up with a standardized metric to facilitate this kind of\nbragging rights that we have now, to say that Noland is\nthe best in the world at this task with his BCI. It's very important for progress that you have standardized\nmetrics that people can compare across different\ntechniques and approaches. How well does this do? So yeah, big kudos to him and\nto all the team at Stanford. Yeah, so for Noland, and\nfor me playing this task, there's also different modes that you can configure this task. So the Webgrid task can be presented as just sort of a left\nclick on the screen, or you could have targets\nthat you just dwell over, or you could have targets\nthat you left, right click on. You could have targets\nthat are left, right click, middle click, scrolling,\nclicking, and dragging. You could do all sorts of things within this general framework. But the simplest, purest\nform is just blue targets show up on the screen. Blue means left click. That's the simplest form of the game. And the sort of prior\nrecords here in academic work and at Neuralink internally with sort of NPS have all been matched or beaten by Noland with\nhis Neuralink device. So sort of prior to Neuralink,\nthe sort of world record for a human using device\nis somewhere between 4.2 to 4.6 BPS, depending on exactly what paper you read and\nhow you interpret it. Noland's current record is 8.5 BPS. And again, this sort of median Neuralink performance is 10 BPS. So you can think of it roughly as he's 85% the level of control\nof a median Neuralinker using their cursor to select\nblue targets on the screen. And yeah, I think there's a\nvery interesting journey ahead to get us to that same\nlevel of 10 BPS performance. It's not the case that sort\nof the tricks that got us from four to six BPS,\nand then six to eight BPS are gonna be the ones that\nget us from eight to 10. And in my view, the core challenge here is really the labeling problem. It's how do you understand at\na very, very fine resolution what the user's attempting to do. And yeah, I highly encourage folks in academia to work on this problem. - What's the journey\nwith Noland on that quest of increasing the BPS on Webgrid? In March, you said that he selected 89,285 targets in Webgrid. - Yep.\n- So he loves this game. He's really serious about improving his performance in this game. So what is that journey\nof trying to figure out how to improve that performance? How much can that be done\non the decoding side? How much can that be done\non the calibration side? How much can that be\ndone on the Noland side of like figuring out how to convey his intention more cleanly? - Yeah, no, this is a great question. So in my view, one of\nthe primary reasons why Noland's performance is so\ngood is because of Noland. Noland is extremely\nfocused and very energetic. He'll play Webgrid sometimes\nfor like four hours in the middle of the night. Like from 2:00 AM to 6:00\nAM, he'll be playing Webgrid, just because he wants to push it to the limits of what he can do. And this is not us like\nasking him to do that. I wanna be clear. Like we're not saying, \"Hey, you should play Webgrid tonight.\" We just gave him the game\nas part of our research, and he is able to play it independently and practice whenever he wants, and he really pushes hard to push it, the technology's the absolute limit. And he views that as like his job really to make us be the bottleneck. And boy, has he done that well. And so the first thing\nto acknowledge is that he's extremely motivated\nto make this work. I've also had the privilege to meet other clinical trial participants\nfrom BrainGate and other trials, and they very much\nshared the same attitude of like they view this\nas their life's work to advance the technology\nas much as they can. And if that means selecting\ntargets on the screen for four hours from 2:00 AM\nto 6:00 AM, then so be it. And there's something\nextremely admirable about that that's worth calling out. Okay, so then how do you sort of get from where he started, which is\nno cursor control to a BPS? So I mean, when he started, there's a huge amount of learning to do on his side and our side to figure out what's the most intuitive control for him. And the most intuitive\ncontrol for him is sort of, you have to find the set intersection of what do we have this signal to decode. So we don't pick up every single\nneuron in the motor cortex, which means we don't have representation for every part of the body. So there may be some\nsignals that we have better sort of decode performance on than others. For example, on his left hand,\nwe have a lot of difficulty distinguishing his left ring finger from his left middle finger. But on his right hand,\nwe have a good control and good modulation\ndetected from the neurons that we're able to record for his pinky, his thumb, and his index finger. So you can imagine how these different sub spaces of modulated activity intersect with what's the\nmost intuitive for him. And this has evolved over time. So once we gave him the ability to calibrate models on his own, he was able to go and explore\nvarious different ways to imagine controlling the cursor. For example, he could imagine\ncontrolling the cursor by wiggling his wrist side to side, or by moving his entire arm. I think at one point, he did his feet. He tried like a whole bunch\nof stuff to explore the space of what is the most natural way for him to control the cursor. That at the same time, it's\neasy for us to decode rules. - Just to clarify, it's through\nthe body mapping procedure that you're able to figure\nout which finger he can move? - Yes, yeah, that's one way to do it. Maybe one nuance of when he's doing it, he can imagine many more things than we represent in that\nvisual on the screen. So we show him sort of\nabstractly, \"Here's a cursor. You figure out what\nworks the best for you.\" And we obviously have hints\nabout what will work best from that body mapping procedure of we know that this particular action, we can represent well. But it's really up to\nhim to go and explore and figure out what works the best. - But at which point does\nhe no longer visualize the movement of his body\nand is just visualizing the movement of the cursor? - Yeah.\n- How quickly does he go from, how quickly does he get there? - So this happened on a Tuesday. I remember this day very clearly, because at some point during the day, it looked like he wasn't doing super well. It looked like the model\nwasn't performing super well and he was like getting distracted. But he actually, it wasn't the case. Like what actually happened\nwas he was trying something new where he was just controlling the cursor. So he wasn't imagining\nmoving his hand anymore. He was just imagining,\nI don't know what it is, some like abstract intention to move the cursor on the screen. And I cannot tell you what the difference between those two things are. I really truly cannot. He's tried to explain it to me before. I cannot give a first person\naccount of what that's like. But the expletives that\nhe uttered in that moment were enough to suggest that it was a very qualitatively\ndifferent experience for him to just have direct\nneural control over a cursor. - I wonder if there's a way through UX to encourage a human\nbeing to discover that, because he discovered\nit, like you said to me, that he's a pioneer. So he discovered that on\nhis own through all of this, the process of trying to move the cursor with different kinds of intentions. But that is clearly a really\npowerful thing to arrive at, which is to let go of trying\nto control the fingers and the hand and control the actual digital device with your mind. - That's right, UX is how it works. And the ideal UX is one\nthat the user doesn't have to think about what they need to do in order to get it done. It just does it. - That is so fascinating. But I wonder on the biological side how long it takes for the brain to adapt. - Yeah.\n- So is it just simply learning like high level software, or is there like a\nneuroplasticity component where like the brain is adjusting slowly? - Yeah, the truth is, I don't know. I'm very excited to see with\nsort of the second participant that we implant what the\njourney is like for them, because we'll have learned a lot more. Potentially, we can help them understand and explore that direction more quickly. This is something I didn't know. This wasn't me prompting\nNoland to go try this. He was just exploring\nhow to use his device and figure it out himself. But now that we know that\nthat's a possibility, that maybe there's a way to, for example, hint the user, \"Don't try\nsuper hard during calibration. Just do something that feels natural, or just directly control the cursor. Don't imagine explicit action.\" And from there, we should be\nable to hopefully understand how this is for somebody who has not experienced that before. Maybe that's the default\nmode of operation for them. You don't have to go through this intermediate phase of explicit motions. - Or maybe if that naturally\nhappens for people, you can just occasionally encourage them to allow themselves to move the cursor. Actually sometimes, just\nlike with a four minute mile, just the knowledge that that's possible. - Pushes you to do it. - Yeah, enables you to do it, and then it becomes trivial. And then it also makes you wonder, it's the cool thing about humans. Once there's a lot more\nhuman participants, they will discover\nthings that are possible. - Yes, and share their experiences. - Yeah, and share.\n- With each other. - And that because of them sharing it, they'll be able to do it. All of a sudden, that's\nunlocked for everybody, because just the knowledge sometimes is the thing that enables it to do it. - Yeah, I mean, and just\nto comment on that too, we've probably tried like\na thousand different ways to do various aspects of decoding, and now we know like what\nthe right subspace is to continue exploring further. Again, thanks to Noland and the many hours he's put into this. And so even just that help, like help constraints\nsort of the beam search of different approaches\nthat we could explore really helps accelerate\nfor the next person the set of things that\nwe'll get to try on day one, how fast we hope to get\nthem to useful control, how fast we can enable 'em\nto use it independently, and to get value out of the system. So yeah, massive hats off to Noland and all the participants\nthat came before him to make this technology a reality. - So how often are the\nupdates to the decoder? 'Cause Noland mentioned like, okay, there's a new update\nthat we're working on, and that in the stream, he said he plays the snake game\nbecause it's like super hard. It's a good way for him to test\nlike how good the update is. And he says like sometimes the\nupdate is a step backwards. It's a constant like iteration. Like what does the update entail? Is it mostly on the decoder side? - Yeah, a couple comments. So one is it's probably\nworth drawing distinction between sort of research sessions where we're actively\ntrying different things to understand like what\nthe best approach is versus sort of independent\nuse where we wanted to have an ability to just go use the device, how anybody would wanna use their MacBook. And so what he's referring to is, I think usually in the\ncontext of a research session, where we're trying many,\nmany different approaches to even unsupervised approaches like we talked about earlier to try to come up with better ways to estimate his true intention and more accurately decode it. And in those scenarios, I mean we try, in any given session, he'll sometimes work for like eight hours a day. And so that can be hundreds\nof different models that we would try in that day. Like a lot of different things. Now, it's also worth noting that we update the application he uses quite frequently. I think sometimes, up to like\nfour or five times a day. We'll update his application\nwith different features or bug fixes or feedback\nthat he's given us. He's a very articulate person\nwho is part of the solution. He's not a complaining person. He says, \"Hey, here's this\nthing that I've discovered is not optimal in my flow. Here's some ideas how to fix it. Let me know what your thoughts are. Let's figure out how to solve it.\" And it often happens that\nthose things are addressed within a couple hours of\nhim giving us his feedback. That's the kind of\niteration cycle we'll have. And so sometimes, at the\nbeginning of the session, he'll give us feedback, and\nat the end of the session, he's giving us feedback\non the next iteration of that process or that set up. - That's fascinating,\nbecause one of the things you mentioned, that there\nwas 271 pages of notes taken from the BCI sessions,\nand this was just in March. So one of the amazing\nthings about human beings that they can provide,\nespecially ones who are smart and excited and all like positive\nand good vibes like Nolan, that they can provide\nfeedback, continuous feedback. - Yeah, it also requires, just to brag on the team a little bit, I work with a lot of exceptional people, and it requires the team\nbeing absolutely laser focused on the user and what will\nbe the best for them. And it requires like a\nlevel of commitment of, \"Okay, this is what the user feedback was. I have all these meetings. We're gonna skip that today\nand we're gonna do this.\" That level of focus\ncommitment is, I would say, underappreciated in the world. And also, you obviously\nhave to have the talent to be able to execute on\nthese things effectively. And yeah, we have that in loads. - Yeah, and this is such a\ninteresting space of UX design because there's so many unknowns here. And I can tell UX is difficult because of how many people do it poorly. It's just not a trivial thing. - Yeah, it's also, you know, UX is not something that\nyou can always solve by just constant iterating\non different things. Like sometimes, you\nreally need to step back and think globally, am\nI even in like the right sort of minima to be\nchasing down for a solution? Like there's a lot of problems in which sort of fast iteration\ncycle is the predictor of how successful you will be. As a good example, like in\nRL simulation, for example, the more frequently you get reward, the faster you can progress. It's just an easier learning prompt, the more frequently you get feedback. But UX is not that way. I mean, users are\nactually quite often wrong about what the right solution is, and it requires a deep understanding\nof the technical system and what's possible, combined with what the problem\nis you're trying to solve. Not just how the user expressed it, but what the true underlying problem is to actually get to the right place. - Yeah, that's the old like stories of Steve Jobs like rolling in there, like, yeah, the user is a useful signal, but it's not a perfect signal. And sometimes, you have to\nremove the floppy disk drive or whatever the, I forgot\nall the crazy stories of Steve Jobs like making\nwild design decisions. But there, some of it is aesthetic, that some of it is about the\nlove you put into the design, which is very much a Steve\nJobs-Jony Ive type of thing. But when you have a human\nbeing using their brain to interact with it, it also\nis deeply about function. It's not just aesthetic. And that you have to empathize with a human being before you, while not always listening\nto them directly. Like you have to deeply empathize. It's fascinating. It's really, really fascinating. And at the same time, iterate. But not iterate in small ways. Sometimes, a complete like\nrebuilding the design. He said that, Noland said in the early\ndays, the UX sucked. But you improved quickly. What was that journey like? - Yeah, I mean, I'll give\none concrete example. So he really wanted to\nbe able to read manga. This is something that he, I mean, it sounds like a simple thing, but it's actually a\nreally big deal for him. And he couldn't do it\nwith his mouth stick. It wasn't accessible. You can't scroll with a\nmouse stick on his iPad on the website that he\nwanted to be able to use to read the newest manga. - Might be a good quick pause to say the mouth stick\nis the thing he's using, holding a stick in his\nmouth to scroll on a tablet. - Right, yeah, it's basically, you can imagine it's a stylus that you hold between your teeth. Yeah, it's basically a very long stylus. - And it's exhausting, it hurts, and it's inefficient. - Yeah, and maybe it's\nalso worth calling out, there are other alternative\nassisted technologies, but the particular situation Noland's in, and this is not uncommon, and I think it's also not\nwell understood by folks, is that he's relatively spastic so he'll have muscle\nspasms from time to time. And so any assistive\ntechnology that requires him to be positioned directly\nin front of a camera, for example, an eye tracker, or anything that requires him\nto put something in his mouth just is a no go, 'cause he'll either be shifted out of frame\nwhen he has a spasm, or if he has something in his mouth, it'll stab him in the face\nif he spasms too hard. So these kind of\nconsiderations are important when thinking about what advantages a BCI has in someone's life. If it fits ergonomically into your life in a way that you can use it independently when your caretaker's not\nthere, wherever you want to, either in the bed or in the chair, depending on your comfort level and your desire to have pressure sores, all these factors matter a lot in how good the solution\nis in that user's life. So one of these very\nfun examples is scroll. So again, manga is something\nhe wanted to be able to read, and there's many ways\nto do scroll with a BCI. You can imagine like different\ngestures, for example. The user could do that,\nwould move the page. But scroll is a very\nfascinating control surface, because it's a huge thing on\nthe screen in front of you. So any sort of jitter in the model output, any sort of error in the model output causes like an earthquake on the screen. Like you really don't\nwanna have your manga page you're trying to read be\nshifted up and down a few pixels just because your scroll decoder\nis not completely accurate. And so this was an example\nwhere we had to figure out how to formulate the problem in a way that the errors of the system,\nwhenever they do occur, and we'll do our best to minimize them, whenever those errors do occur, that it doesn't interrupt the qualia again of the experience that the user is having. It doesn't interrupt their\nflow of reading their book. And so what we ended up building is this really brilliant feature. This is teammate named Ruse, who worked on this really brilliant work called quick scroll. And quick scroll basically\nlooks at the screen, and it identifies where on\nthe screen are scroll bars. And it does this by deeply\nintegrated with MacOs to understand where are the\nscroll bars actively present on the screen using the\nsort of accessibility tool that's available to MacOs apps. And we identified where\nthose scroll bars are and we provided a BCI scroll bar. And the BCI scroll bar looks\nsimilar to a normal scroll bar but it behaves very differently in that once you sort of move over to it, your cursor sort of morphs onto it. It sort of attaches or latches onto it. And then once you push up or down in the same way that you'd\nuse a push to control the normal cursor, it actually\nmoves the screen for you. So it's basically like\nremapping the velocity to a scroll action. And the reason that feels so\nnatural and intuitive is that, when you move over to attach\nto it, it feels like magnetic. So you're like sort of stuck onto it. And then it's one continuous action. You don't have to like switch\nyour imagined movement. You sort of snap onto it\nand then you're good to go. You just immediately can start pulling the page down or pushing it up. And even once you get that right, there's so many little nuances of how the scroll behavior works to make it natural and intuitive. So one example is momentum. Like when you scroll a page\nwith your fingers on the screen, you actually have some like flow. Like it doesn't just stop right when you lift your finger up. The same is true with BCI scroll. So we had to spend some time to figure out what are the right nuances. When you don't feel the screen\nunder your fingertip anymore, what is the right sort of dynamic or what's the right amount\nof page give, if you will, when you push it to make\nit flow the right amount for the user to have a natural experience reading their book. And there's a million, I mean, I could tell you like there's\nso many little minutia of how exactly that scroll works\nthat we spent probably like a month getting right to make\nthat feel extremely natural and easy for the user to navigate. - I mean, even to scroll on\na smartphone with your finger feels extremely natural and pleasant. And it probably takes\na extremely long time to get that right. And actually, the same\nkind of visionary UX design that we're talking about, don't\nalways listen to the users but also listen to them, and\nalso have like visionary big, like throw everything out, think from first principles but also not. Yeah, yeah, by the way,\nit just makes me think that scroll bars on the desktop probably have stagnated\nand never taken that like, 'cause the snap, same\nas like snap the grid, snap the scroll bar action\nyou're talking about is something that could\npotentially be extremely useful in the desktop setting, even just for users to just\nimprove the experience, 'cause the current scroll bar experience in the desktop is horrible.\n- Yeah, agreed. - It's hard to find, hard to control. There's not a momentum. And the intention should be clear. When I start moving towards a scroll bar, there should be a snapping\nto the scroll bar action. But of course, maybe I'm\nokay paying that cost, but there's hundreds of millions of people paying that cost nonstop. But anyway, but in this case, this is necessary because\nthere's an extra cost paid by Noland for the jitteriness. So you have to switch between\nthe scrolling and the reading. There has to be a phase\nshift between the two. Like when you're scrolling,\nyou're scrolling. - Right, right, so that is one drawback of the current approach. Maybe one other just\nsort of case study here, so again, UX is how it works, and we think about that\nholistically from like the, even the feature detection level of what we detect in the brain\nto how we design the decoder, what we choose to decode,\nto then how it works once it's being used by the user. So another good example in\nthe sort of how it works once they're actually using the decoder, the output that's displayed on the screen is not just what the decoder says. It's also a function of\nwhat's going on on the screen. So we can understand, for example, that when you're trying to close a tab, that very small, stupid little\nX that's extremely tiny, which is hard to get precisely hit if you're dealing with sort of a noisy output of the decoder, we can understand that that is a small little\nX you might be trying to hit and actually make it a\nbigger target for you. Similar to how when you're\ntyping on your phone, if you are used to like the\niOS keyboard, for example, it actually adapts the target size of individual keys based on\nan underlying language model. So it'll actually\nunderstand if I'm typing, \"Hey, I'm going to see L.\" It'll make the E key bigger, because in those Lex is the\nperson I'm gonna go see. And so that kind of predictiveness\ncan make the experience much more smooth even without improvements to the underlying decoder or feature detection part of the stack. So we do that with a feature\ncalled magnetic targets. We actually indexed the\nscreen and we understand, okay, these are the places\nthat are very small targets that might be difficult to hit. Here's the kind of cursor\ndynamics around that location that might be indicative of\nthe user trying to select it. Let's make it easier. Let's blow up the size of it in a way that makes it easier for the user to sort of snap onto that target. So all these little\ndetails, they matter a lot in helping the user be independent in their day-to-day living. - So how much of the work on the decoder is generalizable to P2, P3, P4, P5, PM? How do you improve the decoder in a way that's generalizable? - Yeah, great question. So the underlying signal\nwe're trying to decode is gonna look very\ndifferent in P2 than in P1. For example, channel\nnumber 345 is gonna mean something different in user\none than it will in user two, just because that\nelectrode that corresponds with channel 345 is gonna be\nnext to a different neuron in user one versus user two. But the approach is the methods, the user experience of\nhow do you get the right sort of behavioral pattern from the user to associate with that neural signal, we hope that will translate over multiple generations of users. And beyond that, it's very, very possible. In fact, quite likely that we've overfit to sort of Noland's user\nexperience desires and preferences. And so what I hope to see is that when we get second,\nthird, fourth participant, that we find sort of what\nthe right wide minimums are that cover all the cases, that make it more intuitive for everyone. And hopefully, there's a\ncrosspollination of things where, \"Oh, we didn't think\nabout that with this user, because they can speak. But with this user who\njust can fundamentally not speak at all, this user\nexperience is not optimal.\" And that will actually, those improvements that we make there should hopefully translate then to even people who can't speak but don't feel comfortable doing so because we're in a public setting, like their doctor's office. - So the actual mechanism\nof open loop labeling and then closed loop\nlabeling will be the same, and hopefully, can generalize\nacross the different users as they're doing the calibration step. And the calibration step is pretty cool. I mean, that in itself, the interesting thing about Webgrid, which is like closed loop, it's like fun. I love it when there's like, they used to be kind of\nidea of human computation, which is using actions a\nhuman would want to do anyway to get a lot of signal from. - Yeah.\n- And like Webgrid is that, like a nice video game that also serves as great calibration. - It's so funny. I've heard this reaction so many times. Before sort of the first\nuser was implanted, we had an internal perception that the first user\nwould not find this fun. And so we thought really quite\na bit actually about like, should we build other games that like are more\ninteresting for the user so we can get this kind of data and help facilitate research for long duration and stuff like this? Turns out that like people love this game. - Yeah.\n- I always loved it, but I didn't know that that\nwas a shared perception. - Yeah, and just in case\nit's not clear, Webgrid is, there's a grid of, let's\nsay 35 by 35 cells, and one of them lights up blue and you have to move your mouse\nover that and click on it. And if you miss it and it's red- - [Bliss] I played this\ngame for so many hours. So many hours. - And what's your record, you said? - I think I have the highest at Neuralink. Right now, my record's 17 BPS. - 17 BPS.\n- Which is about, if you imagine that 35 by 35 grid, you're hitting about 100\ntrials per minute in. So 100 correct selections\nin that one minute window. So you're averaging about, between 500, 600\nmilliseconds per selection. - So one of the reasons\nthat I think I struggle with that game is I'm\nsuch a keyboard person, so everything is done with via keyboard. If I can avoid touching\nthe mouse, it's great. So how can you explain\nyour high performance? - I have like a whole ritual I go through when I play Webgrid. So it's actually like a diet\nplan associated with this. Like it's a whole thing, so great. - The first thing-\n- You have to fast for five days, have to\ngo up to the mountains. - Actually, it kind of, I mean, the fasting thing is important. So this is like, you know- - Focuses the mind, yeah?\n- Yeah, it's true. So what I do is actually, I don't eat for a little bit beforehand. And then I'll actually eat\nlike a ton of peanut butter right before, and I get like- - This is a real thing?\n- This is a real thing, yeah. And then it has to be\nreally late at night. This is again a night owl\nthing I think we share, but it has to be like midnight,\n2:00 AM kind of time window. And I have a very specific like physical position I'll sit in, which is, I used to be, I\nwas homeschooled growing up and so I did most of my\nwork like on the floor, just like in my bedroom or whatever. And so I have a very specific\nsituation on the floor. On the floor, I sit and play, and then you have to make sure like there's not a lot\nof weight on your elbow when you're playing so\nthat you can move quickly. And then I turn the gain of the cursor, so the speed of the cursor way, way up. So it's like small motions\nthat actually move the cursor. - Are you moving with your wrist or you're never moving on- - I move my fingers. So my wrist is almost completely still. I'm just moving my fingers. - Yeah. You know those, just on a small tangent, which I've been meaning to\ngo down this rabbit hole of people that set the\nworld record in Tetris. Those folks, they're playing, there's a way to, did you see this? - I see like the three, like\nall the fingers are moving. - Yeah, you could find a way to do it where like it's using a loophole, like a bug that you can do\nsome incredibly fast stuff. So it's along that line but not quite. But you do realize there'll\nbe like a few programmers right now listening to this\ncool fast and eat peanut butter and be like-\n- Yeah, please, please trade my record. I mean, the reason I did this, literally, was just because I wanted\nthe bar to be high. The team, like I wanted\nthe number that we aim for should not be like the median performance. It should be able to\nbeat all of us at least. Like that should be the minimum bar. - What do you think is possible, like 20 scrapes?\n- Yeah, I don't know what the limits. I mean, the limits you can calculate just in terms of like screen refresh rate and like cursor immediately\njumping to the next target. But I mean, I'm sure\nthere's limits before that with just sort of reaction time and visual perception\nand things like this. I would guess it's in the\nbelow 40 but above 20, somewhere in there is probably the right that I'd never be thinking about. It also matters like how\ndifficult the task is. You could imagine like\nsome people might be able to do like 10,000 targets on the screen and maybe they can do better that way. There's some like task optimizations you could do to try to boost\nyour performance as well. - What do you think it takes for Noland to be able to do above 8.5? To keep increasing that number? You said like every increase in the number might require different- - [Bliss] Yeah. - Different improvements in the system. - Yeah, I think the\nnature of this work is, I mean, the first answer\nthat's important to say is, I don't know. This is edge of the research. So again, nobody's gotten\nto that number before. So what's next is gonna be a\nheuristic guess from my part. What we've seen historically\nis that different parts of the stack follow\nnext to different time points. So when I first joined Neuralink, like three years ago or so, one of the major problems\nwas just the latency of the Bluetooth connection. It was just like the radial\ndevice wasn't super good. It was an early revision of the implant, and it just like, no matter\nhow good your decoder was, if your thing is updating\nevery 30 milliseconds or 50 milliseconds, it's\njust gonna be choppy. And no matter how good you are, that's gonna be frustrating\nand lead to challenges. So at that point, it was very\nclear that the main challenge is just get the data off the\ndevice in a very reliable way, such that you can enable the\nnext challenge to be tackled. And then at some point, it was, actually, the modeling challenge of how do you just build a good mapping, like the supervised learning problem of you have a bunch of data and you have a label\nyou're trying to predict, just what is the right like\nneuro decoder architecture and hyper parameters to optimize that? And that was a problem for a bit. And once you solve that, it\nbecame a different bottleneck. I think the next bottleneck\nafter that was actually just sort of software\nstability and reliability. If you have widely varying\nsort of inference latency in your system, or your app just lags out every once in a while, it\ndecreases your ability to maintain and get in a state of flow, and it basically just disrupts\nyour control experience. And so there's a variety\nof different software bugs and improvements we made\nthat basically increased the performance of the system, made it much more\nreliable, much more stable, and led to a state where we\ncould reliably collect data to build better models with. So that was a bottleneck for a while. It's just sort of like\nthe software stack itself. If I were to guess right now, there's sort of two major directions you could think about for\nimproving BPS further. The first major direction is labeling. So labeling is again this\nfundamental challenge of given a window of time where the user is expressing some behavioral intent. What are they really trying to do at the granularity of every millisecond? And that, again, is a task design problem. It's a UX problem. It's a machine learning problem. It's a software problem. Sort of touches all\nthose different domains. The second thing you can think about to improve BPS further is\neither completely changing the thing you're decoding\nor just extending the number of things that you're decoding. So this is serving the direction\nof functionality, okay? So you can imagine giving more clicks. For example, a left click, a\nright click, a middle click. Different actions like\nclick and drag, for example. And that can improve the effective bit rate of your communication prosthesis. If you're trying to allow the\nuser to express themselves through any given communication channel, you can measure that with bits per second. But what action measures\nat the end of the day is how effective are they at\nnavigating their computer. And so from the perspective\nof the downstream tasks that you care about, functionality and extending functionality is something we're very interested in,\nbecause not only can it improve the sort of number of BPS, but it can also improve the downstream sort of independence that the user has and the skill and efficiency with which they can\noperate their computer. - Would the number of threads increasing also potentially help? - Yes, short answer is yes. It's a bit nuanced how that curve or how that manifests in the numbers. So what you'll see is\nthat if you sort of plot a curve of number of channels that you're using for decode versus either the offline metric of how good you are at decoding, or the online metric of\nsort of, in practice, how good is the user using this device, you see roughly a log curve. So as you move further\nout in number of channels, you get a corresponding sort\nof logarithmic improvement in control quality and\noffline validation metrics. The important nuance\nhere is that each channel corresponds with a specific represented intention in the brain. So for example, if you have a channel 254, it might correspond with\nmoving to the right. Channel 256 might mean move to the left. If you want to expand\nthe number of functions you want to control,\nyou really want to have a broader set of channels that covers a broader set\nof imagined movements. You can think of it kinda\nlike Mr. Potato man actually. Like if you had a bunch of different imagined movements you could do, how would you map those imagined movements to input to a computer? You can imagine handwriting to output characters on the screen. You can imagine just\ntyping with your fingers and have that output text on the screen. You can imagine different finger modulations for different clicks. You can imagine wiggling your big nose or opening some menu or\nwiggling your big toe to have like command tab\noccur or something like this. So it's really, the amount of different actions\nyou can take in the world depends on how many channels you have on the information\ncontent that they carry. - Right, so that's more\nabout the number of actions. So actually, as you increase\nthe number of threads, that's more about increasing the number of actions\nyou're able to perform. - One other nuance there\nthat is worth mentioning, so again, our goal is really\nto enable a user with process to control their computer\nas fast as I can. So that's BPS. With all the same functionality I have, which is what we just talked about, but then also as reliably as I can. And that last point is very related to channel account discussion. So as you scale out number of channels, the relative importance\nof any particular feature of your model input to the output control of the user diminishes, which means that if the sort\nof neural non-stationary effect is per channel, or if\nthe noise is independent, such that more channels means, on average, less output effect, then your reliability of\nyour system will improve. So one sort of core thesis\nthat at least I have is that scaling channel\naccount should improve the reliability system without any work on the decoder itself. - Can you linger on the reliability here? So first of all, when you see non-stationarity of the signal, which aspect are you referring to? - Yeah, so maybe let's talk briefly what the actual underlying\nsignal looks like. So again, I spoke very\nbriefly at the beginning about how when you imagine\nmoving to the right or imagine moving to the left, neurons might fire more or less. And their frequency\ncontent of that signal, at least in the motor cortex, it's very correlated with\nthe output intention, the behavioral task\nthat the user is doing. You can imagine actually,\nthis is not obvious, that rate coding, which is\nthe name of that phenomenon is like the only way the brain\ncould represent information. You can imagine many different ways in which the brain could encode intention. And there's actually evidence\nlike in bats, for example, that there's temporal codes. So timing codes of like exactly\nwhen particular neurons fire is the mechanism of\ninformation representation. But at least in the motor cortex, there's a substantial evidence\nthat it's rate coding, or at least one, like first order of fact is that it's rate coding. So then if the brain is\nrepresenting information by changing the sort of\nfrequency of a neuron firing, what really matters is sort of the delta between sort of the\nbaseline state of the neuron and what it looks like\nwhen it's modulated. And what we've observed and\nwhat has also been observed in academic work is\nthat that baseline rate, sort of the, if you're\nto target the scale, if you imagine that\nanalogy for like measuring flour or something when you're baking, that baseline state of\nhow much the pot weighs is actually different day to day. And so if what you're trying to measure is how much rice is in the pot, you're gonna get a different\nmeasurement different days, because you're measuring\nwith different pots. So that baseline rate shifting\nis really the thing that, at least from a first order\ndescription of the problem, is what's causing this downstream bias. There can be other effects, non-linear effects on top of that, but at least, at a very first order description of the problem, that's what we observe day to day is that the baseline firing\nrate of any particular neuron are observed on a\nparticular channel is changing. - So can you just adjust to the baseline to make it relative to\nthe baseline nonstop? - Yeah, this is a great question. So with monkeys, we have\nfound various ways to do this. One example way to do this is you ask them to do some behavioral task, like play the game with a joystick, you measure what's going on in the brain, you compute some mean of what's going on across\nall the input features, and you subtract it in the input when you're doing your BCI session. Works super well. For whatever reason, that doesn't work super well with Noland. I actually don't know the full reason why, but I can imagine several explanations. One such explanation could\nbe that the context effect difference between some open loop task and some closed loop task\nis much more significant with Noland than it is with a monkey. Maybe in this open loop task, he's watching the Lex Fridman podcast while he's doing the task, or he's whistling and listening to music and talking with his\nfriend and ask his mom what's for dinner while\nhe's doing this task. And so the exact sort\nof difference in context between those two states\nmay be much larger, and thus lead to a bigger\nsort of generalization gap between the features\nthat you're normalizing at sort of open loop time and what you're trying to\nuse at close loop time. - That's interesting. Just on that point,\nit's kind of incredible to watch Noland be able\nto do, to multitask, to do multiple tasks at the same time, to be able to move the\nmouse courser effectively while talking and while being nervous, because he's talking in front of- - Kicking my ass and chest too, yeah. - Kicking your ass. And talk trash while doing it. So all at the same time. And yes, if you are trying\nto normalize to the baseline, that might throw everything off. Boy is that interesting. - Maybe one comment on that too. For folks that aren't familiar\nwith assisted technology, I think there's a common belief that, well, why can't you\njust use an eye tracker or something like this\nfor helping somebody move a mouse on the screen? And it's a really a fair question, and one that I actually did was not confident before Noland, that this was gonna be a profoundly transformative technology\nfor people like him. And I'm very confident\nnow that it will be, but the reasons are subtle. It really has to do with ergonomically how it fits into their life. Even if you can just offer\nthe same level of control as what they would have\nwith an eye tracker or with a mouse stick, but you don't need to have\nthat thing in your face. You don't need to be\npositioned a certain way. You don't need your caretaker to be around to set it up for you. You can activate it when you want, how you want, wherever you want. That level of independence is\nso game changing for people. It means that they can text\na friend at night privately without their mom needing\nto be in the loop. It means that they can like open up and browse the internet at\n2:00 AM when nobody's around to set their iPad up for them. This is like a profoundly\ngame changing thing for folks in that situation. And this is even before we\nstart talking about folks that may not be able to communicate at all or ask for help when they want to. This can be potentially the only link that they have to the outside world. And yeah, that one doesn't\nI think need explanation of why that's so impactful. - You mentioned neural decoder. How much machine learning\nis in the decoder? How much magic, how much\nscience, how much art, how difficult is it to\ncome up with a decoder that figures out what these\nsequence of spikes mean? - Yeah, good question. There's a couple different\nways to answer this. So maybe I'll zoom out briefly first, and then I'll go down\none of the rabbit holes. So the zoomed out view is that building the decoder is really the process of building the dataset, plus\ncompiling it into the weights. And each of those steps is important. The direction I think\nof further improvement is primarily going to\nbe in the dataset side of how do you construct the\noptimal labels for the model. But there's an entirely separate challenge of then how do you compile the best model. And so I'll go briefly\ndown the second one, down the second rabbit hole. One of the main challenges with designing the optimal model for BCI\nis that offline metrics don't necessarily correspond\nto online metrics. It's fundamentally a control problem. The user is trying to control\nsomething on the screen, and the exact sort of user\nexperience of how you output the intention impacts\ntheir ability to control. So for example, if you just\nlook at validation loss, as predicted by your model,\nthere can be multiple ways to achieve the same validation loss. Not all of them are equally\ncontrollable by the end user. And it might be as simple as saying, \"Oh, you could just add\nauxiliary loss terms that like help you capture the\nthing that actually matters.\" But this is a very\ncomplex nuanced process. So how you turn the labels into the model is more of a nuanced process than just like a standard\nsupervised learning problem. One very fascinating anecdote here, we've tried many different\nsort of neural network architectures that translate brain data to velocity outputs, for example. And one example that's stuck in my brain from a couple years ago\nnow is, at one point, we were using just\nfully connected networks to decode the brain activity. We tried A/B test where we were measuring the relative performance\nin online control sessions of sort of 1D convolution\nover the input signal. So if you imagine per channel,\nyou have a sliding window that's producing some Commvault feature for each of those input sequences for every single channel simultaneously. You can actually get\nbetter validation metrics, meaning you're fitting the data better, and it's generalizing\nbetter on offline data if you use this\nconvolutional architecture. You're reducing parameters. It's sort of a standard procedure when you're dealing with time series data. Now it turns out that when\nusing that model online, the controllability was worse, was far worse, even though the\noffline metrics were better. And there can be many\nways to interpret that, but what that taught me at least was that, hey, it's at least the case right now that if you were to just throw a bunch of computer at this problem, and you were trying to sort\nof hyper parameter optimize, or let some GPT model hard code or come up with or invent\nmany different solutions, if you were just optimizing for loss, it would not be sufficient, which means that there's still some inherent modeling gap here. There's still some artistry\nleft to be uncovered here of how to get your model\nto scale with more compute. And that may be fundamentally\nlabeling problem, but there may be other\ncomponents to this as well. - Is it data constrained at this time? Which is what it sounds like. How do you get a lot of good labels? - Yeah, I think it's\ndata quality constrained, not necessarily data quantity constrained. - But even like even just a quantity. I mean, 'cause it has to be\ntrained on the interactions. I guess there's not\nthat many interactions. - Yeah, so it depends what version of this you're talking about. So if you're talking about like, let's say the simplest\nexample of just 2D velocity, then I think, yeah, data\nquality is the main thing. If you're talking about how to build a sort of multifunction output that lets you do all the inputs, the computer that you and I can do, then it's actually a much more sophisticated nuanced modeling challenge, because now you need to think about not just when the user's left clicking, but when you're building\nthe left click model, you also need to be thinking about how to make sure it doesn't fire when they're trying to right click or when they're trying to move the mouse. So one example of an interesting bug from like sort of week one\nof a BCI with Nolan was, when he moved the mouse, the click signal sort\nof dropped off a cliff, and when he stopped, the\nclick signal went up. So again, there's a contamination\nbetween the two inputs. Another good example was, at\none point, he was trying to do sort of a left click and drag. And the minute he started moving, the left click signal dropped off a cliff. So again, 'cause there's\nsome contamination between the two signals, you\nneed to come up with some way to either in the dataset or in the model, build robustness against this kind of, you think of it like\noverfitting, but really, it's just that the model has not seen this kind of variability before. So you need to find some way\nto help the model with that. - This is super cool because it feels like all of this is very\nsolvable, but it's hard. - Yes, it is fundamentally\nan engineering challenge. This is important to emphasize, and it's also important to emphasize that it may not need\nfundamentally new techniques, which means that people\nwho work on, let's say, unsupervised speech classification using CTC loss, for example,\nwith internal to Siri, they could potentially have\nvery applicable skills to this. - So what things are you excited about in the future development of the software stack on Neuralink? So everything we've been talking about, the decoding, the UX- - I think there's some I'm excited about, like something I'm excited\nabout from the technology side, and some I'm excited\nabout for understanding how this technology is\ngoing to be best situated for entering the world. So I'll work backwards. On the technology entering\nthe world side of things, I'm really excited to\nunderstand how this device works for folks that cannot speak at all. They have no ability to\nsort of bootstrap themselves into useful control by\nvoice command, for example, and are extremely limited in\ntheir current capabilities. I think that will be an\nincredibly useful signal for us to understand, I mean, really\nwhat is an existential threat for all startups, which\nis product market fit. Does this device have the capacity and potential to transform people's lives in the current state? And if not, what are the gaps? And if there are gaps, how do we solve them most efficiently? So that's what I'm very\nexcited about for the next sort of year or so of\nclinical trial operations. The technology side, I'm quite excited about\nbasically everything we're doing. I think it's gonna be awesome. The most prominent one, I would say, is scaling channel count. So right now, we have a\nthousand channel device. The next version, we'll have\nbetween 3 and 6,000 channels. And I would expect that curve\nto continue in the future. And it's unclear what set of problems will just disappear\ncompletely at that scale, and what set of problems will remain and require for their focus. And so I'm excited about\nthe clarity of gradient that that gives us in terms\nof the user experiences we choose to focus our\ntime and resources on. And also in terms of the, yeah, even things as simple as not stationary. Like does that problem just completely go away at that scale, or do we need to come\nup with new creative UXs still even at that point? And also, when we get to that time point, when we start expanding out dramatically the set of functions that you\ncan output from one brain, how to deal with all the nuances\nof both the user experience of not being able to\nfeel the different keys under your fingertips, but\nstill needing to be able to modulate all of them in synchrony to achieve the thing you want. And again, you don't have that appropriate set to feedback loop,\nso how can you make that intuitive for a user to control a high dimensional control surface without feeling the thing physically? I think that's gonna be a\nsuper interesting problem. I'm also quite excited to understand, do these scaling laws continue? Like as you scale channel count, how much further out do you go before that saturation point is truly hit? And it's not obvious today. I think we only know what's in the sort of interpolation space. We only know what's\nbetween zero and 1,024. We don't know what's beyond that. And then there's a whole sort of like range of interesting sort of neuroscience and brain questions, which\nis when you stick more stuff in the brain, in more places, you get to learn much more quickly about what those brain regions represent. And so I'm excited about that fundamental neuroscience learning,\nwhich is also important for figuring out how,\nand to most efficiently, insert electrodes in the future. So yeah, I think all those dimensions, I'm really, really excited about. And that doesn't even\nget close to touching the sort of software stack that\nwe work on every single day and what we're working on right now. - Yeah, it seems\nvirtually impossible to me that a thousand electrodes\nis where it saturates. It feels like this would be one of those silly notions in the\nfuture, where obviously, you should have millions of electrodes, and this is where like the\ntrue breakthroughs happen. - Yeah. - You tweeted-\n- Oh. - \"Some thoughts are most\nprecisely described in poetry.\" Why do you think that is? - I think it's because\nthe information bottleneck of language is pretty steep. And yet you're able to reconstruct in the other person's\nbrain more effectively without being literal. If you can express a sentiment\nsuch that in their brain, they can reconstruct the\nactual true underlying meaning and beauty of the thing that you're trying to get across. The sort of the generator function in their brain's more powerful than what language can express. And so the mechanism of\npoetry is really just to feed or seed that generator function. - So being literal sometimes\nis a suboptimal compression for the thing you're trying to convey. - And it's actually in\nthe process of the user going through that generation that they understand what you mean. That's the beautiful part. It's also like when you look\nat a beautiful painting, like it's not the pixels of the\npainting that are beautiful, it's the thought process that\noccurs when you see that, the experience of that. That actually is a thing that matters. - Yeah, it's resonating with some deep- - [Bliss] Yeah. - Thing within you that\nthe artist also experienced and was able to convey\nthat through the pixels. And that's actually gonna be\nrelevant for full on telepathy. It's like if you just read\nthe poetry, literally, that doesn't say much\nof anything interesting. It requires a human to interpret it. So it's the combination of the human mind and all the experiences\nthat human being has within the context of the\ncollective intelligence of the human species that\nmakes that poem make sense. And they load that in. And so in that same way,\nthe signal that carries from human to human\nmeaning may seem trivial, but may actually carry a lot of power because of the complexity of the human mind on the receiving end. Yeah, that's interesting. Poetry still doesn't, who is it? I think Yoshi Bako first said something about all the people that think we've achieved AGI explain\nwhy humans like music. - [Bliss] Oh yeah. - And until the AGI likes music, you haven't achieved AGI or something like this.\n- Do you not think that's like some next\ntoken entropy surprise kind of thing going on there? - I don't know.\n- I don't know either. I listen to a lot of classical music and also read a lot of poetry. And yeah, I do wonder if\nlike there is some element of the next token surprise\nfactor going on there. - Yeah, maybe.\n- Because I mean, like a lot of the tricks\nin both poetry and music are like basically, you have\nsome repeated structure. And then you do like a twist. It's like, okay, verse or\nlike clause one, two, three is one thing, and then\nclause four is like, okay, now we're onto the next theme. And they kind of play with\nexactly when the surprise happens and the expectation of the user. And that's even true like, through history as musicians evolve music, they take like some nuanced structure that people are familiar with and they just tweak it a little bit. Like they tweak it and\nadd a surprising element. This is especially true in\nclassical music heritage. But that's what I'm wondering,\nlike is it all just entropy- - So breaking structure\nor breaking symmetry is something that humans seem to like. Maybe as simple as that. - Yeah, and I mean, great artists copy, and they also, knowing\nwhich rules to break is the important part. And that fundamentally, it must be about the listener of the piece. Like which rule is the right one to break, it's about the user or the audience member perceiving that as interesting. - What do you think is the\nmeaning of human existence? - There's a TV show I really\nlike called \"The West Wing.\" And in \"The West Wing,\"\nthere's a character. He's the president of the United States who's having a discussion about the Bible with one of their colleagues. And the colleague says something about, \"The Bible says X, Y, and Z.\" And the President says, \"Yeah,\nbut it also says A, B, C.\" And the person says, \"Do you believe the Bible\nto be literally true?\" And the President says,\n\"Yes, but I also think that neither of us are smart\nenough to understand it.\" I think the analogy here\nfor the meaning of life is that largely, we don't know\nthe right question to ask. And so I think I'm very aligned with sort of \"The Hitchhiker's\nGuide to the Galaxy\" version of this question, which is basically, if we can ask the right questions, it's much more likely we find the meaning of human existence. And so in the short term, as a heuristic in the sort\nof search policy space, we should try to increase the diversity of people asking such questions, or generally of consciousness and conscious beings\nasking such questions. So again, I think I'll take\nthe 'I don't know card' here, but say I do think there are\nmeaningful things we can do that improve the likelihood\nof answering that question. - It's interesting how\nmuch value you assign to the task of asking the right questions. That's the main thing is not the answers, it's the questions. - This point by the way is driven home in a very painful way when\nyou try to communicate with someone who cannot speak,\nbecause a lot of the time, the last thing to go is\nthey have the ability to somehow wiggle a lip or move something that allows them to say yes or no. And in that situation, it's very obvious that what matters is, are you asking them the right question to be\nable to say yes or no to? - Wow, that's powerful. Well, Bliss, thank you\nfor everything you do, and thank you for being you, and thank you for talking today. - Thank you. - Thanks for listening to this conversation with Bliss Chapman. And now, dear friends,\nhere's Noland Arbaugh, the first human being to\nhave a Neuralink device implanted in his brain. You had a diving accident in\n2016 that left you paralyzed with no feeling from the shoulders down. How did that accident change your life? - That's sort of a freak\nthing that happened. Imagine you're running into the ocean, although this is a lake, but\nyou're running into the ocean and you get to about waist high, and then you kind of like dive in, take the rest of the plunge\nunder the wave or something. That's what I did. And then I just never came back up. Not sure what happened. I did it running into the\nwater with a couple of guys. And so my idea of what happened is really just that I\ntook like a stray fist, elbow, knee, foot, something\nto the side of my head. The left side of my head was sore for about a month afterwards. So must must've taken a pretty big knock. And then they both came up, and I didn't. And so I was face down\nin the water for a while. I was conscious. And then eventually just realized I couldn't hold my breath any longer. And I keep saying, \"Took a big drink.\" People, I don't know if\nthey like that I say that. It seems like I'm making light of it all, but it's just kind of how I am. And I don't know, like I'm a very relaxed sort of stress-free person. I rolled with the punches\nfor a lot of this. I kind of took it in stride. It's like, \"All right,\nwell what can I do next? How can I improve my\nlife even a little bit on a day-to-day basis?\" At first, just trying\nto find some way to heal as much of my body as possible, to try to get healed, to\ntry to get off a ventilator, learn as much as I could\nso I could somehow survive once I left the hospital. And then thank God I had\nlike my family around me. If I didn't have my parents, my siblings, then I would've never made it this far. They've done so much for me, more than like I can ever\nthank them for, honestly. And a lot of people don't have that. A lot of people in my situation, their families either aren't\ncapable of providing for them, or honestly, just don't want to. And so they get placed\nsomewhere in some sort of home. So thankfully I had my family. I have a great group of friends, a great group of buddies from college who have all rallied around me, and we're all still incredibly close. People always say, \"If you're lucky, you'll end up with one or\ntwo friends from high school that you keep throughout your life.\" I have about 10 or 12 from high school that have all stuck around, and we still get together,\nall of us twice a year. We call it the spring\nseries and the fall series. This last one we all did,\nwe dressed up like X-Men. So I did a Professor Xavier,\nand it was freaking awesome. It was so good. So yeah, I have such a great\nsupport system around me. And so being a quadriplegic\nisn't that bad. I get waited on all the time. People bring me food and\ndrinks, and I get to sit around and watch as much TV and\nmovies and anime as I want. I get to read as much as I want. I mean, it's great. - It's beautiful to see that you see the silver lining in all of this. Just going back, do\nyou remember the moment when you first realized you were paralyzed from the neck down?\n- Yeah, yep. I was face down in the water right when whatever something hit my head. I tried to get up and I\nrealized I couldn't move and it just sort of clicked. I'm like, \"All right, I'm paralyzed. Can't move. What do I do? If I can't get up, I can't flip over, can't do anything, then I'm\ngonna drown eventually.\" And I knew I couldn't\nhold my breath forever, so I just held my breath\nand thought about it for maybe 10, 15 seconds. I've heard from other people\nthat like look on liquors, I guess the two girls that\npulled me out of the water were two of my best friends. They're lifeguards. And one of them said that\nit looked like my body was sort of shaking in the water, like I was trying to flip over and stuff. But I knew, I knew immediately. And I just kind of, I realized that that's\nwhat my situation was from here on out. Maybe if I got to the hospital, they'd be able to do something. When I was in the hospital, like right before surgery, I was trying to calm\none of my friends down. I had like brought her with\nme from college to camp, and she was just bawling\nover me, and I was like, \"Hey, it's gonna be fine. Like don't worry.\" I was cracking some jokes\nto try to lighten the mood. The nurse had called\nmy mom, and I was like, \"Don't tell my mom. She's just gonna be stressed out. Call her after I'm out of surgery,\" 'cause at least she'll\nhave some answers then, like whether I live or not, really. And I didn't want her to be stressed through the whole thing. But I knew. And then when I first\nwoke up after surgery, I was super drugged up. They had me on fentanyl like\nthree ways, which was awesome. I don't recommend it, but\nI saw some crazy stuff on that fentanyl, and it was still the best I've ever felt on drugs. Medication, sorry, on medication. And I remember the first time\nI saw my mom in the hospital. I was just bawling. I had like ventilator in, like I couldn't talk or anything, and I just started crying, because it was more like seeing her. I mean, the whole situation\nobviously was pretty rough, but it was just like seeing her face for the first time was pretty hard. But yeah, I never had like a moment of, \"Man, I'm paralyzed. This sucks. I don't wanna like be around anymore.\" It was always just, \"I hate\nthat I have to do this, but like sitting here and\nwallowing isn't gonna help.\" - So immediate acceptance. - [Noland] Yeah, yeah. - Has there been low points along the way? - Yeah, yeah, sure. I mean, there are days\nwhen I don't really feel like doing anything. Not so much anymore. Like not for the last couple years, I don't really feel that way. I've more so just wanted to\ntry to do anything possible to make my life better at this point. But at the beginning, there\nwere some ups and downs. There were some really\nhard things to adjust to. First off, just like\nthe first couple months, the amount of pain I was\nin was really, really hard. I mean, I remember screaming at the top of my lungs in the hospital, because I thought my legs were on fire. And obviously, I can't feel anything, but it's all nerve pain. And so that was a really hard night. I asked them to give me as\nmuch pain meds as possible. They're like, \"You've had\nas much as you can have, so just kind of deal with it. Go to a happy place sort of thing.\" So that was a pretty low point. And then every now and again, it's hard, like realizing things that\nI wanted to do in my life that I won't be able to do anymore. I always wanted to be\na husband and father, and I just don't think\nthat I could do it now as a quadriplegic. Maybe it's possible, but\nI'm not sure I would ever put someone I love through that, like having to take care of me and stuff. Not being able to go out and play sports. I was a huge athlete growing\nup, so that was pretty hard. Little things too, when I\nrealized I can't do them anymore. Like there's something really special about being able to hold\na book and smell a book. Like the feel, the texture, the smell, like as you turn the\npages, like I just love it. I can't do it anymore. And it's little things like that. The two-year mark was pretty rough. Two years is when they say\nyou will get back basically as much as you're ever gonna get back, as far as movement and sensation goes. And so for the first two years, that was the only thing\non my mind was like try as much as I can to move my fingers, my hands, my feet, everything possible to try to get sensation and movement back. And then when the two-year mark hit, so June 30th, 2018, I was really sad that\nthat's kind of where I was. And then just randomly here and there, but I was never like depressed\nfor long periods of time. Just it never seemed worthwhile to me. - What gave you strength? - My faith. My faith in God was a big one. My understanding that it\nwas all for a purpose. And even if that purpose wasn't anything involving Neuralink,\neven if that purpose was, there's a story in the Bible about Job, and I think it's a really,\nreally popular story, about how Job has all\nof these terrible things happen to him, and he praises God throughout the whole situation. I thought, and I think\na lot of people think for most of their lives that they are Job, that they're the ones going\nthrough something terrible, and they just need to praise\nGod through the whole thing and everything will work out. At some point after my accident, I realized that I might not be Job, that I might be one of his children that gets killed or\nkidnapped or taken from him. And so it's about terrible\nthings that happen to those around you who you love. So maybe, in this case,\nmy mom would be Job, and she has to get through\nsomething extraordinarily hard and I just need to try and make it as best as possible for her, because she's the one that's really going through this massive trial. And that gave me a lot of strength. And obviously, my family. My family and my friends, they give me all the strength that I need on a day-to-day basis. So it makes things a lot easier having that great\nsupport system around me. - From everything I've seen of you online, your streams and the way you\nare today, I really admire, let's say, your unwavering\npositive outlook on life. Has that always been this way? - Yeah, yeah. I mean, I've just always thought I could do anything I ever wanted to do. There was never anything too big. Like whatever I set my mind\nto, I felt like I could do it. I didn't wanna do a lot. I wanted to like travel around and be sort of like a gypsy\nand like go work odd jobs. I had this dream of\ntraveling around Europe and being like, I don't know, a shepherd in like Wales or Ireland. And then going and being\na fisherman in Italy, doing all these things for like a year. Like it's such like cliche things, but I just thought it would be so much fun to go and travel and do different things. And so I've always just seen the best in people around me too. And I've always tried\nto be good to people. And growing up with my mom too, she's like the most positive\nenergetic person in the world. And we're all just people people. I just get along great with people. I really enjoy meeting new people, and so I just wanted to do everything. This is just kind of just how I've been. - It's just great to see that\ncynicism didn't take over, given everything you've been through. - Yeah. - Was that like a\ndeliberate choice you made that you're not gonna\nlet this keep you down? - Yeah, a bit. Also, like it's just kind of how I am. Like I said, I roll with\nthe punches with everything. I always used to tell people, like I don't stress about things much. And whenever I'd see\npeople getting stressed, I would just say, \"It's not hard. Just don't stress about it.\" And that's all you need to do. And they're like, \"That's\nnot how that works.\" I'm like, \"It works for me. Like just don't stress, and\neverything will be fine. Like everything will work out.\" Obviously, not everything\nalways goes well, and it's not like it all works out for the best all the time, but I just don't think stress has had any place in my life since I was a kid. - What was the experience like of you being selected to\nbe the first human being to have a Neuralink device\nimplanted in your brain? Were you scared, excited?\n- No, no, it was cool. (Lex laughing) Like I was never afraid of it. I had to think through a lot. Should I do this? Like be the first person? I could wait until number two or three and get a better version of the Neuralink. Like the first one might not work. Maybe it's actually gonna kind of suck it. It's gonna be the worst\nversion ever in a person. So why would I do the first one? Like I've already kind of been selected. I could just tell them, like,\n\"Okay, find someone else, and then I'll do number two or three.\" Like I'm sure they would let me. They're looking for a few people anyways. But ultimately, I was like, I don't know, there's something about being\nthe first one to do something. It's pretty cool. I always thought that if I had the chance that I would like to do\nsomething for the first time, this seemed like a\npretty good opportunity, and I was never scared. I think my like faith\nhad a huge part in that. I always felt like God was\npreparing me for something. I almost wish it wasn't this, because I had many conversations with God about not wanting to do any\nof this as a quadriplegic. I told him, \"I'll go\nout and talk to people. I'll go out and travel the world and talk to stadiums, thousands of people, give my testimony, I'll do all of it, but like heal me first. Don't make me do all of this in a chair. That sucks.\" And I guess he won that argument. I didn't really have much of a choice. I always felt like there\nwas something going on. And to see how, I guess,\neasily I made it through the interview process and how quickly everything happened, how the stars sort of\naligned with all of this, it just told me like, as the\nsurgery was getting closer, it just told me that it\nwas all meant to happen. It was all meant to be. And so I shouldn't be afraid\nof anything that's to come. And so I wasn't. I kept telling myself like, \"You say that now, but as\nsoon as the surgery comes, you're probably gonna be freaking out. Like you're about to have brain surgery.\" And brain surgery is a big\ndeal for a lot of people, but it's a even bigger deal for me. Like it's all I have left. The amount of times I've been like, \"Thank you God that you\ndidn't take my brain and my personality and\nmy ability to think, my like love of learning, like my character, everything,\nlike thank you so much. Like as long as you left me that, then I think I can get by.\" And I was about to let\npeople go like root around, and they're like, \"Hey, we're gonna go put some stuff in your brain. Hopefully, it works out.\" And so it was something\nthat gave me pause. But like I said, how\nsmoothly everything went, I never expected for a second\nthat anything would go wrong. Plus, the more people I\nmet on the Barrow's side and on the Neuralink side, they're just the most\nimpressive people in the world. Like I can't speak enough to how much I trust\nthese people with my life and how impressed I am with all of them. And to see the excitement on their faces, to like walk into a room\nand roll into a room and see all of these people looking at me, like we're so excited. Like we've been working so hard on this, and it's finally happening. It's super infectious, and it just makes me wanna do it even more and to help them achieve their dreams. Like I don't know, it's so rewarding. And I'm so happy for\nall of them, honestly. - What was the day of surgery like? When did you wake up? What'd you feel? - Yeah.\n- Minute by minute. - Yeah.\n- Were you freaking out? - No, no. I thought I was going to, but as surgery approached\nthe night before, the morning of, I was just excited. Let's make this happen. I think I said something like that to Elon on the phone beforehand. We were like FaceTiming, and I was like, \"Let's rock and roll.\" And he's like, \"Let's do it.\" I don't know, I wasn't scared. So we woke up. I think we had to be at the\nhospital at like 5:30 AM. I think surgery was at like 7:00 AM. So we woke up pretty early. I'm not sure much of us slept that night. Got to the hospital 5:30, went through like all the pre-op stuff. Everyone was super nice. Elon was supposed to be\nthere in the morning, but something went wrong with his plane, so we ended up FaceTiming. That was cool. Had one of the greatest\none-liners of my life. After that phone call, hung up with him. There were like 20 people\naround me, and I was like, I just hope he wasn't too\nstarstruck talking to me. - Nice.\n- Yeah, it was good. - Well done.\n- Yeah, yeah. - Did you write that ahead of\ntime, or it just came to you? - No, it just came to me. I was like, \"This seems right.\" When in surgery, I asked if I could pray right beforehand. So I like prayed over the room. I asked God if you would\nlike be with my mom in case anything happened to me. And just to like calm\nher nerves out there. Woke up and played a bit\nof a prank on my mom. I don't know if you've heard about it. - Yeah, I read about it. - Yeah, she was not happy. - Can you take me through the prank - Yeah, this is something-\n- Do you regret doing that now?\n- No, no, not one bit. It was something I had talked about ahead of time with my buddy, Bain. I was like, \"I would really\nlike to play a prank on my mom.\" Very specifically, my mom. She's very gullible. I think she had knee surgery once even. And after she came out of knee surgery, she was super groggy. She's like, \"I can't feel my legs.\" And my dad looked at her, he was like, \"You don't have any legs. Like they had to amputate both your legs.\" We just do very mean\nthings to her all the time. I'm so surprised that she still loves us. But right after surgery,\nI was really worried that I was going to be too like\ngroggy, like not all there. I had anesthesia once\nbefore, and it messed me up. Like I could not function\nfor a while afterwards. And I like said a lot of\nthings that I was like, I was really worried\nthat I was gonna start, I don't know, like dropping some bombs. And I wouldn't even know,\nI wouldn't remember. So I was like, \"Please\nGod, don't let that happen. And please let me be there\nenough to do this to my mom.\" And so she walked in after surgery. It was like the first time they had been able to\nsee me after surgery. And she just looked at me, she said, \"Hi, like, how are you? How are you doing? How do you feel?\" And I looked at her in this very, I think the anesthesia helped, very like groggy, sort of\nconfused look on my face. It's like, \"Who are you?\" And she just started\nlooking around the room, like at the surgeons,\nat the doctors, like, \"What did you do to my son? You need to fix this right now.\" Tears started streaming. I saw how much she was freaking out. I was like, \"I can't let this go on.\" And so I was like, \"Mom, I'm fine. Like it's all right.\" And still, she was not happy about it. She still says she's\ngonna get me back someday. But I mean, I don't know. I don't know what that's gonna look like. - It's a lifelong battle.\n- Yeah, it was good. - In some sense, it was a demonstration that you still got- - That's all I wanted it to be\n- Sense of humor. - That's all I wanted it to be. And I knew that doing\nsomething super mean to her like that would show her-\n- - - Yeah.\n- To show that you're still there, that you love her. - [Noland] Yeah, exactly, exactly. - It's a dark way to do it, but I love it. What was the first time\nyou were able to feel that you can use the Neuralink device to effect the world around you? - Yeah, the first little taste I got of it was actually not too long after surgery. Some of the Neuralink team had brought in like a little iPad, a\nlittle tablet screen, and they had put up\neight different channels that were recording some\nof my neuron spikes. They put it in front of me. Like this is like real\ntime your brain firing. That's super cool. My first thought was, \"I\nmean, if they're firing now, let's see if I can\naffect them in some way.\" So I started trying to\nlike wiggle my fingers and I just started like\nscanning through the channels, and one of the things I was doing was like moving my index\nfinger up and down. And I just saw this yellow\nspike on like top row, like third box over or something. I saw this yellow spike\nevery time I did it. And I was like, \"Oh, that's cool.\" And everyone around me was just like, \"Well, what are you seeing?\" I was like, \"Look, look at this one. Look at like this top row, third\nbox over this yellow spike. Like that's me right there, there, there.\" And everyone was freaking out. They started like clapping. I was like, \"That's super unnecessary.\" - That's awesome.\n- This is what's supposed to happen, right? - So you're imagining yourself moving each individual finger one at a time, and then seeing like that\nyou can notice something, and then when you did the index\nfinger, you're like, \"Oh.\" - Yeah, I was wiggling\nkind of all of my fingers to see if anything would happen. There was a lot of other things going on, but that big yellow spike was\nthe one that stood out to me. Like I'm sure that if I would've\nstared at it long enough, I could have mapped out\nmaybe 100 different things. But the big yellow spike\nwas the one that I noticed. - Maybe you could speak to what it's like to sort of wiggle your fingers, to imagine that the mental,\nthe cognitive effort required to sort of wiggle your\nindex finger, for example. How easy is that to do? - Pretty easy for me. It's something that,\nat the very beginning, after my accident, they told me to try and move my body as much as possible. Even if you can't, just keep trying, because that's going to create\nnew like neural pathways or pathways in my spinal cord\nto like reconnect these things to hopefully regain some movement someday. - That's fascinating.\n- Yeah, I know. It's bizarre, but I- - So that's part of the recovery process is to keep trying to move your body? - Yep, as much as you can.\n- And that's, and the nervous system does its thing. It starts reconnecting.\n- Yeah. It'll start reconnecting for some people. Some people, it never works. Some people, they'll do it. Like for me, I got some\nbicep control back, and that's about it. I can, if I try enough, I can wiggle some of my fingers. Not like on command. It's more like if I try to move, say my right pinky and I\njust keep trying to move it, after a few seconds, it'll wiggle. So I know there's stuff there. Like I know, and that happens with a few - -different of my fingers and stuff. But yeah, that's what they tell you to do. One of the people at the time when I was in the hospital\ncame in and told me, for one guy who had recovered\nmost of his control, what he thought about every\nday was actually walking, like the act of walking\njust over and over again. So I tried that for years. I tried just imagining\nwalking, which it's hard. It's hard to imagine like all\nof the steps that go into, well, taking a step, like all of the things that have to move, like all of the activations\nthat have to happen along your leg in order\nfor one step to occur. - But you're not just imagining. You're like doing it, right? - I'm trying, yeah. So it's like, it's imagining over again what I had to do to take a step, because it's not something\nany of us think about. You wanna walk and you take a step. You don't think about all\nof the different things that are going on in your body. So I had to recreate that in\nmy head as much as I could. And then I practice it\nover and over and over. - So it's not like a\nthird person perspective. It's a first person perspective. You're like, it's not like you're imagining yourself walking. You're like literally\ndoing this, everything, all the same stuff as if you're walking. - Yeah, which was hard. It was hard at the beginning. - Like frustrating hard, or like actually cognitively hard? Like which way?\n- It was both. There's a scene in one of the\n\"Kill Bill\" movies actually, oddly enough, where she is like paralyzed, I don't know, from like a\ndrug that was in her system. And then she like finds some way to get into the back of a truck or something, and she stares at her\ntoe and she says, \"Move.\" Like move your big toe. And after a few seconds\non screen, she does it. And she did that with every\none of her like body parts until she can move again. I did that for years, just\nstared at my body and said, \"Move your index finger. Move your big toe.\" Sometimes, vocalizing it like out loud, sometimes just thinking it. I tried every different way to do this to try to get some movement back. And it's hard because it\nactually is like taxing, like physically taxing on my body, which is something I\nwould've never expected, 'cause it's not like I'm moving, but it feels like there's\na buildup of, I don't know, the only way I can describe\nit is there are like signals that aren't getting\nthrough from my brain down, 'cause there's that gap in my spinal cord. So brain down, and then from\nmy hand back up to the brain. And so it feels like those signals get stuck in whatever body\npart that I'm trying to move. And they just build up and build up and build up until they burst. And then once they burst, I\nget like this really weird sensation of everything\nsort of like dissipating back out to level, and then I do it again. It's also just like a fatigue\nthing, like a muscle fatigue, but without actually moving your muscles. It's very, very bizarre. And then if you try to\nstare at a body part or think about a body part and move for two, three,\nfour, sometimes eight hours, it's very taxing on your mind. It's takes a lot of focus. It was a lot easier at the beginning because I wasn't able to like control a TV in my room or anything. I wasn't able to control\nany of my environment. So for the first few years, a lot of what I was doing\nwas staring at walls. And so obviously, I did a lot of thinking, and I tried to move a lot just\nover and over and over again. - So you never gave up sort of hope there? - No.\n- Just training hard essentially?\n- Yep, and I still do it. I do it like subconsciously. And I think that that helped a lot with things with Neuralink, honestly. It's something that I\ntalked about the other day at the all hands that I did at\nNeuralink's Austin facility. - Welcome to Austin, by the way. - Yeah, hey, thanks man. I went to school-\n- Nice hat. - Hey, thanks, thanks man. The gigafactory was super cool. I went to school at Texas A&M, so I've been around before. - So you should be saying welcome to me. - Yeah.\n- Welcome to Texas, Lex. Yeah, I get you. - But yeah, I was talking about how a lot of what they've had me do, especially at the beginning, well, I still do it now is body mapping. So like there will be a\nvisualization of a hand or an arm on the screen and\nI have to do that motion, and that's how they sort\nof train the algorithm to like understand what I'm trying to do. And so it made things very\nseamless for me, I think. - That's really, really cool. So yeah, it's amazing to know 'cause I've learned a lot about\nthe body mapping procedure with the interface and\neverything like that. It's cool to know that\nyou've been essentially like training to be like\nworld class at that task. - Yeah, yeah. I don't know if other quadriplegics, like other paralyzed people give up. I hope they don't. I hope they keep trying, because I've heard other\nparalyzed people say, like don't ever stop. They tell you two years,\nbut you just never know. The human body is capable\nof amazing things. So I've heard other people\nsay, \"Don't give up.\" Like I think one girl had spoken to me through some family members and said that she had been\nparalyzed for 18 years, and she'd been trying to\nlike wiggle her index finger for all that time, and\nshe finally got it back like 18 years later. So like I know that it's possible, and I'll never give up doing it. I do it when I'm lying down. Like watching TV, I'll\nfind myself doing it, kind of just almost like on its own. It's just something I've\ngotten so used to doing that I don't know, I don't\nthink I'll ever stop. - That's really awesome to hear, 'cause I think it's one of those things that can really pay off, in the long term. 'Cause like that is training. You're not visibly seeing the results of that training at the moment, but like, there's that like Olympic\nlevel nervous system getting ready for something.\n- Which honestly was like something that\nI think Neuralink gave me that I can't thank them enough for it. Like I can't show my\nappreciation for it enough was being able to visually see that what I'm doing is\nactually having some effect. It's a huge part of the reason why, like I know now that I'm\ngonna keep doing it forever, because before Neuralink,\nI was doing it every day, and I was just assuming\nthat things were happening. Like it's not like I knew. I wasn't getting back any\nmobility or sensation or anything. So I could have been running up against a brick wall for all I knew. And with Neuralink, I get to see like all the signals happen in real time, and I get to see that what I'm doing can actually be mapped. When we started doing like\nclick calibrations and stuff, when I go to click my index\nfinger for a left click, that it actually recognizes that. Like it changed how I\nthink about what's possible with like retraining my body to move. And so yeah, I'll never give up now. - And also, just the signal\nthat there's still a powerhouse of a brain there that's like- - Exactly.\n- And as the technology develops, that brain is, I mean, that's the most important thing about the human body is the brain. And it can do a lot of the control. So what did it feel like when you first, could wiggle the index finger and saw the environment respond? Like that little-\n- Yeah. - Wherever, just being way\ntoo dramatic according to you. - Yeah, it was very cool. I mean, it was cool, but I\nkeep telling this to people, it made sense to me. Like it made sense that like there are signals\nstill happening in my brain, and that as long as you\nhad something near it that could measure those,\nthat could record those, then you should be able to\nvisualize it in some way. Like see it happen. And so that was not very surprising to me. I was just like, \"Oh, cool.\" We found one. Like we found something that works. It was cool to see that\ntheir technology worked, and that everything that\nthey had worked so hard for was like going to pay off. But I hadn't like moved a cursor\nor anything at that point. I had like interacted with a computer or anything at that point. So it just made sense, it was cool. I didn't really know much\nabout BCI at that point either, so I didn't know like what sort of step this was actually making. Like I didn't know if\nthis was like a huge deal, or if this was just like, okay, it's cool that we got this far, but we're actually hoping for something like much\nbetter down the road. It's like, okay. I just thought that they\nknew that it turned on. So I was like, cool. Like this is cool. - Well, did you like read up on the specs of the hardware you're getting installed? Like the number of threads, this kind of stuff?\n- Yeah, I do all of that, but it's all Greek to me. I was like, okay, threads, 64 threads, 16 electrodes, 1,024 channels. Okay. Like that math checks out. - Sounds right.\n- Yeah. - When was the first time you were able to move a mouse cursor? - I know it must have been\nwithin the first maybe week, a week or two weeks that I was able to like first move the cursor. And again, like it kind\nof made sense to me. It didn't seem like that big of a deal. It was like, okay, well, hmm, how do I explain this? When everyone around you starts clapping for something that you've\ndone, it's easy to say, \"Okay, like I did something cool. Like that was impressive in some way.\" What exactly that meant, what it was hadn't really like set in for me. So again, I knew that me\ntrying to move a body part and then that being mapped\nin some sort of like machine learning algorithm\nto be able to identify like my brain signals and then take that and give me cursor control, that all kind of made sense to me. I don't know like all\nthe ins and outs of it, but I was like, there are still\nsignals in my brain firing. They just can't get through because there's like a\ngap in my spinal cord. And so they can't get all\nthe way down and back up, but they're still there. So when I moved the cursor for\nthe first time, I was like, \"That's cool, but I expected\nthat that should happen.\" Like it made sense to me. When I moved the cursor for the first time with just my mind, without\nlike physically trying to move, so I guess I can get into\nthat just a little bit. Like the difference between attempted movement and imagined movement. - Yeah, that's a fascinating difference. - Yeah.\n- From one to the other. - Yeah, yeah, yeah. So like attempted movement\nis me physically trying to attempt to move, say, my hand. I try to attempt to move\nmy hand to the right, to the left, forward and back. And that's all attempted. Attempt to like lift\nmy finger up and down. Attempt to kick or something. I'm physically trying to\ndo all of those things, even if you can't see it. This would be like me attempting to like shrug my shoulders or something. That's all attempted movement. That's what I was doing for\nthe first couple of weeks when they were going to\ngive me cursor control. When I was doing body mapping, it was attempt to do\nthis, attempt to do that. When Nir was telling me\nto like imagine doing it, it like kind of made sense to me, but it's not something\nthat people practice. Like if you started school\nas a child and they said, \"Okay, write your name with this pencil.\" And so you do that. \"Okay, now imagine writing\nyour name with that pencil.\" Kids would think like, I guess like that kind of makes sense. And they would do it. But that's not something we're taught. It's all like how to do things physically. We think about like thought\nexperiments and things, but that's not like a physical\naction of doing things. It's more like what you would\ndo in certain situations. So imagined movement, it never\nreally connected with me. Like I guess you could maybe describe it as like a professional athlete, like swinging a baseball bat\nor swinging like a golf club. Like imagine what you're supposed to do. But then you go right to\nthat and physically do it, then you get a bat in your hand and then you do what\nyou've been imagining. And so I don't have that like connection. So telling me to imagine\nsomething versus attempting it, there wasn't a lot that I\ncould do there mentally. I just kind of had to accept\nwhat was going on and try. But the attempted moving\nthing, it all made sense to me. Like if I try to move, then there's a signal\nbeing sent in my brain. And as long as they can pick that up, then they should be able to map\nit to what I'm trying to do. And so when I first moved\nthe cursor like that, it was just like, \"Yes,\nthis should happen.\" Like I'm not surprised by that. - But can you clarify, is there\nsupposed to be a difference between imagined movement\nand attempted movement? - Yeah, just that in imagined movement, you're not attempting to move at all. - You're like visualizing doing. And then theoretically, is\nthat supposed to be a different part of the brain that lights up in those two different situations? - [Bliss] Yeah, not necessarily. I think all these signals\ncan still be represented in motor cortex, but the\ndifference I think has to do with the naturalness\nof imagining something versus attempting it-\n- Got it. - [Bliss] And sort of the\nfatigue of that over time. - And by the way, on the mic is Bliss. So this is just different\nways to prompt you to kind of get to the\nthing that you arrived at. - [Noland] Yeah, yeah. - Attempted movement does sound\nlike the right thing - try. - Yeah, I mean, it makes sense to me. - 'Cause imagine for me,\nI would start visualizing, like in my mind, visualizing. Attempted, I would actually\nstart trying to like, there's, I mean, I did like combat sports my\nwhole life, like wrestling. When I'm imagining a move,\nsee, I'm like moving my muscle. - Exactly.\n- Like there is a bit of an activation almost,\nversus like visualizing yourself like a picture doing it. - Yeah, it's something that I feel like naturally, anyone would do. If you try to tell someone\nto imagine doing something, they might close their eyes and then start physically doing it. But it's just-\n- Just didn't click. - Yeah. It's hard. It was very hard at the beginning. - But attempted worked.\n- Attempted worked. It worked just like it should. Worked like a charm. - [Bliss] I remember\nthere was like one Tuesday we were messing around, and I think, I forget what swear word you used, but there's a swear word\nthat came out of your mouth when you figured out you could just do the direct cursor control. - Yeah, that's it. It blew my mind. Like no pun intended,\nblew my mind when I first moved the cursor just with my thoughts and not attempting to move. It's something that I've found like over the couple of weeks,\nlike building up to that, that as I get better cursor controls, like the model gets better, then it gets easier for me to like, like I don't have to\nattempt as much to move it. And part of that is something\nthat I'd even talked with them about when I\nwas watching the signals of my brain one day, I was watching, when I like attempted to move to the right and I watched the screen,\nit's like I saw the spikes. It's like I was seeing the\nspike, the signals being sent before I was actually attempting to move. I imagined just because when\nyou go to say move your hand or any body part, that signal gets sent before you're actually moving has to make it all the\nway down and back up before you actually do\nany sort of movement. So there's a delay there. And I noticed that there was\nsomething going on in my brain before I was actually attempting to move, that my brain was like\nanticipating what I wanted to do. And that all started sort of, I don't know, like\npercolating in my brain. It was just sort of there, like always in the back. Like that's so weird\nthat it could do that. It kind of makes sense, but\nI wonder what that means as far as like using the Neuralink. And then as I was playing around\nwith the attempted movement and playing around with the cursor, and I saw that like, as the\ncursor control got better, that it was anticipating my movements and what I wanted it to do. Like cursor movements, what I wanted to do a bit\nbetter and a bit better. And then one day, I just randomly, as I was playing Webgrid,\nI like looked at a target before I had started\nlike attempting to move. I was just trying to like get over, like train my eyes to start looking ahead. Like, okay, this is the target I'm on, but if I look over here to this target, I know I can like maybe be\na bit quicker getting there. And I looked over and the\ncursor just shot over it. It was wild. I had to take a step back. Like I was like, \"This\nshould not be happening.\" All day, I was just\nsmiling, I was so giddy. I was like, \"Guys, do\nyou know that this works? Like I can just think it and it happens,\" which like they'd all been\nsaying this entire time. like I can't believe like you're doing all this with your mind. I'm like, \"Yeah, but is\nit really with my mind?\" Like I'm attempting to move, and it's just picking that up so it doesn't feel like it's with my mind. When I moved it for the\nfirst time like that, it was, oh man. It made me think that this technology, that what I'm doing is actually way, way more impressive than I ever thought. It was way cooler than I ever thought. And it just opened up a whole\nnew world of possibilities of like what could possibly\nhappen with this technology and what I might be able\nto be capable of with it. - Because you had felt for the first time, like this was digital telepathy. Like you're controlling a\ndigital device with your mind. - [Noland] Yep. - I mean, that's a real\nmoment of discovery. That's really cool. Like you've discovered something. I've seen like scientists talk\nabout like a big aha moment. Like Nobel Prize winning, they'll have this like holy crap. - Yeah.\n- Like whoa. - That's what it felt. I didn't feel like, like I felt like I had\ndiscovered something but for me. Maybe not necessarily for\nlike the world at large or like this field at large. It just felt like an aha moment for me. Like, \"Oh this works.\" Like obviously, it works. And so that's what I do\nlike all the time now. I kind of intermix the attempted movement and imagined movement. I do it all like together, because I've found that there\nis some interplay with it that maximizes efficiency with the cursor. So it's not all like one or the other. It's not all just, I only use attempted or I only use like imagined movements. It's more I use them in parallel, and I can do one or the other. I can just completely think\nabout whatever I'm doing. But I don't know. I like to play around with it. I also like to just\nexperiment with these things. Like every now and again, I'll get this idea in my head like, \"Hmm, I wonder if this works.\" And I'll just start doing it, and then afterwards, I'll tell them, \"By the way, I wasn't doing\nthat like you guys wanted me to. I thought of something and I\nwanted to try it and so I did. It seems like it works, so maybe we should like\nexplore that a little bit.\" - So I think that discovery\nis not just for you, at least from my perspective,\nthat's the discovery for everyone else who\never uses a Neuralink that this is possible. Like I don't think this an obvious thing that this is even possible. It's like, I was saying to Bliss earlier, it's like the four-minute mile. People thought it was\nimpossible to run a mile in four minutes, and once\nthe first person did it, then everyone just started doing it. So like just to show that it's possible, that paves the way to\nlike anyone can not do it. That's the thing that's actually possible. You don't need to do\nthe attempted movement. You can just go direct. That's crazy. - It is crazy, it's crazy. - For people who don't know, can you explain how the Link app works? You have an amazing stream on the topic. Your first stream, I think,\non X describing the app. Can you just describe how it works? - Yeah, so it's just an\napp that Neuralink created to help me interact with the computer. So on the Link app, there\nare a few different settings and different modes and\nthings I can do on it. So there's like the body mapping, which we kind of touched on. There's a calibration. Calibration is how I\nactually get cursor control. So calibrating what's going on in my brain to translate that into cursor control. So it will pop out models. What they use I think is like time. So it would be five minutes, and calibration will give\nme so good of a model. And then if I'm in it for\n10 minutes and 15 minutes, the models will progressively get better. And so the longer I'm in it generally, the better the models will get. - That's really cool, 'cause\nyou often refer to the models. The model's the thing that's constructed once you go through the calibration step. And then you also talked about, sometimes you'll play like\na really difficult game, like Snake, just to see\nhow good the model is. - Yeah, yeah, so Snake is kind of like my litmus test for models. If I can control Snake decently well, then I know I have a pretty good model. So yeah, the Link app has all of those. It has Webgrid in it now. It's also how I like connect to the computer just in general. So they've given me a lot\nof like voice controls with it at this point,\nso I can say like connect or implant disconnect. And as long as I have that charger handy, then I can connect to it. So the charger is also how\nI connect to the Link app, to connect to the computer. I have to have the implant\ncharger over my head when I wanna connect to have it wake up, 'cause the implant's in hibernation mode, like always when I'm not using it. I think there's a setting to\nlike wake it up every so long. So we could set it to\nhalf an hour or five hours or something if I just want\nit to wake up periodically. So yeah, I'll like\nconnect to the Link app, and then go through all sorts of things. Calibration for the\nday, maybe body mapping. I made them give me like\na little homework tab, because I am very forgetful and I forget to do things a lot. So I have like a lot of\ndata collection things that they want me to do. - Is the body mapping part\nof the data collection, or is that also part of the- - Yeah, it is. It's something that they\nwant me to do daily, which I've been slacking on, 'cause I've been doing so much media and traveling so much. So I've been-\n- You've been super famous. - Yeah, I've been a\nterrible first candidate for how much I've been\nslacking on my homework. But yeah, it's just\nsomething that they want me to do every day to track how well the Neuralink\nis performing over time and to have something to give. I imagine to give to the FDA to create all sorts of fancy charts\nand stuff and show like, \"Hey, this is what the Neuralink, this is how it's performing\nday one versus day 90 versus day 180 and things like that. - What's the calibration step like? Is it like move left, move right? - It's a bubble game. So there will be like yellow bubbles that pop up on the screen. At first, it is open loop. So open loop, this is something that I still don't fully understand, the open loop and closed loop thing. - And me and Bliss talked for a long time about the difference between\nthe two on the technical side. - Okay.\n- So it'd be great to hear your side of the story. - Open loop is basically, I have no control over the cursor. The cursor will be moving on\nits own across the screen, and I am following by intention the cursor to different bubbles. And then the algorithm is training off of what like the signals it's\ngetting are as I'm doing this. There are a couple different\nways that they've done it. They call it center out target. So there will be a bubble in the middle and then eight bubbles around that. And the cursor will go from\nthe middle to one side. So say middle to left, back to middle, to up, to middle, like up, right. And they'll do that all\nthe way around the circle. And I will follow that\ncursor the whole time, and then it will train\noff of my intentions, what it is expecting my intentions to be throughout the whole process. - Can you actually speak to, when you say follow-\n- Yes. - You don't mean with your eyes. You mean with your intentions. - Yeah, so generally for calibration, I'm doing attempted movements, 'cause I think it works better. I think the better models, as I progress through calibration, make it easier to use imagined movements. - Wait, wait, wait, wait. So calibrated on attempted movement will create a model that\nmakes it really effective for you to then use the force? - Yes, I've tried doing calibration with imagined movement, and it just doesn't work\nas well for some reason. So that was the center out targets. There's also one where a random target will pop up on the\nscreen and it's the same. I just like move, I follow along with wherever the cursor\nis to that target, all across the screen. I've tried those with imagined movement, and for some reason,\nthe models just don't, they don't give as high level as quality when we get into closed loop. I haven't played around with it a ton, so maybe like the different ways that we're doing calibration\nnow might make it a bit better. But what I've found is there\nwill be a point in calibration where I can use imagined movement. Before that point, it doesn't really work. So if I do calibration for 45 minutes, the first 15 minutes, I\ncan't use imagined movement. It just like doesn't work for some reason. And after a certain point, I can just sort of feel it. I can tell it moves different. That's the best way I can describe it. Like it's almost as if it is anticipating what I am going to do\nagain before I go to do it. And so using attempted\nmovement for 15 minutes, at some point, I can kind of tell when I like move my\neyes to the next target that the cursor is\nstarting to like pick up. Like it's starting to understand, it's learning like what I'm going to do. - So first of all, it's really cool that, I mean, you are a true\npioneer in all of this. You're like exploring how to do every aspect of this most\neffectively, and there's just, I imagine so many lessons\nlearned from this. So thank you for being a pioneer in all these kinds of different\nlike super technical ways. And it's also cool to hear\nthat there's like a different like feeling to the experience when it's calibrated in different ways, 'cause I mean, I imagine your brain is doing something different, and that's why there's a\ndifferent feeling to it. And then trying to find the\nwords and the measurements to those feelings would\nbe also interesting. But at the end of the\nday, you can also measure that your actual performance on whether it's Snake or Webgrid, you could see like what\nactually works well. And you're saying, for\nthe open loop calibration, the attempted movement works best for now. - Yep, yep. - So the open loop, you\ndon't get the feedback that you did something. - Yeah-\n- Is that frustrating? - No, no, it makes sense to me. Like we've done it with a cursor and without a cursor in open loop. So sometimes, it's just,\nsay, for like the center out, you'll start calibration\nwith a bubble lighting up, and I push towards that bubble. And then when that bubble, when it's pushed towards\nthat bubble for say, three seconds a bubble will pop and then I come back to the middle. So I'm doing it all just by my intentions. Like that's what it's learning anyways. So it makes sense that as long as I follow what they want me to do, like\nfollow the yellow brick road, that it'll all work out. - You're full of great references. Is the bubble game fun? - Yeah, they always feel so\nbad making me do calibration. Like, \"Oh, we're about to\ndo a 40-minute calibration.\" I'm like, \"All right, do you\nguys wanna do two of them?\" Like I'm always asking to, like whatever they need,\nI'm more than happy to do. And it's not bad. Like I get to lie there or sit in my chair and like do these things\nwith some great people. I get to have great conversations. I can give them feedback. I can talk about all sorts of things. I could throw something on\non my TV in the background and kinda like split my\nattention between them. Like it's not bad at all. I don't mind it.\n- Is there a score that you get? Like can you do better on the bubble game? - No, I would love that. I would love-\n- Yeah. Writing down suggestions from Noland. - That's-\n- Make it more fun. Gamified. - Yeah, that's one thing that I really, really enjoy about Webgrid is 'cause I'm so competitive. Like the higher the BPS,\nthe higher the score, I know the better I'm doing. I think I've asked at one\npoint one of the guys, like if he could give me some\nsort of numerical feedback for calibration, like I would like to know what they're looking at. Like, \"Oh, we see like this number while you're doing calibration, and that means, at least on our end, that we think calibration is going well.\" And I would love that,\nbecause I would like to know if what I'm doing is going well or not. But then they've also told me like, \"Yeah, not necessarily like one-to-one.\" It doesn't actually mean that calibration is\ngoing well in some ways. So it's not like 100%, and they don't wanna like\nskew what I'm experiencing or want me to change things based on that. If that number isn't always accurate to like how the model will turn out or how like the end result, that's at least what I got from it. One thing I do that I have asked them and something that I\nreally enjoy striving for is towards the end of calibration, there is like a time between targets. And so I like to keep, like at the end, that\nnumber as low as possible. So at the beginning,\nit can be four or five, six seconds between me popping bubbles. But towards the end, I like\nto keep it below like 1.5. Or if I could, get it to like one second between like bubbles, because in my mind, that translates really nicely\nto something like Webgrid where I know if I can hit\na target one every second that I'm doing real, real well. - There you go, that's a way to get a score on the calibrations. Like the speed, how quickly can you get from bubble to bubble.\n- Yeah. - So there's the open loop, and then it goes to the closed loop. - Closed loop.\n- The closed loop can already start giving you a sense, 'cause you're getting feedback of like how good the model is. - Yeah, so closed loop is when\nI first get cursor control and how they've described it to me, someone who does not\nunderstand this stuff, I am the dumbest person in the room every time I'm with-\n- I love the humility. - Yeah, is that I am closing the loop. So I am actually now the\none that is like finishing the loop of whatever this loop is. I don't even know what the\nloop is, they've never told me. They just say there is a loop. And at one point, it's\nopen and I can't control. And then I get control and it's closed. So I'm finishing the loop. - So how long the\ncalibration usually take? You said like 10, 15 minutes. - Well, yeah, they're trying to get that number down pretty low. That's what we've been\nworking on a lot recently is getting that down as low\nas possible, so that way, if this is something that people\nneed to do on a daily basis or if some people need to do on a like every other\nday basis or once a week, they don't want people to\nbe sitting in calibration for long periods of time. I think they've wanted to get it down seven minutes or below, at\nleast where we're at right now. It'd be nice if you never\nhad to do calibration. So we'll get there at\nsome point, I'm sure, the more we learn about the brain and like I think that's the dream. I think right now, for me to get like really,\nreally good models, I am in calibration 40 or 45 minutes. And I don't mind. Like I said, they always feel really bad, but if it's gonna get me a model that can like break\nthese records on Webgrid, I'll stay in it for flipping two hours. - Let's talk business. So Webgrid. I saw a presentation\nwhere Bliss said by March, you selected 89,000 targets in Webgrid. Can you explain this game? What is Webgrid, and what does it take to be a world class performer in Webgrid, as you continue to break world records? - Yeah. - It's like a gold medalist, like well. - Yeah, I'd like to thank, I'd like to thank everyone\nwho's helped me get here, my coaches, my parents for\ndriving me to practice every day at five in the morning. Like to thank God. And just overall, my\ndedication to my craft. - The interviews with athletes, they're always like that, it's like that template. - Yeah. - So Webgrid is a grid that sells.\n- Webgrid is, yeah. It's literally just a grid. They can make it as big or\nsmall as you can make a grid. A single box on that grid will light up and you go and click it. And it is a way for them to\nbenchmark how good a BCI is. So it's pretty straightforward. You just click targets. - [Lex] Only one blue cell appears, and you're supposed to move the mouse to there and click on it.\n- Yep. So I like playing on like bigger grids, 'cause the bigger the\ngrid, the like more BPS. It's bits per second that you\nget every time you click one. So I'll say, I'll play\non like a 35 by 35 grid, and then one of those little squares cell, we call it target, whatever, will light up and you\nmove the cursor there and you click it and\nthen you do that forever. - And you've been able to achieve at first eight bits per second. And then you recently broke that. - Yeah, I'm at 8.5 right now. I would've beaten that literally the day before I came to Austin. But I had like, I don't know, like a five second lag right at the end. And I just had to wait until\nthe latency calmed down and then I kept clicking. But I was at like 8.01 and\nthen five seconds of lag, and then the next like\nthree targets I clicked all stayed at 8.01. So if I would've been able to\nclick during that time of lag, I probably would've hit, I\ndon't know, I might've hit nine. So I'm there. I'm really close. And then this whole Austin trip has really gotten in the way of my Webgrid playing ability.\n- It's frustrating. - Yeah, it's-\n- So that's all you've been thinking about right now? - Yeah, I know. I want to do better at nine. I want to do better. I wanna hit nine, I think. Well, I know nine is\nvery, very achievable. I'm right there. I think 10 I could hit\nmaybe in the next month. Like I could do it probably in the next few weeks if I really push it. - I think you and Elon are\nbasically the same person, 'cause last time I did a podcast with him, he came in extremely frustrated that he can't beat Uber Lilith as a droid. That was like a year ago I\nthink, I forget, like solo. And I could just tell, there's\nsome percentage of his brain the entire time was thinking like, \"I wish I was right now attempting.\" - I think he did it. - He did it that night.\n- Yeah. - He stayed up and did it that night. It's just crazy to me. I mean, in a fundamental\nway, it's really inspiring. And what you're doing is\ninspiring in that way, 'cause I mean, it's not\njust about the game. Everything you're doing there has impact. By striving to do well on Webgrid, you're helping everybody figure out how to create the system all along, like the decoding, the\nsoftware, the hardware, the calibration, all of it, how to make all of that work so you can do everything else really well. - Yeah, it's just really fun. - Well, that's also,\nthat's part of the thing is like making it fun. - Yeah, it's addicting. I've joked about like\nwhat they actually did when they went in and put\nthis thing in my brain. They must have flipped a switch to make me more susceptible\nto these kinds of games, to make me addicted to\nlike Webgrid or something. Do you know Bliss's high score? - Yeah, he said like 14 or something. - 17.\n- Oh boy. - 17.1 or something, 17.01. - 17 dot, 17.01.\n- Yeah. - He told me he like does it on the floor with peanut butter and he like fasts. It's weird. That sounds like cheating. Sounds like performance enhancing. - [Bliss] No, like the first\ntime Noland played this game, he asked, \"How good are we at this game?\" And I think you told me right then, \"You're gonna try to beat me on that.\" - I'm gonna get there someday.\n- Yeah. I fully believe you.\n- I think I can. - I'm excited for that.\n- Yeah. So I've been playing, first\noff, with the dwell cursor, which really hampers my\nWebgrid playing ability. Basically, I have to wait\n0.3 seconds for every click. - Oh, so you can't do the clicks. So you click by dwelling, you said 0.3? - 0.3 seconds, which sucks. It really slows down how much I'm able to, like how high I'm able to get. I still hit like 50, I think I hit like 50 something trials, net\ntrials per minute in that, which was pretty good,\n'cause I'm able to like, there's one of the settings is also like how slow you need to be moving in order to initiate a\nclick, to start a click. So I can tell sort of\nwhen I'm on that threshold to start initiating a\nclick just a bit early, so I'm not fully stopped over\nthe target when I go to click. I'm doing it like on my\nway to the targets a little to try to time it just right.\n- Oh wow. So you're slowing down. - Yeah, just a hair\nright before the targets. (Lex laughing) - This is like elite performance, okay. But that's still, it sucks that there's\na ceiling of the 0.3. - Well, I can get down to 0.2 and 0.1. Point one's what-\n- I get it. - Yeah, and I've played\nwith that a little bit too. I have to adjust a ton\nof different parameters in order to play with 0.1, and I don't have control\nover all that on my end yet. It also changes like how\nthe models are trained. Like if I train a model like in Webgrid, like I bootstrap on a model,\nwhich basically is them training models as I'm playing Webgrid based off of like the Webgrid data, so like if I play Webgrid for 10 minutes, they can train off that data specifically in order to get me a better model. If I do that with 0.3 versus 0.1, the models come out different. The way that they interact\nis just much, much different. So I have to be really careful. I found that doing it with 0.3 is actually better in some ways, unless I can do it with 0.1 and change all of the\ndifferent parameters, then that's more ideal, 'cause obviously, 0.3 is faster than 0.1. So I could get there. I can get there. - Can you click using your brain? - For right now, it's the hover clicking with the dwell cursor. Before all the thread\nretraction stuff happened, we were calibrating clicks\n- left click, right click. That was my previous ceiling. Before I broke the record\nagain with the dwell cursor was I think on a 35 by 35 grid\nwith left and right click. And you get more BPS, more bits per second using multiple clicks\n'cause it's more difficult. - Oh, because what is it, you're supposed to do either a left click or a like right click? You use a different color\nfor stuff like this? - Yeah, blue targets for left click; orange targets for right\nclick is what they had done. - Got it.\n- So my previous record of 7.5 was with the blue and\nthe orange targets, yeah, which I think if I went back to that now doing the click calibration, and being able to like\ninitiate clicks on my own, I think I would would\nbreak that 10 ceiling like in a couple days max. - Like yeah, you would\nstart making Bliss nervous about his 17-\n- You should be. - Why do you think we\nhaven't given him the- - Yeah, exactly. So what did it feel like\nwith the retractions? That some of the threads retracted? - It sucked. It was really, really hard. The day they told me was the\nday of my big Neuralink tour at their Fremont\nfacility, and they told me like right before we went over there. It was really hard to hear. My initial reaction was,\n\"Alright, go in, fix it. Like go in, take it out, and fix it.\" The first surgery was so easy. Like I went to sleep. A couple hours later, I\nwoke up and here we are. I didn't feel any pain, didn't take like any\npain pills or anything. So I just knew that if they\nwanted to, they could go in and put in a new one like next\nday if that's what it took, 'cause I wanted it to be better and I wanted not to lose the capability. I had so much fun playing with it for a few weeks, for a month. It had opened up so many doors for me. It had opened up so\nmany more possibilities that I didn't want to\nlose it after a month. I thought it would've\nbeen a cruel twist of fate if I had gotten to see the view from like the top of this mountain and then have it all come\ncrashing down after a month. And I knew like, say,\nthe top of the mountain, but how I saw it was I was just now starting to climb the mountain. There was so much more\nthat I knew was possible. And so to have all of that be taken away was really, really hard. But then on the drive\nover to the facility, I don't know, like five\nminute drive, whatever it is, I talked with my parents about it. I prayed about it. I was just like, \"I'm not\ngonna let this ruin my day. I'm not gonna let this\nruin this amazing like tour that they have set up for me. Like I wanna go show everyone how much I appreciate all\nthe work they're doing. I wanna go like meet all of the people who have made this possible, and I wanna go have one of\nthe best days of my life.\" And I did, and it was amazing, and it absolutely was one of the best days I've ever been privileged to experience. And then for a few days, I\nwas pretty down in the dumps. But for like the first few days\nafterwards, I was just like, I didn't know if it was\ngonna ever gonna work again. I made the decision that\neven if I lost the ability to use the Neuralink, even if I lost, even if I like lost out\non everything to come, if I could keep giving\nthem data in any way, then I would do that. If I needed to just do like some of the data collection every day or body mapping every day for\na year, then I would do it, because I know that everything I'm doing helps everyone to come after me. And that's all I wanted. I guess the whole reason that\nI did this was to help people, and I knew that anything\nI could do to help, I would continue to do, even if I never got to\nuse the cursor again, then I was just happy to be a part of it. And everything that I'd\ndone was just a perk. It was something that I got to experience, and I know how amazing it's gonna be for everyone to come after me. So might as well just keep trucking along. - Well, that said, you were able to get to work your way up, to\nget the performance back. So this is like going from\nrocky one to rocky two. So when did you first realize\nthat this is possible, and what gave you sort of the strength and motivation, determination to do it? To increase back up and\nbeat your previous record? - Yeah, it was within a couple weeks. - Again, this feels like I'm interviewing an athlete. (laughs) This is great. I like to thank my parents. - The road back was long and hard, fraught many difficulties. There were dark days. It was a couple weeks, I think. And then there was just a turning point. I think they had switched\nhow they were measuring the neuron spikes in my brain. Bliss, help me out. - [Bliss] Yeah, the way\nin which we're measuring the behavior of individual neurons. - Yeah.\n- So we're switching from sort of individual spike detection to something called spike band power, which if you watch the previous segments with either me or DJ, you\nprobably have some content. - Yeah, okay, so when they\ndid that, it was kind of like, a light over the head,\nlike light bulb moment. Like, \"Oh, this works.\" And this seems like we can run with this. And I saw the uptick in\nperformance immediately. Like I could feel it\nwhen they switched over. I was like, \"This is better. Like this is good.\" Like everything up till this point for the last few weeks, last like whatever, three or four weeks, 'cause it was before they even told me, like everything before this sucked. Like let's keep doing\nwhat we're doing now. And at that point, it was not like, \"Oh, I know I'm still only at, like saying Webgrid terms, like four or five BPS\ncompared to my 7.5 before. But I know that if we keep doing this, then like I can get back there.\" And then they gave me the dwell cursor, and the dwell cursor sucked at first. It's not, obviously, not what I want, but it gave me a path forward to be able to continue using it, and hopefully, to continue to help out. And so I just ran with\nit, never looked back. Like I said, I'm just kind of person that roll with the punches anyways. - What was the process? What was the feedback\nloop on the figuring out how to do the spike detection in a way that would actually\nwork well for Noland? - Yeah, it's a great question. So maybe just describe first\nhow the actual update worked. It was basically an\nupdate to your implant. So we just did an over\nthe air software update to his implants, same way\nyou'd update your Tesla or your iPhone, and that\nfirmware change enabled us to record sort of averages of populations of neurons nearby individual electrodes. So we have sort of less resolution about which individual\nneuron is doing what, but we have a broader\npicture of what's going on nearby an electrode overall. And that feedback, I mean, basically, Noland described it was immediate when we flipped that switch. I think the first day we did that, you had three or four\nBPS right out of the box. And that was a light bulb moment for, \"Okay, this is the right path to go down.\" And from there, there's\na lot of feedback around like how to make this\nuseful for independent use. So what we care about ultimately is that you can use it independently\nto do whatever you want. And to get to that point, it required us to re-engineer the UX, as you\ntalked about the dwell cursor, to make it something that\nyou can use independently without us needing to be\ninvolved all the time. And yeah, this is obviously the\nstart of this journey still. Hopefully, we get back to\nthe places where you're doing multiple clicks and using that to control much more fluidly, everything, and much more naturally, the applications that you're trying to interface with. - And most importantly,\nget that Webgrid number up. - Yes.\n- Yeah. So how is the, on the hover click, do you accidentally click cells sometimes? Like how hard is it to\navoid accidentally clicking? - I have to continuously\nkeep it moving, basically. So like I said, there's a threshold where it will initiate a click. So if I ever drop below that, it'll start, and I have 0.3 seconds to move\nit before it clicks anything. And if I don't want it to ever get there, I just keep it moving at a certain speed, and like just constantly\nlike doing circles on screen, moving it back and forth to\nkeep it from clicking stuff. I actually noticed a couple weeks back, when I was not using the implant, I was just moving my hand\nback and forth or in circles. Like I was trying to keep\nthe cursor from clicking, and I was just doing it\nlike while I was trying to go to sleep, and I was like,\n\"Okay, this is a problem.\" (both laughing) - To avoid the clicking. I guess, does that create\nproblems like when you're gaming accidentally click a thing? - Yeah, yeah, it happens in chess. I've lost a number of games because I'll accidentally click something. - [Bliss] I think the\nfirst time I ever beat you was because of an accident.\n- Yeah, I misclicked, yeah. - It's a nice excuse, right? - Yeah.\n- You can always, anytime you lose-\n- You could just say- - That was accidental.\n- Yeah. - You said the app improved\na lot from version one. When you first started using\nit, it was very different. So can you just talk\nabout the trial and error that you went through with the team? Like 200 plus pages of notes. What's that process like of- - Yeah.\n- Going back and forth and working together to improve the thing? - It's a lot of me just using\nit like day in and day out and saying, like, \"Hey, can\nyou guys do this for me? Like give me this. I wanna be able to do that. I need this.\" I think a lot of it just\ndoesn't occur to them maybe until someone is actually using the app, using the implant. It's just something that they just never would've thought of. Or it's very specific to even like me, maybe what I want. It's something I'm a little worried about with the next people that come is maybe they will want\nthings much different than how I've set it up, or what the advice I've given the team. And they're gonna look\nat some of the things they've added for me. Like that's a dumb idea. Like why would he ask for that? And so I'm really looking\nforward to get the next people on because I guarantee that\nthey're going to think of things that I've never thought of. They're gonna think of improvements. I'm like, \"Wow, that's a really good idea. Like I wish I would've thought of that.\" And then they're also\ngonna give me some pushback about like, \"Yeah, what you\nare asking them to do here, that's a bad idea. Let's do it this way.\" And I'm more than happy\nto have that happen. But it's just a lot of\nlike different interactions with different games or applications, the internet, just with\nthe computer in general. There's tons of bugs that end up popping up\nleft, right, center. So it's just me trying to\nuse it as much as possible and showing them what\nworks and what doesn't work and what I would like to be better. And then they take that feedback, and they usually create\namazing things for me. They solve these problems in\nways I would've never imagined. They're so good at everything they do. And so I'm just really thankful that I'm able to give them feedback and they can make something of it, 'cause a lot of my feedback\nis like really dumb. It's just like, \"I want this. Please do something about it.\" And we'll come back and\nsuper well thought out, and it's way better than anything I could have ever thought\nof or implemented myself. So they're just great. They're really, really cool. - As the BCI community grows,\nwould you like to hang out with the other folks with Neuralink? What relationship, if any,\nwould you wanna have with them? Because you said like they\nmight have a different set of like ideas of how to use the thing. - Yeah.\n- Would you be intimidated by their Webgrid performance? - No, no, I hope compete. I hope day one, they like\nwipe the floor with me. I hope they beat it and they crush it. Double it if they can. Just because, on one hand, it's only gonna push me to be better, 'cause I'm super competitive. I want other people to push me. I think that is important\nfor anyone trying to achieve greatness is they need\nother people around them who are going to push them to be better. And I even made a joke about it on X once. Like once the next people get chosen, like qubadi cot music, like\nI'm just excited to have other people to do this with and to like share experiences with. I'm more than happy to interact with them as much as they want, more than happy to give them advice. I don't know what kind of\nadvice I could give them, but if they have questions,\nI'm more than happy. - What advice would you have for the next participant\nin the clinical trial? - That they should have fun with this, because it is a lot of fun. And that I hope they\nwork really, really hard, because it's not just for us, it's for everyone that comes after us. And come to me if they need anything. And to go to Neuralink\nif they need anything. Man, Neuralink moves mountains. Like they do absolutely\nanything for me that they can. And it's an amazing\nsupport system to have. It puts my mind at ease\nfor like so many things that I have had like questions about, or so many things I wanna do. And they're always there, and\nthat's really, really nice. I would tell them not to be\nafraid to go to Neuralink with any questions that\nthey have, any concerns, anything that they're\nlooking to do with this. And any help that Neuralink\nis capable of providing, I know they will. And I don't know, I don't know. Just work your ass off,\nbecause it's really important that we try to give our all to this. - So have fun and work hard. - Yeah, yeah, there we go. Maybe that's what I'll just\nstart saying to people: have fun, work hard. - Now, you're a real pro athlete. Just keep it short.\n(Noland laughing) Maybe it's good to talk about\nwhat you've been able to do now that you have a Neuralink implant. Like the freedom you gain from this way of interacting\nwith the outside world. Like you play video games all night. And you do that by yourself. And that's a kind of freedom. Can you speak to that\nfreedom that you gain? - Yeah, it's what all, I don't know, people in my position want. They just want more independence. The more load that I can take away from people around me, the better. If I'm able to interact with the world without using my family, without going through any of my friends, like needing them to help\nme with things, the better. If I'm able to sit up\non my computer all night and not need someone to like sit me up, say like on my iPad, like in\na position where I can use it and then have to have them wait up for me all night until I'm ready\nto be done using it, it takes a load off of all of us. And it's really like all I can ask for. It's something that I could never thank Neuralink enough for. And I know my family feels the same way. Just being able to have the\nfreedom to do things on my own at any hour of the day or\nnight, it means the world to me. And I don't know. - When you're up at 2:00 AM\nplaying Webgrid by yourself, I just imagine like it's darkness and there's just a light glowing. And you're just focused. What's going through your mind? (Noland laughing) Or you were like in a state of flow where it's like the mind is empty, like those like Zen masters? - Yeah, generally, it is me\nplaying music of some sort. I have a massive playlist, and so I'm just like rocking out to music. And then it's also just\nlike a race against time, 'cause I'm constantly looking at how much battery percentage\nI have left on my implant. Like, all right, I have 30%, which equates to x amount of time, which means I have to break this record in the next hour and a half, or else, it's not happening tonight. And so it's a little\nstressful when that happens. When it's above 50%, I'm\nlike, \"Okay, like I got time.\" It starts getting down to 30 and then 20. It's like, all right, 10%, a little popup is gonna pop up right here, and it's gonna really\nscrew my Webgrid flow. It's gonna tell me that\nthere's like the low battery, low battery popup comes up, and I'm like, it's really gonna screw me over. So if I have to, if I'm\ngonna break this record, I have to do it in the\nnext like 30 seconds, or else, that popup is\ngonna get in the way, like cover my Webgrid. After that, I go click on\nit, go back into Webgrid. And I'm like, \"All\nright, that means I have 10 minutes left before this thing's dead.\" That's what's going on\nin my head generally, that and whatever song's playing. I want to break those records so bad. Like it's all I want\nwhen I'm playing Webgrid. It has become less of like, \"Oh, this is just a leisurely activity.\" Like I just enjoy doing this, because it just feels so\nnice and it puts me at ease. No, once I'm in Webgrid,\nyou better break this record or you're gonna waste like five hours of your life right now. And I don't know, it's just fun. It's fun, man. - Have you ever tried Webgrid with like two targets and three targets? Can you get higher BPS with that? - Can you do that? - [Bliss] You mean, like\ndifferent color targets, or you mean-\n- Oh, multiple targets, 'cause that change the thing. - Yeah, so BPS is a log\nof number of targets times correct minus\nincorrect divided by time. And so you can think of\nlike different clicks as basically doubling the\nnumber of active targets. - Got it.\n- So you know, you get basically higher BPS,\nthe more options there are, the more difficult to task. And there's also like zen\nmode you've played in before, which is like-\n- Yeah. Yeah, it covers the\nwhole screen with a grid. And I don't know. - [Lex] Yeah, and so you can go like, that's insane.\n- Yeah. - [Bliss] He doesn't like it\n'cause it didn't show BPS. - I had them put in a giant\nBPS in the background, so now it's like the opposite of Zen mode. It's like super hard mode,\nlike just metal mode. It's just like a giant\nnumber in the back counter. - [Bliss] We should rename that. Metal mode is a much better name now. - So you also play Civilization VI? - I love Civ VI, yeah. - [Lex] You usually go with Korea, you said?\n- I do, yeah. So the great part about\nKorea is they focus on like science tech victories,\nwhich was not planned. Like I've been playing Korea for years, and then all of the\nNeuralink stuff happened. So it kind of aligned. But what I've noticed with tech victories is if you can just rush\ntech, rush science, then you can do anything. Like at one point in the game, you'll be so far ahead of\neveryone technologically that you'll have like\nmusket men, infantry men, plane sometimes, and people\nwill still be fighting with like bows and arrows. And so if you want to\nwin a domination victory, you just get to a certain\npoint with the science and then go and wipe out\nthe rest of the world. Or you can just take science\nall the way and win that way, and you're gonna be so\nfar ahead of everyone 'cause you're producing so much science that it's not even close. I've accidentally won in different ways just by focusing on science. - Accidentally won by focusing on science. - I was playing only science, obviously. Like just science all the way, just tech. And I was trying to get like every tech in the tech tree and stuff. And then I accidentally won\nthrough a diplomatic victory, and I was so mad.\n(Lex laughing) I was so mad, 'cause it just like ends the game. One turn, it was like, \"Oh, you won, you're so diplomatic.\" I'm like, \"I don't wanna do this. I should have declared war\non more people or something.\" It was terrible, but you don't need like giant civilizations with\ntech, especially with Korea. You can keep it pretty small. So I generally just get\nto a certain military unit and put them all around my\nborder to keep everyone out, and then I will just build up. So very isolationist. - Nice.\n- Yeah. - Just work on the science and the tech. - [Noland] Yep, that's it. - You're making it sound so fun. - It's so much fun.\n- And I also saw Civilization VII trailer. - Oh man, I'm so pumped.\n- Yeah. And that's probably coming out- - Come on, Civ VII, hit me up. Alpha, beta tests, whatever. - Wait, when is it coming out? - 2025.\n- Yeah, yeah, next year, yeah. What other stuff would\nyou like to see improved about the Neuralink app and\njust the entire experience? - I would like to, like I said, get back to the like click on demand, like the regular clicks. That would be great. I would like to be able to\nconnect to more devices. Right now, it's just the computer. I'd like to be able to use it on my phone or use it on different\nconsoles, different platforms. I'd like to be able to control as much stuff as possible, honestly. Like an Optimus robot\nwould be pretty cool. That would be sick if I could\ncontrol an Optimus robot. The Link app itself, it\nseems like we are getting pretty dialed in to what it\nmight look like down the road. Seems like we've gotten through a lot of what I want from it at least. The only other thing I would say is like more control\nover all the parameters that I can tweak with my\nlike cursor and stuff. There's a lot of things that go into how the cursor moves in certain ways. And I have, I don't\nknow, like three or four of those parameters and there might- - Like gain and friction and all that? - Gain and friction, yeah. And there's maybe double\nthe amount of those with just like velocity and then with the actual dwell cursor. So I would like all of it. I want as much control over\nmy environment as possible, especially-\n- So you want like advanced mode? Like there's menus usually,\nthere's basic mode. And you're like one of those folks like- - I go the-\n- Power user advanced. - Yeah, yeah.\n- Got it. - That's what I want. I want as much control\nover this as possible. So yeah, that's really all I can ask for. Just give me everything. - Has speech been useful? Like just being able to talk also in addition to everything else? - Yeah, you mean like while I'm using it? - While you're using\nit, like speech to text? - Oh yeah.\n- Or do you type, or like, 'cause there's also a keyboard? That's really nice.\n- Yeah, yeah. So there's a virtual keyboard. That's another thing I\nwould like to work more on is finding some way to type\nor text in a different way. Right now, it is like\na dictation basically and a virtual keyboard that\nI can use with the cursor. But we've played around\nwith like finger spelling, like sign language, finger spelling. And that seems really promising. So I have this thought in my head that it's going to be a\nvery similar learning curve that I had with the cursor, where I went from attempted movement to imagined movement at one point. I have a feeling, this\nis just my intuition, that at some point, I'm going\nto be doing finger spelling and I won't need to actually attempt to finger spell anymore, that I'll just be able to think the like letter that I\nwant and it'll pop up. - That would be epic.\n- Yeah. - That's challenging, that's hard. That's a lot of work for\nyou to kinda take that leap. But that would be awesome. - And then like going from\nletters to words is another step. Like you would go from, right now, it's finger spelling of like just the sign language alphabet. But if it's able to pick that up, then it should be able to pick up like the whole sign\nlanguage like language. And so then if I could do\nsomething along those lines, or just the sign language spelled word, if I can spell it at a reasonable speed and it can pick that up, then I would just be able\nto think that through and it would do the same thing. I don't see why not. After what I saw with the cursor control, I don't see why it wouldn't work, but we'd have to play around with it more. - What was the process in\nterms of like training yourself to go from attempted movement\nto imagined movement? How long did that take? So how long would this\nkind of process take? - Well, it was a couple weeks before it just like happened upon me. But now that I know\nthat that was possible, I think I could make it\nhappen with other things. I think it would be much, much simpler. - Would you get an\nupgraded implant device? - Sure, absolutely. Whenever they'll let me. - So you don't have any concerns for you with the surgery experience? All of it was like no regrets? - No. - So everything's been good so far? - Yep. - You just keep getting upgrades. - Yeah, I mean, why not? I've seen how much it's\nimpacted my life already. And I know that everything\nfrom here on out, shit's gonna get better and better. So I would love to. I would love to get the upgrade. - What future capabilities\nare you excited about sort of beyond this kind of telepathy? Is vision interesting? So for folks who, for\nexample, who are blind, so you're like enabling\npeople to see, or for speech. - Yeah, there's a lot that's\nvery, very cool about this. I mean, we're talking about the brain, so like this is just motor cortex stuff. There's so much more that can be done. The vision one is fascinating to me. I think that is going\nto be very, very cool. To give someone the ability to see for the first time in their\nlife would just be, I mean it, it might be more amazing than\neven helping someone like me. Like that just sounds incredible. The speech thing is really interesting, being able to have some sort\nof like real time translation and cut away that language\nbarrier would be really cool. Any sort of like actual\nimpairments that it could solve, like with speech, would\nbe very, very cool. And then also, there are a\nlot of different disabilities that all originate in the brain. And you would be able to, hopefully be able to solve a lot of those. I know there's already stuff\nto help people with seizures that can be implanted in the brain. This would do, I imagine, the same thing. And so you could do something like that. I know that even someone like\nJoe Rogan has talked about the possibilities with being able to stimulate the brain in different ways. I'm not sure. I'm not sure how ethical\na lot of that would be. That's beyond me honestly. But I know that there's\na lot that can be done when we're talking about the brain and being able to go in and physically make changes to help people\nor to improve their lives. So I'm really looking forward to everything that comes from this. And I don't think it's all that far off. I think a lot of this can be\nimplemented within my lifetime, assuming that I live a long life. - What you were referring\nto is things like people suffering from depression or things of that nature\npotentially getting help. - Yeah, flip a switch like\nthat, make someone happy. I know, I think Joe has\ntalked about it more in terms of like you want to experience like what a drug trip feels like. Like you wanna experience\nwhat it'd be like to be on- - Of course.\n- Yeah, mushrooms or something like that, DMT. Like you can just flip\nthat switch in the brain. My buddy Bain has talked about\nbeing able to like wipe parts of your memory and\nre-experience things that, like for the first time, like your favorite movie\nor your favorite book. Like just wipe that out real quick, and then re-fall in love with\nHarry Potter or something. I told him, I was like, \"I don't know how I feel about like people being able to just\nwipe parts of your memory. That seems a little sketchy to me.\" He's like, \"They're already doing it.\" - Sounds legit. Yeah, I would love memory replay. Just like actually high\nresolution replay of old memories. - Yeah, I saw an episode of\n\"Black Mirror\" about that once. I don't think I want it. - Yeah, so \"Black Mirror\"\nalways kind of considers the worst case, which is important. I think people don't\nconsider the best case or the average case enough. I don't know what it is about us humans. We wanna think about the\nworst possible thing. We love drama. - [Noland] Yeah. (laughs) - It's like, how's this new\ntechnology gonna kill everybody? We just love that. Again like, yes, let's watch. - Hopefully, people don't think\nabout that too much with me. It'll ruin a lot of my plans. - Yeah, yeah. I assume you're gonna have\nto take over the world. I mean, I loved your Twitter. You tweeted, \"I'd like to make jokes about hearing voices in my head since getting the Neuralink, but I feel like people\nwould take it the wrong way. Plus, the voices in my\nhead told me not to.\" - Yeah.\n- Yeah. - Yeah.\n- Please never stop. So you're talking about Optimus. Is that something you would\nlove to be able to do, to control the robotic arm\nor the entirety of Optimus? - Oh yeah, for sure. For sure, absolutely. - You think there's something\nlike fundamentally different about just being able to\nphysically interact with the world? - Yeah, oh, 100%. I know another thing with like being able to like give people the\nability to like feel sensation and stuff too by going in with the brain and having the Neuralink maybe do that. That could be something that\ncould be translated through, transferred through the Optimus as well. Like there's all sorts of really cool interplay between that. And then also, like you said,\njust physically interacting. I mean, 99% of the things that I can't do myself obviously need, I need a caretaker for, someone to physically do things for me. If an Optimus robot could\ndo that, like I could live an incredibly independent life and not be such a burden\non those around me and it would change the\nway people like me live, at least until whatever\nthis is gets cured. But being able to interact\nwith the world physically, like that would just be amazing. And they're not just like for being, for having to be a caretaker or something, but something like I talked about, just being able to read a book. Imagine an Optimus robot just being able to hold a\nbook open in front of me, like get that smell again. I might not be able to\nfeel it at that point. Or maybe I could again with\nthe sensation and stuff. But there's something\ndifferent about reading like a physical book\nthan staring at a screen or listening to an audio book. I actually don't like audio books. I've listened to a ton\nof them at this point, but I don't really like 'em. I would much rather like\nread a physical copy. - So one of the things you would love to be able to experience\nis opening the book, bringing it up to you. And to feel the touch of the paper. - Yeah. Oh man, the touch, the smell. I mean, it's just like something about the words on the page, and they've replicated that page color on like the Kindle and stuff. Yeah, it's just not the same, yeah. So just something as simple as that. - So one of the things you miss is touch. - I do, yeah. - A lot of things that I\ninteract with in the world, like clothes or literally\nany physical thing that I interact with in\nthe world, a lot of times, what people around me will\ndo is they'll just come like, rub it on my face. They'll like lay something on\nme so I can feel the weight. They will rub a shirt on\nme so I can feel fabric. Like there's something\nvery profound about touch, and it's something that I miss a lot, and something I would love\nto do again, but we'll see. - What would be the first thing you do with a hand that can touch? Give your mom a hug after that, right? - Yeah, I know. It's one thing that I've asked like God for basically every day since my accident was just being able to like one day move, even if it was only like my hand. So that way, like I could\nsqueeze my mom's hand or something just to like show her that, like how much I care and how\nmuch I love her and everything. Something along those lines. Being able to just interact\nwith the people around me, handshake, give someone a hug. I don't know, anything like that. Being able to help me eat, like\nI'd probably get really fat, which would be a terrible, terrible thing. - Also beat Bliss in chess\non a physical chess board. - Yeah, yeah. I mean, there are just\nso many upsides. (laughs) And any way to find some way to feel like I'm bringing Bliss down to my level. - Yeah.\n- Because- - Yeah.\n- He's just such an amazing guy, and everything about him is just so above and beyond\nthat anything I can do to take him down a notch,\nI'm more than happy. - Yeah, humble him a bit, he needs it. - [Noland] Yeah. (laughs) - Okay. As he's sitting next to me. Did you ever make sense of why God puts good people through such hardship? - Oh, man. I think it's all about understanding how much we need God. And I don't think that there's\nany light without the dark. I think that if all of us\nwere happy all the time, there would be no reason\nto turn to God ever. I feel like there would be\nno concept of good or bad. And I think that as much\nof like the darkness and the evil that's in the\nworld, it makes us all appreciate the good and the things\nwe have so much more. And I think, like when I had my accident, one of the first things I said to one of my best friends was,\nand this was within like the first month or two\nafter my accident, I said, \"Everything about this\naccident has just made me understand and believe\nthat like God is real and that there really is a God, basically. And that like my interactions with him have all been real and worthwhile.\" And he said, if anything, seeing me go through this accident, he believes that there isn't a God. And it's a very different reaction. But I believe that it is\na way for God to test us, to build our character, to send us through trials and tribulations, to make sure that we\nunderstand how precious he is, and the things that he's given us and the time that he's given us. And then to hopefully\ngrow from all of that. I think that's a huge part of being here is to not just have an easy life and do everything that's easy, but to step out of our comfort zones and really challenge ourselves, because I think that's how we grow. - What gives you hope\nabout this whole thing we have going on, human civilization? - Oh, man. I think people are my biggest inspiration. Even just being at\nNeuralink for a few months, looking people in the eyes\nand hearing their motivations for why they're doing\nthis, it's so inspiring. And I know that they\ncould be other places, cushier jobs, working somewhere else, doing X, Y, or Z that doesn't\nreally mean that much. But instead they're here, and\nthey want to better humanity and they want a better,\njust the people around them, the people that they've\ninteracted with in their life, they wanna make better lives\nfor their own family members who might have disabilities,\nor they look at someone like me and they say, \"I can\ndo something about that so I'm going to.\" And it's always been what I've connected with most in the world are people. I've always been a people person and I love learning about people, and I love learning like\nhow people developed and where they came from. And to see like how much\npeople are willing to do for someone like me\nwhen they don't have to, and they're going out of their\nway to make my life better. It gives me a lot of hope\nfor just humanity in general, how much we care and how\nmuch we're capable of when we all kind of get together and try to make a difference. And I know there's a lot of\nbad out there in the world, but there always has been\nand there always will be. And I think that that is, it shows human resiliency and it shows what we're able to endure, and how much we just want to\nbe there and help each other, and how much satisfaction\nwe get from that, because I think that's one of the reasons that we're here is just\nto help each other. And I don't know, that\nalways gives me hope. It's just realizing that\nthere are people out there who still care and who wanna help. - And thank you for being\none such human being and continuing to be a great human being through everything you've been through and being an inspiration to many people, to myself, for many reasons,\nincluding your epic, unbelievably great performance on Webgrid. I'll be training all night\ntonight to try to catch up. - You can do it. - And I believe in you, that you can once you come back, so sorry to interrupt\nwith the Austin trip, once you come back, eventually beat Bliss. - Yeah, yeah, for sure. Absolutely.\n- I'm rooting for you. The whole world is rooting for you. - Thank you.\n- Thank you for everything you've done, man. - Thanks, thanks man. - Thanks for listening\nto this conversation with Nolan Arbaugh, and\nbefore that, with Elon Musk, DJ Seo, Matthew MacDougall,\nand Bliss Chapman. To support this podcast, please check out our sponsors in the description. And now, let me leave you with\nsome words from Aldous Huxley in \"The Doors of Perception.\" \"We live together. We act on and react to one another, but always, and in all\ncircumstances, we are by ourselves. The martyrs go hand in\nhand into the arena. They are crucified alone. Embraced, the lovers\ndesperately try to fuse their insulated ecstasies into a single self-transcendence; in vain. By its very nature, every embodied spirit is doomed to suffer and enjoy in solitude. Sensations, feelings, insights, fancies, all these are private and,\nexcept through symbols and at second hand, incommunicable. We can pool information about experiences, but never the experiences themselves. From family to nation, every human group is a society of island universes. Thank you for listening, and hope to see you next time.",
    "status": "success",
    "error": null
  },
  {
    "video_id": "r-vbh3t7WVI",
    "transcript": "it's called neurotransmitters neurotransmitters are released from the end of an axon in response to an electrical spike called an action potential when a cell receives enough of the right kind of neurotransmitter input a chain reaction is triggered that causes an action potential to fire and the neuron to in turn relay messages to its own downstream synapses action potentials produce an electric field that spreads from the neuron and can be detected by placing electrodes nearby allowing recording of the information represented by a neuron [Applause] [Music] [Applause] [Music] [Music] [Music] hello everybody so that that video was not too Shutterstock that was actually your link so that that's actual video from the company so if you want to get a sense for what it's like to work in your link that video is indicative of the atmosphere of your link it's an incredibly talented team and you're gonna hear a lot from from them tonight so we're gonna actually go quite into depth on what we're doing why we're doing how we're doing it and I'm just incredibly impressed with the caliber of talent at your link and the in fact the main reason for during this presentation is recruiting so we really want to have the the best talent in the world come and work at near link anyone that's interested in trying to solve this problem and that's actually the primary purpose for this this presentation so okay so the why of neuro-link just to go over it he is I think it's important for us to address brain related diseases the everyone if they if you survive cancer and heart disease odds are that you will have some brain related disorder so be like Alzheimer's or dementia and it if you don't friends and family will for sure and I think unless we have some sort of brain machine interface that can solve brain ailments of all kinds whether it's an accident or congenital or any kind of brain related disorder or or a spineless order if you know somebody who's broken their neck or broken their spine we can solve that with a chip and and this is something that I think most people don't quite understand yet and we're going to go over in detail how this is possible but I think there's there's an incredible amount we can do to solve brain disorders acted damage and all this will occur actually and it quite slowly so do what emphasize that it's not going to be like suddenly neuro-link will have this incredible neuro lace and start taking over people's brains okay it will take a long time so and you'll see it coming so getting getting FDA approval for implantable or devices of any kind is quite quite difficult and this will be a slow process where we will gradually increase the issues that we solve until ultimately we can do a full brain machine interface meaning that we can in Eltham Utley yeah this is gonna sound pretty weird but achieve a sort of symbiosis with artificial intelligence so this is this is not a mandatory thing this is the thing that you can choose to have if you want and and this is something I think it's gonna be really important at a civilization level scale so and I've said a lot about AI over the years but I think even in a benign AI scenario we will be left behind and so hopefully it is a benign scenario but I think with a high bandwidth brain-machine interface I think we can actually go along for the ride and we can effectively have the option of merging with AI this is extremely important and if you think about your limbic system and your cortex your your limbic system is kind of your primal needs and one since it's like where you're a lot of your emotions are coming from and then the cortex is like the the thinking planning part of your brain and I haven't met anyone who yes who wants to get rid of either the cortex or the Olympic system so clearly they worked were together well even though your cortex is in principle far smarter than your limbic system everybody wants to keep the limbic system and their cortex so hopefully we can have a tertiary layer which is the kind of a digital super intelligence layer and in fact you already have this layer so it's your phone and your laptop and the constraint is just how well you interface that the input and output speed so the Alpha speed is especially slow since most people typing with thumbs these days so you have a very slow output speed your input speed is much faster due to vision but the thing that will ultimately constrain our ability to be somatic with AI is bandwidth so in the limit after after solving a bunch of brain related diseases there is the existential it's mitigation of the existential threat of AI or yeah this is the pointed it so creating a well line future this is that that's the idea of nearly 100 billion cells called neurons neurons come in many complex shapes but generally they have a dendritic Arbor a cell body called a soma and an axon the neurons of your brain connect to form a large network through axon dendrite junctions called synapses at these connection points neurons communicate with each other using chemical signals called neurotransmitters neurotransmitters are released from the end of an axon in response to an electrical spike called an action potential [Music] when a cell receives enough of the right kind of neurotransmitter input a chain reaction is triggered that causes an action potential to fire and the neuron to in turn relay messages to its own downstream synapses action potentials produce an electric field that spreads from the neuron and can be detected by placing electrodes nearby allowing recording of the information represented by a neuron [Music] I rethought we'd play that twice it's so good you have to play it twice well I think it like a lot of people in the audience you know there's a wide range of knowledge about neurons I mean some people view the brain is like this incredibly mystical thing that cannot you cannot interface the brain but and and then some people are aware of the brain simulation such as occurs for Parkinson's patients so try to try to address the broad range of understanding so I mean you're on essentially but like you know there's that whole idea what if we were just a brain-in-a-vat this is often posed by philosophers except we are a brain-in-a-vat and that's it that VAT is us golf everything that you perceive feel here think it's it's all action potentials it's all just its neural spikes and it feels so real it feels very real but but it's it's this that these are all impulses from neurons what's called a a spike and a goal is to record from and stimulate spikes in neurons and and do so in a way that is orders of magnitude more than anything has been done to date and safe and good enough that you can it's not like a major operation it's sort of equivalent to just sort of a lasik type of thing so wait where you can sort of sit down machine does this thing and here you can walk away with within a few hours that's it and you know this you're not even in a hospital so so like this was basically it was a key points that worth taking away the system that we were designed in version one is capable of on the order of 10,000 electrodes so each each chip which is four by four millimeters is capable of a thousand electrodes or has thousand electrode and we think during after 10 is feasible so this is in contrast to the the best fda-approved system which is like a Parkinson's deep brain stimulation a thing which would have on the order of 10 electrodes so the system even in version one that we're going to unveil today is capable of a thousand times more electrodes than the the best system out there and they're all read and write so this is this is really quite I think I mean for something to be a thousand times more than what is public approved is quite a big difference and and this will this will get better with subsequent yes subsequent versions the slide may seem a little generic it was like everything's got robots electronics and algorithms at this point but no threads so the the feel like human transcendence there's actually I wasn't transcendence [Laughter] so that there's there's very tiny threads that are about about a tenth roughly of the cross-sectional area of a human hair so there are extremely tiny threads in fact the threads that we have it likes it even in version one are about the same size as a neuron so if you're gonna go stick something in your brain you wanted to not be giant you want to be tiny and to be approximately on par with the things that are already there but the neurons so this is about the size of of a neuron the each thread and then you really need this to be done with the robot because it's very tiny and it needs to be very precise so you don't and you don't want to pierce a blood vessel so when you sew each thread that the robot looks looks sort of basically through a microscope and puts a inserts each electrode specifically bypassing any vasculature you know any kind of blood vessel and and making sure it's like we inserted without causing trauma or minimal trauma yeah it's not zero but you won't notice it that's the important part you won't like you know yeah what feel thing so and yeah as the algorithms so just give you a sense of scale this is how tiny the threads are that is not even a big finger that is a small finger so the the these threads are just like like I said where smaller than hair and there's a thousand of them and this is what what the robot looks like it's sort of quite quite a complex device but it I it all comes down to a very tiny tiny point so just like you see the robot the robots on the left and and then the what looks like the needles for insertion next to a penny but in fact that the the actual needle that gets inserted is way way tinier it's that little tiny thing at the where the arrow is pointing that's actually the size of the the needle it's about 24 microns in diameter extremely extremely small it's so small you can't really even see it within the picture with the penny and then this is a your reign on uranium not really that's a car so you can get a sense for the robot doing the electrode insertion but that's a very zoomed in view so they're all very very tiny and the robot is very selectively applying them very delicately and and then this is what the jib looks like action potentials so the each one of those represents one electrode so there would be up to 10,000 of these about these lines yeah so I guess like it's always difficult sake there's gonna be a list there's a lot more in this presentation so in terms of things I think are important to bear in mind this I think has a very good purpose which is to cure important diseases and ultimately to help secure humanity's future as a civilization relative to AI the threads are very tiny and there's a lot of them and they're very carefully placed and the the operation on a per chip basis it involves just a a to me a two millimeter incision which is dilated to eight millimeters and then the the Chavez place placed through that and then we add it goes back to being two millimeters and you can basically good shot you don't need a stitch so and then the the interface to the to the to the chip is what is wireless so you have no wires poking out of your head very very important so you it's it's basically bluetooth to your phone because we'll have to watch the App Store updates for that one make sure we don't have a driver issue updating so but the key is like this this is something that is it's gonna be not not stress for our girls not stressful to to put in should work well hopefully it would check it out very carefully before it becomes obviously FDA approved and I and it's wireless so you the this this I think has tremendous potential and we hope to have this aspirationally in in a human patient before the end of next year so this is not not far and then as I mentioned earlier this is the main purpose of this presentation is recruiting and we need very talent people in very tell people in all these areas so it's a lot of very challenged people are needed to make this ultimately successful and then speaking of talented people let me hand it over to max yeah thank you thank you I'm max Hodak I'm the president of neural link I remember a couple years ago when we started talking about the idea of neural link and that there might be a company and whether this was even a good idea I mean my first reaction was that I wasn't sure that this actually was a good idea the technology was there yet and I think it's Elon has this incredible like incredible optimism where help Pierce through these imagined constraints and show you that really a lot more escape a lot more is possible then you really think is today and you have to be very careful telling them that something's impossible it better be limited by a law of physics or you're going to end up looking stupid and so I so I've wanted to build a neural interface has really been like a central goal of my life basically as long as I can remember this is I thinking like we talked about AI being potentially the last invention that we have I think that I've been with BMI might be like really the first invention in many ways of like the next chapter of us it's just real like as Elon alluded to earlier everything about your experience or thoughts or memories it's all in your brain and represented in the firing statistics of action potentials so all right so just what is a BMI and we'll go through this really like fairly quickly I think so there's you start with hopefully a brain and a machine but the machine is just a stand-in for the outside world it could be it could be another brain because it software it could be a robotic arm but you want to receive energy from that world and in part through the senses like vision and audition and impart energy back into the world to things like motor control and that that language that they use to communicate are they putting aside the hardware for a second it's very important understand what that is because people ask like oh can I talk to my dog or can I do these things but it's more to understand what that language is in that language in the most general sense is information to a first approximation everything is information but we just consider here the information represented in neurons and so consider two like toy neurons one so these lines are imagine action potentials and so imagine a neuron that fires very regularly like a metronome like this doesn't tell us anything there's no information conveyed in this signal we don't learn anything from it on the opposite end of the spectrum imagine or that fires completely randomly this also doesn't tell us anything that's also doesn't carry any information now we know that this is these two degenerate cases are not what neurons do because if you fit a model from recorded neural activity to behavior of things like a cursor of a patient or a subject that's implanted and you correlate these then you can build a graph that looks like this and this is a figure from a classic paper in this field from like the academic heritage in this field from 2003 I think that actually some of the authors of this paper in the audience today and you can see it's the x-axis is number of neurons and the R is the goodness of fit and you can see that as you add neurons the quality of the model improves this tells us that neurons care and their spike trains carry information about things here to ask them to us fairly quickly that's because what they were fitting here was just 2d cursor control which has simple dynamics and if you have tasks that are more complicated then you need more neurons so the classic definition of information is a difference that makes a difference it's just some piece of information or knowledge that tells you something it's like a very abstract concept but it's such like information theory is such a deep rabbit hole if you haven't seen it before the original paper mathematical theory of communication it's like it's very readable I highly recommend it you'll start seeing information everywhere it will totally change the way you view the world because the world is information as we've talked about before and understanding information also gets this question of well why do you have to have an implantable device why don't you have eg or a wearable or an optical thing and the answer of course is like well what's like these are different information carriers and what information are they carrying and we know that like if you open a back issue of the Journal of Neuroscience and you understand how some species of bird encode sound localization or something you'll find a discussion of spikes and we as far as we know everything that we care about is found in the statistics of spikes so that's what we focus on there are other things like fMRI or EEG these are different information carriers carrying different information which we think is it which we believe is impoverished relative to spike saying that's the scientific consensus and so the question for all these different things is well what information is found in your carrier we focus on spikes that means we have to be inside the brain because the there's no ceiling that we're aware of on that with respect to that like grand vision of your perception your thoughts everything like motor output and you like rien loss limbs and so why does that mean that you have to be inside the brain so you want spikes well people have studied if you take a neuron and you put an electrode on that specific neuron so you have a ground truth electrical potential of the that one neuron and then you place an extracellular electrode nearby which is what our electrodes and the Ute are a and other people are like we're not in the cell we're near the cells and then you measure how far away from a neuron can you be when you know what the ground truth spiking activity is can you no longer see the spikes and it turns out that the answer is about 60 microns which is like 0.06 it's it's very small it's a lot less than a millimeter so you have to be firmly under the skull like you're not there is no wearable that is going to get you spikes this is a physics constraints as far as we're aware and so now I want to I just want to talk about briefly there's like normally didn't come out of nowhere there's a long academic heritage of research here the cochlear implant has reached millions of patients since the 50s for deaf patients over a hundred thousand patients have received deep brain stimulation for Parkinson's and a central tremor and dystonia now other other indications and about twenty patients have received the Utah Ray which is a little hundred electrode rigid metal silicon device and even though it has very few channels they've been able to do some really cool stuff with it there's videos on YouTube of BrainGate patients doing things like controlling tablet computers or even texting each other through through Utah raised just from these the small number of electrodes and so there's many of the people on the team that normally came from this academic like this academic work I got my start working in a lab at Duke University studying the how mappings between brain and and like the screen space change so if you make it so that the joystick goes like the cursor goes sideways and you push forward instead of up like how does the brain change the representation so the point is that there are lots of people that have been looking at this problem from lots of angles for decades and we're in the greatest sense building on the shoulders of giants here and so the question is why not use one of those devices why not use a Utah or a deep brain stimulator implanted pulse generator and there's it's just in the Utah rate case the the rigid sharp metal electrodes produce a fairly strong immune response and this doesn't end up hurting the patient but it does mean that you lose the ability to record single spikes over some period of time usually between one and a couple of years there's also a big percutaneous connector through the scallop so you need to plug in big external electronics and you're never really confident that the rusco infection is is gone for the duration that you have the implant deep brain stimulators solve just solve a very different type of problem they are very effective for some Parkinson's patients but they have only a couple electrodes and they're really geared towards injecting large amounts of current not recording single spikes so they're really very different the DBS is really just a very different type of platform for very different type of problem so we had to go back to the drawing board and start over to build something that met the goals that you on laid out for us we knew as Elin mentioned that whatever we built we wanted it to be wireless ooh you know what any connectors or wires coming through the skin it had to be something that would last for a long period of time not something that you'd have to take out at two three or four years in it had to have practical bandwidth so we talked about high bandwidth or ultra high bandwidth like what matters is that it for the task that you're after there's practical bandwidth that allows you to effectively do that thing whether that's cursor control or typing or robotic arm or maybe in the future vision and it has to be usable at home it can't be something that you go into a clinic at the hospital for two hours a week and under tight supervision of technicians plug you into the amplifiers and turn it on that's me saying that you can live with and so two and half years ago we were nowhere close to any of that this is a photo of some of the prototypes that we've gone through over that over that time so we started on the far left that's the entirely passive board that has 64 electrodes on it and connects two connectors that go to big external amplifiers and then we added integrated electronics with our first custom chip that's also 64 64 channels and then there was a big leap to the the device that Ilan showed a photo of earlier that has 3072 electrodes in a fully implantable package with just a USB C port coming out and then we we took a step back in channel count B's room we have to optimize safety longevity and bandwidth altogether and so in order to optimize some of those other things we moved to an easier to manufacture system as 1536 channels in a USB C port and those last two are the focus of the paper that we released today and so we've we've learned a lot from these record a lot of data through these like these have are actually used every day at der link to record neural data and work with it and they taught us a lot about the architecture that we think is the basis for our first human product that we're calling n1 and the central component of that is the n1 sensor this is it's a little hermetic package it's about it's when it's fully assembled this is missing an outer mold it's into an 8 millimeter diameter force millimeter tall cylinder and each of these has 1024 electrodes and we can stim and record through through every one of those channels exploding it blowing opening it up a little bit you can see there's there the thin film which has the threads that Elin talked about which is the wisp going off to the side there's a hermetic substrate and then that gets welded later to a package that goes over top and that's made into our custom electronics and we'll go into more detail leaders that work on each of these talked about these in more detail over the restless' presentation so yeah I mean this is just to not to belabor the point I know that Elon really hammered this in but these things are very very small they're like they're not you can't you can't manipulate these this is one photo this is not two photos going together and you really can't manipulate these with your hand that that part at the top is just a backing material that's surgical packaging they're they're peeled off the threads were peeled off that one at a time by the robot to place it into the brain and then yeah and we had to build it a surgical robot and the first impetus for this is just you have to place these threads you can't manipulate these threads you need a robot and then that turned out to that grew into understanding where the blood vessels are and imaging into the tissue and the surface of the brain moves because you're breathing and you have a heartbeat and there's lots of complexity of dealing with this incredibly high entropy substrate and it's not all offload to the robot it's the robots under the supervision of a human neurosurgeon who lays out where the threads were placed but it would not be for the surgery is not possible without the robot and so the n1 implant we can place as Elin mentioned many of these possibly up to ten in one hemisphere for our first patients we're looking at four four sensors three in motor areas and one in a somatosensory area which are connected via very small wires tunneled under the scalp to an inductive coil behind the ear and that connect wirelessly through the skin to a wearable device that we call the link which contains a Bluetooth radio and a battery and this is importantly the only battery and radio in the implant so if you take this off the implant shuts off and if there's software upgrades or security issues it's much easier to upgrade the firmware on the pod than it is to try and change the implant it'll be controlled through an iPhone app you won't have to go to a doctor's office and have them have an exotic programmer to to configure it and the first thing that you'll have to do is learn to use it like a mad if you've never had arms and then suddenly you have an arm and you have to pick up a glass on the table it's like not a cognitive task you just like how how do you you can't think your way through that and so it's kind of a trippy experience at the beginning where like patience at first it just kind of wanders around and then they figure out how to break the symmetry and they learn how to control it and and that's like it's a long process it's like learning to touch type or play piano and so the for the first product where we're really focusing on three distinct types of control the first is giving patients the ability to control their mobile device to be as we heard from over and over again from patient groups that if you have to have a caretaker around the pressed buttons for you what's the point you might as well have them do the thing you have to get self-sufficient using using the devices on your own and then redirect the output from from your phone to a keyboard or a mouse on a normal computer it'll just show up as a as a Bluetooth mouse or a Bluetooth keyboard like any keyboard or mouse that you can use on any computer and as Elon mentioned this is now this is a forward-looking statement there's a whole FDA process we have to go through we haven't done that yet this is this is like these are aspirations but we are working as hard as we can towards our first in human clinical study next year and again these are plans but the the primary indication for that will be complete paralysis by spinal by upper cervical spinal cord injury and we're expecting that those patients will get four 1024 channel sensors one each in primary motor cortex some at supplementary motor area and dorsal premotor cortex which or two motor planning areas and closed-loop feedback into primary somatosensory cortex which is like if when you type or or walk or pick up a pen you don't there's aren't visually guided movements you have your body has all these senses of where it is in space and pressure and temperature and lots of other feedback and and we think for really high fluent control you have to provide that back to the brain for the synthetic effectors also and of course fully wireless and able to use it at home we think that there's a huge difference between something that you get to use two hours a week at the hospital versus something that you're living with every day and your brain is adapting to as much as the device is adapting to your brain and so to bring up the other other colleagues this team is like incredibly lucky you get to work with this team go into a little more detail on in that decoupling implantation from the electrodes is incredibly important the reason that you have these issues where things like these electric like a tungsten micro wires get rejected is they're stiff and they have they're stiff and sharp then they tear the brain and they have to because I have to get into the brain and so if you can decouple the process of getting it into the brain from what is left there where it can be much softer and have material properties like the brain and maybe be coated in things that help the brain recognize it as itself that's that's really important and then the thin film polymer leads the threads themselves are really cool material science and and we're going to more detail on that and then we'll also talk more about the chips and then a little bit more on just the neuroscience of how information is represented in in firing statistics in your brain and so with that I'd like to welcome dr. Matthew MacDougall thanks max I'm dr. Matthew MacDougall I'm head neurosurgeon at neural Inc when I'm not at neural Inc I'm a brain and spine surgeon here in San Francisco at CPMC California Pacific Medical Center before that I was at Stanford where I worked in labs that have implanted and designed advanced brain computer interfaces I originally became a neurosurgeon because I wanted to help people live happier healthier longer lives I've been humbled in practice by how powerless we are to treat many of the most debilitating neurologic diseases people afflicted with spinal cord injury schizophrenia autism and a host of other neurologic conditions have far too few options I work with neural Inc because we for the first time in history have the potential to solve some of these problems before we get to how we get the device in we have to talk about the guiding principle in our link safety everything we do it in our link is filtered through the question will this make me more likely to want to get one will it make me more like to recommend this to my family and friends this approach impacts every design decision we make so well for the immediate future knurlings devices will only be intended for patients with serious unmet medical needs our design philosophy is that this should be safe enough that it can be an elective procedure so what have we done to try to make it safe for starters we've created very small threads they displace a lot less tissue than the traditional methods in my regular practice today I routinely implant large deep brain stimulator electrodes into the brains of my patients they're big enough to have about a one and a hundred chance of causing a significant hemorrhage they displace and disrupt enough brain tissue that you can often see neurologic consequences just from placing the wire we can do better than that neural links threads are so thin that they're difficult to see with the naked eye they're much smaller than the width of a human hair they're small enough that a human surgeon can't actually implant them without help so we created help our link developed a tool that we're extremely proud of the robotic inserter inspired by designs conceived of in labs here in the bay at UCSF and Berkeley we developed this robot that can rapidly and precisely insert hundreds of individual threads representing thousands of distinct electrodes into the cortex in less than an hour this tool allows the surgeon to aim between the blood vessels they'll cover the surface of the brain with micron scale precision the region of the brain shown in this video represents only a few millimeters of surface of the brain as you can see the brain surface moves with the heartbeat and breathing the robot tracks and adjusts for this movement using this tool we can greatly reduce the risk of harming cortical vessels and causing bleeding here the robot is selecting individual electrode threads and placing them into the brain in the pre-planned location with remarkable accuracy and repeatability using this system we've been able to rapidly place thousands of electrodes into the cortex without causing noticeable bleeding we also have an in-house histology team that examines brain tissue to help us choose electrode profiles and materials to help us minimize tissue damage when you think of traditional neurosurgery you probably think of something very invasive traditional surgery on the brain isn't something that patients ever look forward to or are excited about except in the most dire circumstances usually a clamp is attached to the skull to keep it rigidly immobilized to the operating table we often shave all or most of the patients hair patients can end up with large visible scars at neuro-link we want to create an entirely different patient experience something more like LASIK no scars no big scars no hospital stays no short procedures sorry no hospital stays very short procedures and of course in the end you get to keep all your hair we even want this to be possible under conscious sedation that means you can get rid of the complexity and the risk of general anesthesia as well as many of the unpleasant side effects nausea sore throat from a breathing tube to be absolutely clear our first clinical trial patients are going to receive an experience much more like traditional neurosurgery but our aim is to simplify the procedure down to the injection of local anesthetic a very small opening in the skin a painless opening in the skull below quick and precise placement of threads into the cortex and then we fill that hole in the skull with the sensor allowing the scalp to be closed up over it behind the ear we'll make a small incision to insert the coil we will tunnel tiny wires under the scalp to connect the sensors to the coil that's the process I believe that neural Inc is going to be able to provide us in the medical community with a platform that can finally enable us to treat some of these very difficult to treat diseases also to understand them better I hope you find this as exciting as I find it if you feel you might be able to help us don't hesitate to contact us to talk more about the technology behind all this I'd like to introduce Vanessa Tolosa director of our neural interfaces group hi I'm Vanessa Llosa I lead the neural interface group at neural Inc our team consists of engineers and material scientists who are responsible for making the probes that get implanted into tissue the packaging for the electronics and integrating these two components together we also do all the testing and characterization of these parts before joining neural Inc I led a neuro tech team at Lawrence Livermore National Lab there we worked on a wide range of neuro first static technologies that were used both in the academic and clinical settings I decided to join neural Inc because I saw an opportunity to take all of this exciting work that we were seeing in neuro tech research and actually make them accessible to patients at a much faster time rate than what medical device companies have traditionally been able to do with that in mind at noir link we set out to create a fully implantable neural interface with thousands of channels that are capable of single spike resolution this device must last a long time in the body to do this it must be small flexible and made of biocompatible materials that will minimize the brain's immune response to protect the electronics from the caustic environment of the body it must have airtight packaging also known as hermetic Packaging the device must also be able to both record from and stimulate neurons this is essential for a highly functioning BMI finally the manufacturing process must be scalable and capable of making micrometer sized features consistently currently there are no research or commercial commercial devices that meet all of our requirements so we built one out of microfabricated thin film polymers just like in semiconductor chip manufacturing we use a layer by layer process that generally consists of three repeating steps we're either always depositing material patterning and materials through photolithography or etching away a material depending on complexity of the design these steps can be repeated over a hundred times and to make things more challenging we are limited to materials that are safe for the body in our current design we have a three metal layer process that results in a five micron thick and tend to my 40 micron wide probe to give you an idea of how small this is red blood cells have a diameter of about eight microns and an average strand of hair is about a hundred microns yet in the small footprint we're able to fit our electrodes our wires and insulation for each of those wires with micro fabrication we can drive features down to the size of an electron beam so this is great because we want to make our probes as small as possible essentially we want it to be invisible to the brain but there are other factors that limit the size of our probes for example as we make the wire smaller it increases the resistance of those wires and as a resistance increases it makes it more difficult for us to separate our signals from our noise similarly there are other technical challenges and trade-offs there related to higher channel counts and manufacturing yield electrode size and material and tissue much safety at neuro-link we have an incredible team that's been tackling these challenges and have been able to make high channel counts polymer probes and this image is a silicon wafer that holds ten of these arrays these polymer arrays in this design each of those arrays has over 3,000 channels so what that means is in this one wafer we've manufactured over 30,000 electrodes and over 30,000 insulated wires this is something that can't be done with the way current medical devices are being made that rainbow effect is caused by the small feature sizes on these devices that are interacting with the nanometer sized wavelengths of light that are reflecting off of them if you were to zoom in on the ends of one of these arrays you'll see these this region where we put all of our electrodes so each of these vertical filaments that end in a loop is what we've referred to as a thread and each of these threads can be placed independently into the brain using our robot during surgery this design is called linear edge it's one of over 20 designs that we've made for our R&D work we progressively been increasing the number of electrodes per thread without significantly increasing the width of each of these threads at the base we've been able to do this by adding layers and reducing the sizes of the of the wires down to as small as 350 nanometers this is less than a wavelength of visible light because we're using a lithographic process essentially if we can draw it we can also make it so in one end of our probes are the electrodes on the other end or where we connect the probes to the electronic package or to the electronics through conductive feeders this substrate is part of the Hermetic electronics package standard methods of connecting the probes to the electronics package usually involves some kind of large plugin type connector or a polymer based glue that bonds the two components together but as we increase density and decrease the the footprint becomes impossible to receive to achieve hermiticity in standard medical device connect connectors this is due to several reasons one of them is how these substrates are currently manufactured hermetic feedthroughs consists of holes that have been packed with conductive materials and are embedded in an insulating substrate as you drill more holes and pack them more tightly together these brittle substrates typically made of ceramics become more susceptible to cracking also as you make the hole smaller it becomes more difficult to fill them with this conductive material without getting non hermetic voids standard processing also requires exposure to high temperatures typically over 700 degrees Celsius at these high temperatures the coefficient of thermal expansion or CTE mismatch between the insulator and the conductor can cause circumferential cracking or interfacial gaps during the cooling phase we're able to get around these problems by developing a new process so rather than making the probes and then the substrates and then connecting them together instead we micro fabricate them together into one monolithic component this provides a tight seal at densities that current methods with standard materials for medical devices can't achieve so far we've used this process to make a hermetic thin film substrate with over a thousand connections over a 2.4 millimeter by two point four millimeter footprint next we assemble the electronics and then also attach a wired lid using a laser welding process these two steps have required a lot of internal development as well the result is the sensor that's ready for final assembly and implants into the body next you'll hear from my colleague DJ our customer about our custom electronics Thank You Vanessa my name is DJ Sol and I'm the director of implant systems and neuro-link my team focuses on building chips and systems to get neural signals recorded from our electrode out of the brain and also to put information into the brain before neural link I was at UC Berkeley where I co invented neural dos which is a technology to power and communicate with small implantable systems using ultrasound waves typical chip life cycle from design to verification to tape out is approximately one to several years a neural link we had the ability to co.design our chip with the rest of the system and the type feedback loop from this organization has enabled our small team of analog and digital chip designers to tape out a new design every three months on average that means over the past 24 months we've done eight papers in total representing 15 different chips that have been designed fabricated tested and used in development the artwork that you see on the top of the slide is of some of the actual chips that we made so far for any custom chips we make the architecture can vary substantially but the basic ideas are the same neural signals recorded from the electrode typically look like the one on the slide and in order for us to extract the information that we care about we need to first amplify filter and digitize those neural signals and use digital logic to process and send out the bits we want for BMI we also need ways to diagnose any issues with our electrodes and be able to drive stem stimulation engine to inject charge to the brain when required our latest trip is called n1 system on trip and it is physically small measuring only 20 millimetre squares or four by five millimeters it is low power highly configurable and it has 1024 simultaneous record and stimulation capable and it has on chip spike detection to dive deeper into n1 SOC I like to highlight three key innovations and they are one analog pixel two on chip spike detection and three stimulation on every channel the first is analog pixel before we can convert analog neural signals into digital bits we need to amplify and filter them and this is where the analytical comes in we want to have one analog pixel per electrode so that we can configure them independently so in the case of N one SOC there are 1,024 an old pixels and all that pixels also take up a significant portion of the physical space on the chip and how well they work determines both the signal quality and the characteristics of the overall neural interface the goal of analog design is analog pixel design is to make it as small as possible so we can fit more as low power as possible so we generate less heat and have longer battery runtimes and as low noise as possible so we get the best signals now the challenge here is that these goals are at odds with each other for example we want to achieve lower noise on the amplifier so that more spikes can be detected but as transistors get smaller it becomes harder to get lower noise while keeping the power the same or less since the start of nor link we've gone through three major revisions to the analog pixel progressively improving both the size and power while maintaining performance over the past 24 months we had seven fold improvements in the size of the analog pixel and our latest pixel on the right is at least five times smaller than the known state of the art of similar architecture with one pixel dedicated per electrode as published in the academic literature second innovation is on shift spike detection once the signals are amplified they're converted and digitized to zeros and ones by our on chip analog to digital converters as you'll hear in a second spikes or action potentials shown in this slide are often critical for certain BMI tasks currently there are several different methods for detecting spikes such as thresholding or more sophisticated methods such as principal component analysis and neural link one of the robust ways that we came up with is by directly characterizing the shape and it's worth noting that this is different than template matching and that it gives us more information in a general way in certain cases we can actually identify different neurons from the same electrode based on their shapes our analog pixel can capture the entire neural signals sampled at 20,000 samples per second with 10 bits of resolution resulting in over 200 megabits per second of neural data for each 1,024 channels that we would that we record in our previous systems that you heard about we were able to stream this entire broadband signals through a single USBC connector and cable and we performed real-time spec detection on an equal machine running our optimized decode now we wanted to completely eliminate connectors and cables for n1 so we had to modify our algorithms to fit into the hardware by scaling both making it both scaleable and also low-power and then we were able to also implement this algorithm in our n1 SOC our algorithms can compress neural data by more than 200 times and it only takes nine hundred nanoseconds to compute which is faster than the time it takes for the brain to realize that happen finally it was important for us to enable stimulation from every channel that we can record from and make a configurable and high resolution to make this work we custom-designed stimulation engine for electrical stimulation that can coexist alongside our analog pixels our stimulation engine has point to micro amp of amplitude resolution and 7.8 microsecond of time resolution there is a 16 to one ratio of electrode to stimulation engine so we can't stimulate every channel simultaneously but we can within each stem pulse usually in milliseconds and we can also stimulate any combination of 64 channels at the same time so in summary looking through our n1 SOC it has 1024 analog pixels that we can record from simultaneously with 7.2 micro volt RMS noise while only consuming 6.6 micro out of power it has on chip analog to digital converters on chip spike detection that can compress neural data more than 200 times and it only takes nine hundred nanoseconds to compute stimulation engine with point two micro amp of amplitude and 7.8 microsecond of time resolution and finally Diagnostics for electrode and impedance measurement all of these functionalities that I outlined are integrated into a single four by five millimeter silicon die next my colleague flip will tell you more about what can be done with these signals thanks DJ my name is Philip sabbaths and I'm the senior scientist at neural ink before neuro-link I was at UCSF where I was a professor of physiology there for 16 years I ran a lab that studied how the brain processes sensory and motor signals we developed new neuro technologies and we studied how to take those tools and use them for neural engineering applications today I'm going to tell you about how it is that we can use those amazing devices that Vanessa and DJ just told you about to communicate with the brain now specifically I want to tell you about two things first I want to show you that the work that we're doing doesn't come out of thin air we're building on over a century of neuroscience research and decades of neural engineering research these provide a solid foundation for the sorts of things that that we're talking about second I I want to show you why we believe that even more advanced applications are possible with more advanced devices now when Ilan contacted me over two-and-a-half years ago now and told me about his vision for the company I knew that I wanted to join for these two reasons because I knew that the technology was at a point where with the right team and the right to right vision and a long term vision we could do the sorts of things that we're talking about and I knew that with that team we could do things that no one had even dreamed about yet okay so the first thing I want to show you is a video many of you who are seeing this have seen videos like this before so you know what it is but if you don't know what this is I have the distinct pleasure of telling you that right now what you're looking at is the brain at work eat this is in fact traces of a bunch of electrodes that came off of one of our devices a bunch of electrodes from a single thread and each trace shows you a voltage waveform in time as it's coming off of one of those threads now if we focus in on one of those traces the first thing you may notice is that there are these big voltage deflections that happen periodically and these are the spikes that max and Ilana and others have talked about these spikes occur again when a neuron has an action potential and this is the fundamental element of communication within the brain and this is the thing that we want to tap into this is what we want to be able to record now as DeeDee just told you we have algorithms that can detect these spikes in real time as they're happening and that allows us to collect data that looks something like this this is what we call a spike raster so each row there represents one channel of recording and time goes from left to right and each of those little tick marks is the time of a single spike in action potential all right so presumably there's some information somewhere in there how do we get at it what are we going to do with it well for the first application which max told you about which is allowing paralyzed individuals to be able to control a computer what we want to do is we want to reach into primary motor cortex and record the activity that's happening their primary motor cortex is the part of the brain that sends signals down the spinal cord and to the muscles to drive movement of course it does that with action potentials and in particular we want to record from the hand and arm portions of primary motor cortex so imagine imagine that you have a person sitting holding a mouse and they're sitting still and then they make an outward movement with their mouse and then they reach back what would you see in the brain well here's a here's a synthetic neuron I made data up but but it gives you the idea here's a synthetic neuron that shows that in the background activity when the person's at rest maybe there's some firing but when that neuron when that person reaches outward that neuron starts to fire a lot and when he reaches back the neuron becomes quiet so this is what we call a neuron that's tuned to a particular direction of movement now maybe we'll record from another neuron and this neuron may have a different pattern it may be tuned to the return movement and not to the outward movement so it fires more on the return what if we ask the person to do that movement again what we would see is a similar pattern of activation so the neuron on top still fires more for outward movement in the burner on the bottom still fires more for the return movement but you'll notice that the patterns are different and that's because neural activity in the brain is random it has stochasticity which means that even though the person may be intending to do the exact same thing from one movement to the next the neural code the neural representation at the level of an individual neuron is noisy and this is just one of the reasons why we need to record from lots of neurons in order to be able to gain a high fidelity readout of what the intention is so okay so let's say we record from a bunch of neurons it might look something like this if you look at that you might think that looks pretty messy and it's not clear what's going on but I'm gonna do a little trick I want to take those neurons I want to rearrange them so that they're in the order of the tuning that they have look just as I told you about those two neurons and if you do that look what happens now suddenly structure emerges and I think you'll agree looking at that but there's information in that stack of neurons that tells you about the movement and that's exactly we want to do we want to do that kind of magic in an automated way to read out and to read out the movement the way we do that is by building something that we call decoding algorithms these are mathematical algorithms that we tune based on data like these to be able to take in just those raster's of spiking activity and output the movement that's that the person wants to make okay so for these little fake data I built a very very simple decoder and sure enough it's able to to capture the intended movement this is what we want to do on skel no you might say to yourself I don't understand your talk about moving but I thought it was about paralyzed people right so how does that work well it turns out we know from a lot of prior research but even if you're not actually making the movement even if you're just thinking about the movement or even if you're watching someone else make movement the cells and motor cortex respond in a similar way so we can build up these decoding algorithms just from from those kind of data and then a paralyzed person can think about moving the mouse and the cursor will move now this this kind of decoding has been done in a fair number of academic labs including my own before I came here and in humans and academic studies well what we want what knurlings goal though is to be able to do this with a clinical device that people can take home and use on their own and there's orders of magnitude more channels or orders of magnitude more neurons that we're recording from with that we think the people will be able to get naturalistic control over the computers not just a mouse but also keyboard game controllers and potentially other devices that's what we're trying to do now I've told you about the arm and hand area of motor cortex but the devices that we're talking about because of their high bandwidth and the ability to tailor the location of each and devote individual electrode to a person's individualized cortical anatomy we should be able to reach anywhere in motor cortex so for example there are areas at the base of motor cortex that are responsible for driving activation of the speech articulate errs there was a recent lovely study from UCSF that showed that from activity like that you can actually decode the speech so you can you can decode the movement of the articulator and from that you can create synthetic speech so potentially with a device like this you could restore speech to a paralyzed person who's no longer able to talk but there's no reason in principle that we can't reach all of motor cortex and that would give us access to any movement that a person thinks about any movement at all a person could imagine running or dancing or even kung-fu and we would be able to decode that signal so that could give a paralyzed person the ability to control say for example a 3d avatar that they could use for online gaming for sports it could allow them to control a wide range of assistive robotic devices and ultimately if and when the technology for spinal cord nerve or muscle stimulation gets far enough ultimately it could be used to restore that individual's control of their own body okay I've talked about readout but we remember we want bi-directional information we don't only want to read information out of the brain we want to be able to put it back into the brain now some of you that may seem a little bit fantastical that you could write information into the brain but actually the the basic building box of that technology are already there this is the same image that you saw before of an electrode next to a cell it turns out if you pass a tiny amount of current through that electrode what happens is that you activate cells nearby you cause them to to fire an action potential one or many and that is the technology that is already being used widely outside the brain say for example for cochlear implants which have been used for decades to restore hearing to the death and more recently in the eye to restore vision to the blind in a fairly rudimentary way as I'll tell you more later but in addition you can use the same technology in the brain for example to restore the sense of touch or to restore vision and I'm going to tell you very briefly about those two applications so let's start with the sense of touch consider this little bit of tissue that of brain that I've just highlighted here that's at the border between motor and somatosensory cortex so if we blow that up what you can see is that somatosensory cortex has a very special property it has what we refer to as spatial spatial map and what I mean by that is that there are regions that encode the palm of the hand and each of the five digits for example so if we were to stimulate at one little location say in the thumb part of the of the cortex the person would feel a sense of touch of pressure on their thumb or if we were to stimulate two sites on the palm in the palm area of cortex you might feel a couple of points or touches on your hand this kind of technology has been demonstrated in in many academic labs and in a recent really nice paper it was shown that with subjects controlling a robot arm through BMI getting tactile feedback of when that arm or when the hand of that arm was grasping an object improved the ability to pick up and place objects with the robot so this is this is the kind of thing that can really help decoding so imagine what we could do if we're able to take our device and cover all of somatosensory cortex we could give rich sensation of objects of haptic feedback when you're manipulating objects we could maybe feel different textures but it's not just about improving the user experience it's also about getting to the level of functionality that we want imagine for a second imagine typing now imagine typing with your fingers anesthetized that's going to be pretty hard so that haptic feedback that sense of sensory feedback during movement is going to be important going forward and and yeah okay so that sensory feedback for the hand we can also potentially provide visual feedback so visual cortex just like somatosensory cortex has maps so there's a spatial map in visual cortex which is here in orange in the back of the brain so for example if we stimulate a particular point in cortex we might see a flash of light in a little punctate spot in front of us and this was demonstrated many years ago in in by neurosurgeons and it's been used in academic labs and that we call that a phosphine and you know if you stimulate another area look at a phosphine in a different location so the idea here is that you could stimulate a bunch of different areas and you could create kind of like a dot matrix image of the visual world which could provide a rudimentary form of vision and there are academic labs and even companies that are working on technology just like this but there isn't just one map in visual cortex actually there are a bunch of different maps was a good example of how the brain works is a spatial map but there are also there also maps telling you about the orientation of edges in the field their maps telling you about color there are maps telling you about the size and speed of objects moving so what we want is a device that has sensors that are small enough electrodes that are small enough and high enough density that we can tap into that rich collection of maps with our stimulation devices so that we can do better than just dot matrix so that we can actually create rich visual feedback for the blind that's that's the long-term goal okay that's just again one more example of the way that these devices can be used so I've talked about recording signals and I've talked about stimulating you can combine those two to treat a variety of neurological disorders max talked earlier about deep brain stimulation to treat say for example Parkinson's disease and many people have have those devices in academic labs have recently shown that you can do better with stimulation you can treat better if you also are able to record the state of the brain say for motor cortex and use that to shape the pattern of stimulation deep brain stimulation or DBS has also been used for dystonia is it already proved for dystonia and obsessive-compulsive disorder and we think again close the therapies can do better and in fact for epilepsy there's already a commercial product that does this kind of closed loop seizure detection and disruption although it does it with only about eight electrodes there are many other sorry many other diseases oh no all right we'll get there sorry there are a number of other neurological disorders where DBS has promise but it's still an investigational stage like depression chronic pain tinnitus now even though these diseases are currently being treated with these big DBS electrodes like you've seen we think that there's a potential here for the kinds of devices were or designing an individualized highly focused treatment that will reach broader patient populations and be able to be more effective in the way that they treat these disorders alright lastly I want to tell you about not just sensory input and motor output but about about thought so there are parts of the brain where we know that there's neural activity that encodes the things that you're thinking about and one great example is an area called the hippocampus the hippocampus is involved in memory formation and it helps store episodic memory things that you remember from your life it also has a particular kind of memory for locations and views that you know for example it'll have cells that represent places in your own home or in a city that you know well so imagine that you had that you could record from a collection of neurons in the hippocampus of somebody who lived in San Francisco and knew it well then it's likely that they would have some neurons there in the hippocampus that represent various locations in Golden Gate Park and so if that person were to take a car ride for example from the ocean through the park you would see those neurons fire in order as they took that ride first a neuron that maybe represents a view of the ocean and then the Bison in their Parekh and so on so I've told you about the way that the brain represents information and that these these sorts of of encoding methods representations in the brain are things that we can learn to decode and I've told you about some of the applications and how they might work as the device technology gets better and better and as we get more and more experience with those devices we and other researchers will be able to bootstrap off of those advances to reach other brain areas and other applications that's that's what we're trying to do nor links goal is neuroscience has shown that a wide variety of information content is readily available in the brain we know for example that there are signals that encode speech and language there are signals that encode your mood there are signals that encode the sense of pain when you're hungry and when you're thirsty there are signals that encode your memories and even esoteric things like mathematical reasoning but knurling wants to do is to give people the ability to tap into those representations to get act better access to that information both to repair broken brain circuits and also to ultimately give us better access to better connections to the world to each other and to ourselves all right thanks so we're gonna take a risk Aaron just do some Q&A so that's hopefully was a good understanding of the brain forever yeah nice nice work guys like it's a very proud of the neuro-link team it's done amazing work and yeah it's a really smart smart group I was a lot more where that if there's a lot more really smart people so what you're saying here is the outcome of a lot of hard work by the new rolling team and yeah I think it's it's there's some pretty impressive stuff so we take some questions from the audience it's the lights are bright so you might have to like yeah just take you later this is this is a really interesting question so it's it's definitely too early to really think about this there's nothing new on I don't I think we'll hold open the custom code immediately yeah I think I mean it's conceivably that could be some kind of App Store thing in the future or some sort of platform with like a very rigorous you know verification of the application but yeah I mean I think that there cuz this is certainly not meant to be sort of a closed system ultimately if we can enable others to contribute whether they're at Munich or not that that would be a good thing you just add one more thing to this we've had some discussions like it might be that if you want to build an app or a business on top of it like a brain enabled API then your business model can't be advertising for example there's that's like it's very important to us but there's like that's that this turns out well for everyone and so there's there's thinking like that going on [Laughter] yeah there's no doubt that that plasticity will make things so the question was whether neuroplasticity will help or hurt our effort and I think there's no doubt that it will help first of all there's just the effect you have to learn how to use these devices but for example in work that we did in my lab earlier we showed that you can write in information that isn't perfect it doesn't get that map perfect or even it can be somewhat quite different from the map but ultimately you can learn to use that through plasticity and you know it's it so it's it's there's going to be a lot of learning required and the ability of the brain to adapt to new information and a particular the ability of the brain to take information that comes from multiple sources and merge it in an effective way is I think the kind of thing that really will will facilitate complex new tasks with these devices it's a sense reason that the the neurons are responding to electrical pulses so it as a the electrode is providing electrical pulses it's it to the neuron and electrical pulse is is a neuron it just thinks it's in your and it's gonna for sure adapt dynamically because it just sees electrical pulses and it's it's going to respond to those [Laughter] we cannot do that that would be so the question is if this will not be advertising driven which I think would be unwise then how will it be paid for essentially is that correct or how will you ensure that it's broadly available yeah well I think that the cost of these you know brain disease or brain injuries is extremely high to society if you have to take care of somebody or that they need if they need comprehensive medical care or hospice that this is actually very very costly to society so I think it the economics of solving for that make a ton of sense and if you enable somebody to you know work and be a productive you know it could you know could Rivard contribute to the economy I think that that will I think that the economics of that will easily pay for itself and and then in the limit of course if you want to be somatic with with a I feel like I think it's safe to say you could repay the loan if with superhuman intelligence I think it's a safe bet so I think the economics this will work out and the first order is really just to make sure that it works and works safely and then and I think it'll really be the option of other person but but it is critical that this be so as we've talked about for like a laser Glock device if if one has to be if this has to be done by a neurosurgeon it is it cannot be scaled there's just aren't enough neurosurgeons so it must be just just as one one wouldn't want sort of like a hand operated laser for you know an ophthalmology situation you really want the robot doing it with precision the same thing goes for the brain interface so sure yeah it says the question is have we implant the chip in animals and if so what are the results so I if it's important to say that we regard you know any any triple implants even if it's an in a rat as a very serious thing like so we care we even care about rats even though they have the Black Plague and everything you know so like the arguably have some karmic payback if it but nonetheless nonetheless we're we care about rats so and then we're extremely sensitive with with with monkeys and we work with University of California at Davis for the the any of the monkey activity so and the results have been been very positive do you want to every talk about I know this is this is a sensitive subject yeah but I think it is yeah we definitely need to address the elephant in the room yeah I think that it's there's we wish that we didn't have to work with animals right that we just wish that wasn't like a step in the process but it but it is it's like it's a very important part in the research and development process to produce better outcomes for human patients and improvements in human health and we're try to be very thoughtful and and we follow the the 3 R's of like reduction replacement and refinement of laboratory animal medicine and and we try to be very careful and thoughtful about it and do it as efficiently as possible because we believe that the benefit to humanity is is in the end like about the the benefits outweigh the negatives but the question are also asked about the results and there is paper available I think now soon that has some of the results in them yeah but we have made a you know monkey has been able to control the computer with its brain just yeah why I didn't really start running that result today but there goes the lung he's gonna come out of the back so much fun a point oh yeah so can we speak about the FDA path that we want to pursue and how we might work with the scientific community I guess yeah sure it's well let me start with you well I was also said like we're under no illusion that we think we can do all of the science required for this ourselves like there's an immense amount of neuroscience to be done with these devices and there's a huge amount that we have to learn about the brain and and that's going to be a much larger thing than just a neural link and we want to get these tools into the hands ventually at the right time I think that we're still very small company just focused on getting our first patients and we have to be laser-focused on that but we want this to be a thing that is much larger than this to be a field right we want this to really fuel advancement of the field because the most important thing is not that like neural link is this like one specific place but that it advances all of us and for the for the FDA there's there's a pathway we're pursuing an early feasibility study IDE and it's and and there's the the FDA actually put out draft guidance in February that's very specific to the type of thing we're doing and it's pretty prescriptive it's it's a checklist of what they want to see and and there's a lot in it you have to show that it's gonna be expected to be safe and biocompatible and and stable but you work through that and you give them the documentation I mean people in academia right now quite constrained in working with the the Ute are a is that that's the most advanced thing in academia and our system is at least a hundred arguably a thousand times well it'd be on the order of a hundred I suppose relative to the potential of the Utah Ray tours of magnitude improvement at the experimental level so I think it probably would make sense for us to make more of the robots and provide the chips to academia to further the science sure hard for me to see so you've obviously been working under the radar for quite some time and you've made significant advances now that you're public and very obviously recruiting today and I would assume looking for additional academic partnerships to sort of accelerate your development what is the best way for to get in touch with you and start to show what is best way for academics get in touch with us to collaborate on furthering the field and and yeah hey guys first if anybody out there has technologies or ideas that they haven't heard us talk about today and if they think could be useful for us they should reach out and tell us about it we're always interested in new technology and new things that are happening in the field so that's a first as far as getting the academic community access to data for example this is something that we are committed to doing the details the pathway on that aren't are fully worked out there a number of options and to be honest if you have ideas about ways that anyone in the academic community thinks that it would be good to engage with us we're willing to listen and we'll we'll pick one or two and we'll we will make it possible for the technology that we're working on to have a very broad impact on the field that's definitely one of our goals yeah I mean the things that really drive the technology are you know advancing that the chip design the the software on chip and for interpretation of the results material science for the especially for the coatings of the electrodes like how do you have a long life electrode is it's quite a difficult thing to get get that coating get the materials right and then the application materials for the Mayo something that you you you want to be because you want these electrodes to last for many decades I in the brain but the is quite a difficult environment it really wants to corrode so hacked getting the right the right coatings is incredibly difficult it's a tough material science problem sure right dude do we consider longevity a solved problem definitely not so I think the longevity is one of the key questions and they supposed to say like until you actually haven't implanted it how long does it last and then if it does if it does start failing does a fail in a benign way or in a bad way so I think the it's there's not enough time yet to actually say whether it is a long time with the network obviously makes sense to have accelerated life testing of the electrodes so in a non brain situation so you figure out something that's actually a worse environment than the brain and actually which is actually quite a difficult environment for a chip so and electrodes II if I find something it's even worse and have accelerated life testing that's that's one of the key things and then they need to confirm that in and in in a actual brain so I mean the the latest results are quite promising but it's it's too early to reach conclusions conclusions I think we've just recently seen what we think is a rock will break through but we're time will tell for sure how will we address the mechanical mismatch of the electrodes and the brain essentially because the cells are like jello and electrodes are really hard and so they have a big stiffness difference between brain cells and metal electrode yeah yeah I mean that's part of the reason one of the reasons why we went towards flexible materials like there are a lot of different kinds of probes out there right now if you look it up and that's that's the main purpose of going flexible and also going smaller so as I said that we're really trying to make it as invisible to the brain as possible and then on top of that as was mentioned coatings so all of those combined we're hoping to significantly decrease that immune response but essentially have if you have something with high modulus and something with low modulus but if you make you for making the hime ologist thing very thin it becomes quite flexible yeah moment ever no sure I said I'll try to answer question with the back there I could talk to me to see behind the camera so I think if somebody's a question like essentially if we look at like higher-order feedback where it's sort of at the maybe a whole limb level or combination of limbs or a whole word or letter as opposed to an individual phosphine or something like that okay I think that of course that's a stimulation result not a recording result and it may have something to do with the kind of low resolution of the stimulus that the psychophysics of that it's hard to perceive it as a complete letter but you get filling in of motion that helps you perceive the object I that's kind of sort of my thinking of what's going on and so you know I think these are the kinds of questions that honestly we we haven't yet addressed when I talked about visual stimulation and the kind of rich visual stimulation we want to just to be clear that was aspirational and so you know come back and ask us in a little bit you know yeah but there are is the you know they are like individual neurons that you can't trace to particular names and concepts and people and you know at a kind of advanced long-term level I think people would have kind of like if you if two people had a neural link you'd be able to effectively have a sort of really high bandwidth telepathy or you know who actually technically going over radio waves but it would you could actually communicate at the sort of complex meme structure level using the Dawkins version meme yeah so then you really can't have like potentially a new kind of communication it's sort of like conceptual telepathy essentially it also be consensual I'll try to answer something in the back yeah I can hardly see you but anywhere anyone in the back there basically Thanks [Music] haha okay what's your answer to what do you think we should do I mean we're open to ideas here the overarching objective is to make future better aspirationally and and you know hopefully not pave the road hell with good intentions I think the word hell is mostly paid with bad intentions though big question yeah I think that's it's it's a it's a big philosophical question what what we'll it's hard to say what the future will be with something like this brain machine interface I I doubt that we would be able to eliminate all suffering and it actually maybe ultimately hardly dystopian if we do it eliminate all all suffering if that actually may not be a true utopia you know there's like generally like stories about utopia tend to turn into just Sophia so I but I think we can definitely make a significant difference and we can address that you know when I say we I mean humanity can't address a lot of the suffering that occurs in the world and make things a lot better and I think a lot a lot of times people are quite so if neg about negative about the present and about the future but really I think if if you're a student of history the you when would you when else would you really want to be alive knows now's the best time pretty much it's like those who think the past is better I'm not right enough history i wreck your way in the back there I see actually you can see a hand yeah yeah yeah I think that this but so so know you often have very tight latency constraints in this yeah they have to run locally yeah so the question is how are we doing the backend computation what are some of the methods that we're doing it yes max just mentioned latency is something that we do I want to minimize as much as possible especially in the context of doing closed loop PMI so being able to record and also stimulate and put information back into the brain so in order to make sure that that latency between the end-to-end system is minimized we need to have all that computation locally so that's you know one instance of the evidence of spite detection that you saw that's slowly progressed from you know computation on to from the computer on to the integrated trip is something that we strive to do as well with close look beyond my algorithms yeah essentially the the vast amount of the computation is actually done locally on an ASIC effectively so the amount of data that needs to be communicated beyond the body or the brain is really distilled down to a small amount and and as these are saying like especially important for if you if you want to say it gives on who is a tetraplegic the ability to type at 40 words minute which is one of our goals and that that requires a very very low latency feedback loop I think okay that last question who do okay go yeah far away okay this is a few questions packed in there so one of the first question was I mean if it actually essentially summarizes what we're saying is like what's the ratio of electrodes to neurons because you wouldn't want a one-to-one ratio because that's a lot of electrodes so you really want to actually have that ratio of years as big as possible and ideally at least a hundred to one ask maybe a thousand to one ideally so because the big of the ratio of neurons to electrodes the fewer implantations it's going to be just way better so for sure you'd want what you'd want to to read a whole bunch of neurons and then be able to stimulate neurons and or a cluster of neurons by varying the field potential so that you don't you don't need to have a one-to-one stem or read ratio what I deal II 100 to one because when I've had more yeah I think it's important to add to that that any answer we give to what's the right number of electrodes at this point is speculation yeah that's that's part of the cutting speculate yeah but but but but the point I'm trying to make though is that this bootstrapping that we anticipate is going to happen we're gonna put in devices they're going to be lower level from that we'll be able to find out the information content and information density in the areas that we're looking at with the electrodes we have and we'll and we'll be able to bootstrap up from that so you know I actually believe that we are the ones that are going to be able to answer that question without speculation and I'll say when you look at the anatomy of the brain the brain is mostly silent like if so it on average a classic electrode can see somewhere between 0 & 4 neurons electrically by by different wave form templates but when you just look at the anatomy and like the distances you'd expect it to see more like a thousand and so this question of why is the brain so silent is an interesting one and one of the hypotheses is that a lot of neurons that are very narrow receptive fields that only fire when they have very high information content updates with respect to something specific and one of the challenges with spike sorting is that you can't tell like you can't tell apart like another spike of a neuron that you think you're recording from with that was the first time I heard from that neuron specifically that I don't know is there right and as you have these long lasting chronic devices you'll be able to get more of that out in the decoding yeah I mean I think it's the kind of very the the electrode in your own dense density is gonna vary quite a bit depending upon what part the brain it is as well if it's sort of sort of somatosensory sensory it's probably like a pretty big ratio and like you can like you can get pretty impressive results like controlling you know uh essentially a cursor with your brain is you don't need to buy me like electrodes for that so and there's a lot of neurons in the sort of motor cortex so I think there's some cases where you you could have all right this is just this is really speculation of course I mean some of some places where you could have maybe 10,000 won and some places where you don't want maybe ten to one it could probably I suspect it will vary quite a bit I just really quickly I think dance questioned businesses or longevity is a really important question we think about all the time I I don't think that we're releasing histology in the paper today but I think that that just to put some pressure on this team I think that we're running that around as a phone probably there's some really cool stuff in progress alright thanks I'm overcoming thank you great questions [Applause]",
    "status": "success",
    "error": null
  },
  {
    "video_id": "rVSb0u9OTtM",
    "transcript": "[Music] hey everyone thank you all so much for coming my name is gam I'm a software engineering lead at neuralink and today we're going to be talking about neuro Lake which is our internal data platform and the recurring theme through this presentation is going to be that we are building really simple systems for really complex data and simple doesn't mean easy and I really want to show what we've built uh over the past couple years so to start out I can introduce myself so I'm Gotham I'm a software engineering lead at neuralink for our full stack internal development team and our data engineering team so our team does a whole bunch of stuff it's all really interesting it's a really fun team to work on lots of unique verticals to drive we do data engineering some of which I'll talk about today we also do manufacturing software we do full stack development for lab Lab management tools we build visualizers for a billion pixel images um we have a lot of different things going on and yeah I can't even cover a tiny percentage of it today so if you have more questions please feel free to come and ask me afterwards so before we talk about neural Lake let's talk about neuralink so I'm sure some of you have heard about neuralink in the news um but for those who haven't so neuralink is a company creating a generalized brain interface and what we mean by that is a generalized IO interface into the human brain that can read and write signals from the brain and the reason we're doing this is to restore autonomy and help those with unmet medical needs there are millions of people in the world that suffer from not having their medical needs met people with spinal cord injury people with blindness people with ALS and more and what we want to do is solve these ailments of the brain and of the body with a generalized brain interface and in the long term we really want to unlock human potential for what it means to be a human and really unlock what the human species are capable of so how are we how are we doing this well we have a stack um and the nuring stack is composed of three main components so on the left here you can see that we have our device is called the link and this is an invisible device that is surgically implanted into the cortex of the brain you can see that there is a chip it's a circular disc about the size of a US Quarter and that chip has long thin electrodes coming from it those threads are extremely thin much smaller than human hair extremely flexible and can be inserted into the brain with minimal um minimal invasiveness and these threads have electrodes on them that can sense brain activity so as neurons activate we have signals going up these threads into the chip and the threads are inserted via a surgical robot these threads the these threads are too thin and too flexible to be inserted by a human hand so we've built our own Neurosurgical robot to insert these threads avoid vasculature and move with Micron precision and once these threads are inserted and reading brain data we send signals over Bluetooth over to neuros Signal decoding software and this software can classify brain signals into something useful in the external world this might be moving a cursor on a screen this might be moving a robotic arm there are many things you can do once you have these signals on a computer and the purpose is to decode neural intent and again the purpose of this is to restore autonomy so we want to help people with unmet medical needs and two cases here that we have are um telepathic control of a computer cursor as you can see on the left for people who have motor impairments and are unable to move their arms in a way to control a computer we want to allow for telepathic control and this year we did start our first clinical trial and we have one person who is actually benefiting from um this therapeutic case and another example of unmet medical needs that we are hoping to address is the ability to cure blind you can imagine if you have a perfectly working visual cortex but have other issues with your uh retina or optic nerve that prevents you from being able to see we can send signals directly to the visual cortex and stimulate the brain and induce Vision so both telepathic control and restoring Vision are two applications of this device and you can imagine because our brains make up a core part of who we are and what we do um there are many many many applications for helping people around the world so let's talk about data at neuralink and you can imagine there's a lot of data involved here right we have data from the brain we have data from our devices and we have a lot of data to manage and the data is incredibly complex so neuralink the neuralink stack I just described is one of the most complex medical devices to have ever existed and we have correspondingly complex data we have of course the brain signals and the signals that come from the device we have Telemetry coming from the device that's that's um safety critical that's indicating um whether the device is functioning properly we also have the brain signals themselves and we also have manufacturing data as we manufacture our devices we have information about the manufacturing history and machines on the manufacturing lines we have histopathology data that is large billion pixel images of tissue that um we use to assess the safety profile of our devices we have data collected during surgeries to make sure that our surgeries are safe and effective and more this data is incredibly multimodal it's incredibly complex we have images we have video we have huge images we have logs that uh that end up being terabytes large so this data is very very very complex it's not just simple tables or simple log stores there's a lot of different modalities we need to address here and we have a really strong design Philosophy for our Data Systems we want to have simple systems and again simp Le doesn't mean easy but simple means that we want systems that can scale down to a single developer machine and up to stateless clusters the development experience for a developer who is building the system should be seamless running something on your local laptop should be the same as running something in production and we want to really prioritize the local development experience instead of having large distributed services that we need to put up like a spark cluster to be able to access our data which runs in a jvm and can run slow on a local laptop we want to make sure that we prioritize the local development experience by using composable libraries and we want to avoid large stateful distributed clusters for our data lakes and warehouses we don't want to have a a database cluster that we need to maintain in order to store and access our data and finally we want to have code that's a catalog in and of itself and what we mean by that is we want to be able to generate a catalog that contains all of our data sources so internal users can access this data and Discover it and we don't want to have a database server and like all this complexity to maintain when we have this catalog application we want to just be able to define a table in code and have that generated catalog so you can see the Simplicity here from the end user experience internally when users access our systems or when developers build the these systems they can replicate uh production environment locally they can build a system easily using composable libraries and end users can access data without us having to maintain a bunch of different services so there's three main prongs that I'm going to cover here today we're going to cover ingestion Discovery and access so ingestion is the ability to ingest data into our systems so we want to have low latency streaming ingestion pipelines and we want to have an elegant way to handle schemo versioning and backfilling of our data lakes and as many of you can guess we we're going to talk about Delta Lake a little bit as a part of this we also want to allow for Discovery now with all of this complex data it it's really hard for people to know where all the data lives no one person knows where all the data lives in their heads right they need to be able to organically have a certain query or certain problem they're trying to solve and then look at our or look at our catalog and find data organically so we want a simple data catalog that's generated from source and we want low code dashboards that people without software experience can build that are generated from our catalog definitions again a single source of Truth for all of our dashboarding data cataloging and so on and then it should be easy to access now a software engineer can of course write a query to a Delta table and get get some information but we want everybody at the company to be able to access our data so our goal is to have a single click to get a data frame you click a button copy some code and you have a data frame on your computer that you can you can play with and query we also want to create autogenerated SQL and rest apis from our data catalog so that our internal dashboards can query these apis and again using the single source of truth of the data cat catalog so this is a 32,000 foot view of what our neural Lake data platform looks like um and this is really just a part of it again I can't cover all of the complexity of our data um today but we have data sources we have offline batch writers and low latency writers for processing that data and putting it into our data data stores and we have our data stores themselves we have Delta Lake we have relational stores uh and so on and something you'll see from this talk is that our our data stores can be spread across multiple different instances so we can have data in Google Sheets if we have data in spreadsheets we can have data in Delta Lake we can have data in postgress tables if we have high transactional needs which we do and all of this data is then easily discoverable and accessible via a data catalog via a python client and Via visualization tools and what you'll see is that the the catalog the python client and the visualization tools all come from the same Source there's a single source of Truth for defining these and then everything else is autogenerated which makes maintaining these systems very easy so first I'm going to focus on the data sources the low latency writers and how that data ends up in our Delta Lake stores and there's a lot more to talk about with offline batch writers and so on so if you're interested in those use cases find me afterwards so before we talk about our data ingestion let me just give you some primer on our Technologies we use so we have Delta Lake and we use the library Delta RS in our realtime inje pipelines so what is Delta Lake I'm sure a lot of people here are familiar but for those who aren't Delta lake is a open source project and is essentially a file protocol or file format that allows you to have acid compliant trans actions on top of blob Stores um a lot of data Lakes before Delta Lake were just parket files on S3 or on um a different blob store and the downside to that is you can't really get any kind of assd compliance across multiple files right you can't have transactionality across multiple files because these systems only allow for atomicity for a single file and you might might have partial rights if clients are reading from multi multifile stores while you are also updating multiple files at once and we don't want that so with Delta Lake we have a transaction log and park files on S3 and as long as the client and the writer both uh follow the protocol which means that they both read from the transaction log before pulling the paret files we can ensure that all of the Delta rights are shown in a transactional way so you'll you'll never have partial rights seen by the client and the Delta l prot is implemented by Delta RS so Delta RS is a rust library that implements the Delta Lake protocol it allows for writing and reading of Delta Table stores so the Delta RS library takes care of both reading and writing the Delta Lake updating the transaction log doing Atomic commits across multiple files and so on um the Delta RS library is written in Rust it's incredibly performant and it has uh it's highly concurrent and it also has python bindings so you can import the Delta Lake Library uh in Python and you can run it locally on your machine you can run it in a stateless cluster and the developer experience is the same so we're going to talk about a use case for Delta Lake we have multiple Delta tables each of them have slightly different use cases at the company we're not going to cover all of them but I do want to cover one interesting use case that we have so this is real time or close to real-time data ingestion into our Delta tables so on the left you can see that we have the and one implant being used um and while it's being used we have data being emitted we have events being emitted from the device these may contain neural signals and we also have events being emitted from the usage of a computer so the individual using the neuralink device is going to be playing a game on their computer they're going to be moving a cursor in some way and information about that cursor and about uh their neural signals will be being sent um into an ingestion API and this ingestion API stages these events onto a message queue then which are then consumed by a writer in batches written to Delta Lake stores and downstream those are consumed by rust based query engines and the purpose of this is to train a model to better calibrate to the neural signals by a user so as we have neural signals leaving the device we also have events on the game itself from the cursor movement and we want to align the time stamps of these and create labeled data that can be trained on so this whole pipeline uses Delta Lake as the main store for the um label data which is then queried via Delta RS or um uh other rust based query engines and that data can then be trained upon and this whole pipeline makes it so that the label data is available in seconds so in a few seconds we have all of the data available for training and this is is updated live and the reason we want such real-time availability is because during a session our Engineers might train multiple models and we want to be respectful of somebody's time if we're we're engaging in a session with them and we want to make sure that we can train a model as quickly as possible to try to converge on an optimal solution so let's really focus on the message cu the writer and the Delta Lake instance so we're going to talk about how the writer actually writes to the Delta Lake instance while consuming from the from the message CU and how the endend flow works for data to arrive in the Delta Lake store so the neural leg writer instance has three primary processes we have a writer process a compaction process and a vacuum process now these three processes each have a different function the writer process as you might guess consumes batches of messages from the message q and writes them in batch to the Delta L table now there's a problem here we have messages arriving very rapidly um in real time and if we write a new right to Delta Lake for every batch of let's say like 10 messages then we'll end up with a lot of small files in our Delta table and if we have a lot of small files in our Delta table then we have a problem because Downstream readers need to read a bunch of small files on S3 and that means that we have high latency for our reads because we need to do individual get requests for multiple files so instead of doing that we have a compaction process that once files are written to S3 at some Cadence right now we have it configured to be once a minute um but this can be tuned we have a compaction process that reads all of the files within a partition and compacts them into a single file now in Delta Lake this is possible because again we have assd compliance across multiple files this wouldn't be possible POS if we had a bunch of just park files on S3 right so in this case we read all of the files that are uh in the Delta table compact them and write back a single file and now when clients read that single file um it's uh it's it's very fast to be able to um get that single file into into memory now there's a problem though so with a compaction process running alongside a writer process we need to make sure that all of the commits themselves are serialized now remember Delta lake has a commit file and it has parquet files and both readers and writers access the um the the commit file the transaction log before they access the parquet files now if two simultaneous processes are trying to write to the same file you could have a conflict and this could lead to data loss right so we need to make sure that we have a serialized commit even if our rights are happening simultaneously so the compaction process takes a long time it takes some time for compaction to run but that can run in parallel while rights are continuing to come in and once the compaction is complete and the compacted file is written to S3 we can then serialize a commit and commit to our transaction log and we do that by we have a there's a custom put if absent semantic in um in the in the Delta RS Library where we use Dynam DB and we take a lock on on the um on on the commit file itself and once we take a lock on that commit file we can then commit and then release the lock and then the other process can can write and there's some more complexity here because there uh when a process has a lock for a critical section um it takes a time to live for that lock and while holding the lock a writer Can it can crash right or it can pause indefinitely for any reason you know you could have other processes on the machine then you could be thrashed by the scheduler on the OS and so on so when this happens we need to have a custom put if absence semantic with a repair mechanism on the Dynam DP lock itself to make sure that if a process crashes or pauses while holding the lock we don't have um we don't have data corruption um I have a link in here for the actual for another talk that describes the put of absent semantic in detail so I recommend you all take a look at that and if you have questions on this please find me after so that's how data gets into our system we have writers we have low latency we have uh High latency batch writers and both of these kinds of writers are writing to our Delta stores and all of this data is available so let's talk about data Discovery and data access so like I said we have a data catalog we have a neural likee python client and we have visualization tools so all of this allows internal users to be able to access our data without needing to ask a bunch of people where data is stored or know all of the different Delta tables all of the different Park files that lie in uh on our blob store and so on we want to make data access really easy internally at the company so I'm going to start off by talking about the Nur Lake python client so the Nur python client um has a very simple structure we have a catalog which can have many databases which Each of which can have many tables so this is a very standard uh data catalog structure and a table is really just a way to get a data frame via the neur like python client so it's just an abstraction to be able to get a data frame and when we use the python client it looks something like this we have a very clean API we have the ability to get a data frame for a specific database for a specific table in this case this table is called normalized band power you can pass in some filters and you can collect the data frame and the return data frame is actually a subass of the Polar lazy frame so polar is another rust based data frame library and polar is very similar to pandas but much much more performant um because it's written in Rust it allows for very very powerful um concurrency and it is able to process data very quickly and the lazy frame allows us to LA execute these data processing queries so queries are only run once collect is called and evaluated lazily and we've subclassed that so every table returns a Polar Polar lazy frame that can then be executed on and something we have planned is to actually do push down queries where we actually filter on the parket files themselves before even pulling them over the network so we can lazily evaluate and op even our Network calls so how do you add tables like so you saw that table it was uh normalized band power right here right so how did somebody add that well it's super easy so all a developer has to do once they've set up their data inje pipelines once they have data being written is they just have to write a single line of code they we have declarative syntax that looks like this where they can declare a paret table or declare a Delta table we support both uh and they can essentially just give a name the URI the partitioning scheme and some documentation information for the documentation that's automatically generated so this is a single line of code it can take a developer maybe a few minutes to write and their table will be available in the catalog we also have custom functions that can be a part of our catalog so you might imagine a view that involves consuming for multiple Delta tables or multiple paret tables um you might have a function that Crees a Google sheet or a relational database and get some data back we want to make that data also available for internal users so in this case we have a function it's called robot insertions the doc string is automatically pulled into our documentation and all they have to do is add the at table annotation at the top and once they add The annotation that is automatically registered as a table in our catalog and available inside of our catalog it really could not be easier and the catalog looks like this so you can see we have an autogenerated catalog we have all of our databases all of our tables and internal users can just go onto our internal site they can access this catalog this catalog does not hit a database to get its um information this catalog is a static site that's generated um in our C and deployed and does not have um any any kind of backing to it it's autogenerated from the code that you saw earlier and in the catalog we have the view of a single table and you can see here that we autogenerate the code for the python client as well so an internal user can literally go to the catalog browse a few tables looking for the data they want once they find the table they can just hit that copy button they can go to jupyter HUB which we host internally and they can just paste that code into a jupyter notebook and sorry and once it's in the Jupiter notebook the um the code automatically can be executed and the jupyter Hub instance is already authenticated to S3 it's already authenticated to the buckets it needs to access so an internal user doesn't need to Wrangle with credentials or logins they don't need to they don't need to worry about where the DAT is stored just copy paste analyze data so in addition to our catalog we have readon apis that are also generated from our table definitions so when developers Define a table in Python we create readon apis and these readon apis accept HTTP SQL queries they also accept the post wire protocol and this means that dashboarding tools like grafana can be configured to point to R readon apis so just to really emphasize the Simplicity here a developer all a developer has to do is add a single function like this or a single line of code a single command like this to Clare atively and that is already available in the catalog automatically and it's also available in the dashboard the developer doesn't have to do anything else and internal users can then set up their own dashboards for this data source so I want to Deep dive into how exactly this works and at the center you can see that python client really is the center of all of this the python client which contains the table definitions generates everything you see here and no additional action is needed from the developer again from the from a developer experience this is simple I just add a function I don't need to worry about anything else so the python client itself uses polers and P arrow in order to query the Delta tables and the parquet files so with a single declarative command a a developer can create a table definition and we've already written the code to automatically configure that to be a pi Arrow query that fetches from uh from S3 or from whatever data source you have in mind and the result is a polar's lazy frame that we have that lazily evaluates the query once the user actually wants to do something useful with it and once that table is defined in the catalog then we generate the catalog website and all we do is we take the table definition that you saw where we have the annotated function or we have the declarative Syntax for that table and we just serialize that to Json in CI so once you merge your code to to the main branch we serialize the result to Json and then we generate the static site and we deploy the static site it's easy no database to manage for the catalog nothing the site is up and the catalog is super responsive and doesn't require any additional maintenance and users can query use the python client directly so internally somebody wants to just pip install neural Lake they can do that on their own local machines they can use Jupiter Hub Jupiter Hub comes pre-loaded with a Nur Lake Library they don't need to install it on their notebook it's just everything is just ready one click to copy one click to paste one click to run very easy and in CI we also autogenerate a row API config so some of you might have heard about row API but for those who don't know row API stands for readon API and uses Apache data Fusion which is a um which is a query library that executes um executes logical plans on data and is written in Rust and what we do here is we generate a row API instance and row API accepts HTTP requests containing SQL it accepts graphql queries and it also accepts the Press wire protocol so you can literally configure a PC cool client connect it to our neural Lake stores which is autogenerated from a single line of developer uh a developer wrote to add a table and you can pretend like you're interacting with a postest database so that is incredibly powerful and Ro API stateless you can scale it up scale it down just by adding more uh more instances and Roy API allows for really fast querying and then grafana the dashboarding tool can simply plug into Roy API because Roy API takes HTTP requests um containing SQL it can be configured as a grafana data source and the grafana dashboard can then just be built by users so if I'm a developer and I just created for example a new way to ingest manufacturing data from our manufacturing machines on our line and all of this data is being streamed into my Delta lake table all I have to do is add a single line and I can tell the manufacturing team hey you guys get dashboards you guys get jupyter notebooks you guys get an HTTP API you can access data from and it's all really easy people who aren't software Engineers but know a little bit of python know a little bit of scripting can use the python a python client they can use Jupiter Hub people who don't know any python at all can use grafana and configure their own dashboards and this just makes it really powerful for us to access all of our data internally so this allows us to build the best product we can because our data is so complex that we really need to be able to make data access at the company incredibly easy otherwise we would not be able to make good decisions and we would not be able to help millions of people so in conclusion the way we do this is we Define tables in code everything is defined in code One Time One command one function and we can generate a catalog API and dashboarding tools and we don't need to maintain a catalog database at all and read and writs both scale down to a laptop so you can run the neural client on your laptop you can if you're developing a writer you can run that on your laptop too the neur lake writers that write data to Del Lake can run as a single process on your laptop very easy there's no overhead of a jvm there's no overhead of trying to run spark locally and this means that we can also scale it up to a stateless cluster and these clusters scale right because they're stateless you can just add more add more instances and this flexible design makes it really easy to extend all of our systems so you can add offline batch processing you're not limited to just Delta Lake and real-time event uh ingestion you going to have B offline batch processes running an airflow which we do you can have any kind of backend for your data you can have a Google sheet and just have the neur lake python client read from that Google sheet if that's useful to you you can read relational data we have relational databases we have highly transactional workloads that we need to have in relational databases we don't want to exclude those from our data Lake and all you have to do is just add another line to your python client very easy and the rust based systems that I talked about so polers Apache data Fusion Delta RS really allow for high performance data access um writing code in Rust means that you can write write code with high concurrency without worrying about race conditions because of the inherent safety of the language and it means that we can build systems that are composable libraries that can just be imported into a developer machine and then run in production and scale up and down without needing to worry about the overhead of larger systems and you know the the real uh Champion here is Delta l which allows for us to do all of this because it allows us to have asset transactionality on blob stores we can have commits across multiple files on our blob stores and we don't need to maintain a database server we don't need to maintain a big cluster that stores our database that we need to maintain so I do want to thank the Nur Lake team so we have some of the most brilliant people that I've ever worked with in my career on this team and they've contributed immensely to this effort and I feel incredibly privileged to be able to work with people like this every single day I think we've built a really unique culture at our company where we have a really high bar for talent we have really talented individuals who are incredibly mission-driven and determined to help millions of people around the world and who are interested in building simple systems to really manage our uh really manage our data and again simple isn't easy and everybody here has worked incredibly hard to build these systems and finally uh before we go to Q&A I I want to leave you all with some quotes um these are from uh participants in our clinical trials we have started clinical trials in the United States as of this year and there are people that benefit from these devices whose lives have been changed by the by the use of our device and this is honestly what keeps me going this is what keeps most people at our company going is knowing that we can help people in this way and knowing that we can change people's lives and plan to change many many more people's lives one more quote and to be honest I get a little emotionally even thinking about it because it really is it really is a privilege for us to be able to work on such amazing engineering work and work on bleeding edge Technologies and also help people in the most fundamental and meaningful of ways possible and to really have that be the way I spend most of my waking hours is an incredible privilege and and I I I personally feel incredibly lucky and most of my colleagues also feel incredibly lucky with with the way um with the way to spend their time so if that kind of an engineering culture interests you if you want to help billions of people around the world if you want to work on these kinds of bleeding gadge Data Systems we are hiring so I encourage you all to go to ning.com careers and I will be available after this talk so please come and find me as well and I think we're out of time thanks everyone for all your questions great thank you all so much [Music]",
    "status": "success",
    "error": null
  }
]