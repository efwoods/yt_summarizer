2[Music] hey everyone thank you all so much for coming my name is gam I'm a software engineering lead at neuralink and today we're going to be talking about neuro Lake which is our internal data platform and the recurring theme through this presentation is going to be that we are building really simple systems for really complex data and simple doesn't mean easy and I really want to show what we've built uh over the past couple years so to start out I can introduce myself so I'm Gotham I'm a software engineering lead at neuralink for our full stack internal development team and our data engineering team so our team does a whole bunch of stuff it's all really interesting it's a really fun team to work on lots of unique verticals to drive we do data engineering some of which I'll talk about today we also do manufacturing software we do full stack development for lab Lab management tools we build visualizers for a billion pixel images um we have a lot of different things going on and yeah I can't even cover a tiny percentage of it today so if you have more questions please feel free to come and ask me afterwards so before we talk about neural Lake let's talk about neuralink so I'm sure some of you have heard about neuralink in the news um but for those who haven't so neuralink is a company creating a generalized brain interface and what we mean by that is a generalized IO interface into the human brain that can read and write signals from the brain and the reason we're doing this is to restore autonomy and help those with unmet medical needs there are millions of people in the world that suffer from not having their medical needs met people with spinal cord injury people with blindness people with ALS and more and what we want to do is solve these ailments of the brain and of the body with a generalized brain interface and in the long term we really want to unlock human potential for what it means to be a human and really unlock what the human species are capable of so how are we how are we doing this well we have a stack um and the nuring stack is composed of three main components so on the left here you can see that we have our device is called the link and this is an invisible device that is surgically implanted into the cortex of the brain you can see that there is a chip it's a circular disc about the size of a US Quarter and that chip has long thin electrodes coming from it those threads are extremely thin much smaller than human hair extremely flexible and can be inserted into the brain with minimal um minimal invasiveness and these threads have electrodes on them that can sense brain activity so as neurons activate we have signals going up these threads into the chip and the threads are inserted via a surgical robot these threads the these threads are too thin and too flexible to be inserted by a human hand so we've built our own Neurosurgical robot to insert these threads avoid vasculature and move with Micron precision and once these threads are inserted and reading brain data we send signals over Bluetooth over to neuros Signal decoding software and this software can classify brain signals into something useful in the external world this might be moving a cursor on a screen this might be moving a robotic arm there are many things you can do once you have these signals on a computer and the purpose is to decode neural intent and again the purpose of this is to restore autonomy so we want to help people with unmet medical needs and two cases here that we have are um telepathic control of a computer cursor as you can see on the left for people who have motor impairments and are unable to move their arms in a way to control a computer we want to allow for telepathic control and this year we did start our first clinical trial and we have one person who is actually benefiting from um this therapeutic case and another example of unmet medical needs that we are hoping to address is the ability to cure blind you can imagine if you have a perfectly working visual cortex but have other issues with your uh retina or optic nerve that prevents you from being able to see we can send signals directly to the visual cortex and stimulate the brain and induce Vision so both telepathic control and restoring Vision are two applications of this device and you can imagine because our brains make up a core part of who we are and what we do um there are many many many applications for helping people around the world so let's talk about data at neuralink and you can imagine there's a lot of data involved here right we have data from the brain we have data from our devices and we have a lot of data to manage and the data is incredibly complex so neuralink the neuralink stack I just described is one of the most complex medical devices to have ever existed and we have correspondingly complex data we have of course the brain signals and the signals that come from the device we have Telemetry coming from the device that's that's um safety critical that's indicating um whether the device is functioning properly we also have the brain signals themselves and we also have manufacturing data as we manufacture our devices we have information about the manufacturing history and machines on the manufacturing lines we have histopathology data that is large billion pixel images of tissue that um we use to assess the safety profile of our devices we have data collected during surgeries to make sure that our surgeries are safe and effective and more this data is incredibly multimodal it's incredibly complex we have images we have video we have huge images we have logs that uh that end up being terabytes large so this data is very very very complex it's not just simple tables or simple log stores there's a lot of different modalities we need to address here and we have a really strong design Philosophy for our Data Systems we want to have simple systems and again simp Le doesn't mean easy but simple means that we want systems that can scale down to a single developer machine and up to stateless clusters the development experience for a developer who is building the system should be seamless running something on your local laptop should be the same as running something in production and we want to really prioritize the local development experience instead of having large distributed services that we need to put up like a spark cluster to be able to access our data which runs in a jvm and can run slow on a local laptop we want to make sure that we prioritize the local development experience by using composable libraries and we want to avoid large stateful distributed clusters for our data lakes and warehouses we don't want to have a a database cluster that we need to maintain in order to store and access our data and finally we want to have code that's a catalog in and of itself and what we mean by that is we want to be able to generate a catalog that contains all of our data sources so internal users can access this data and Discover it and we don't want to have a database server and like all this complexity to maintain when we have this catalog application we want to just be able to define a table in code and have that generated catalog so you can see the Simplicity here from the end user experience internally when users access our systems or when developers build the these systems they can replicate uh production environment locally they can build a system easily using composable libraries and end users can access data without us having to maintain a bunch of different services so there's three main prongs that I'm going to cover here today we're going to cover ingestion Discovery and access so ingestion is the ability to ingest data into our systems so we want to have low latency streaming ingestion pipelines and we want to have an elegant way to handle schemo versioning and backfilling of our data lakes and as many of you can guess we we're going to talk about Delta Lake a little bit as a part of this we also want to allow for Discovery now with all of this complex data it it's really hard for people to know where all the data lives no one person knows where all the data lives in their heads right they need to be able to organically have a certain query or certain problem they're trying to solve and then look at our or look at our catalog and find data organically so we want a simple data catalog that's generated from source and we want low code dashboards that people without software experience can build that are generated from our catalog definitions again a single source of Truth for all of our dashboarding data cataloging and so on and then it should be easy to access now a software engineer can of course write a query to a Delta table and get get some information but we want everybody at the company to be able to access our data so our goal is to have a single click to get a data frame you click a button copy some code and you have a data frame on your computer that you can you can play with and query we also want to create autogenerated SQL and rest apis from our data catalog so that our internal dashboards can query these apis and again using the single source of truth of the data cat catalog so this is a 32,000 foot view of what our neural Lake data platform looks like um and this is really just a part of it again I can't cover all of the complexity of our data um today but we have data sources we have offline batch writers and low latency writers for processing that data and putting it into our data data stores and we have our data stores themselves we have Delta Lake we have relational stores uh and so on and something you'll see from this talk is that our our data stores can be spread across multiple different instances so we can have data in Google Sheets if we have data in spreadsheets we can have data in Delta Lake we can have data in postgress tables if we have high transactional needs which we do and all of this data is then easily discoverable and accessible via a data catalog via a python client and Via visualization tools and what you'll see is that the the catalog the python client and the visualization tools all come from the same Source there's a single source of Truth for defining these and then everything else is autogenerated which makes maintaining these systems very easy so first I'm going to focus on the data sources the low latency writers and how that data ends up in our Delta Lake stores and there's a lot more to talk about with offline batch writers and so on so if you're interested in those use cases find me afterwards so before we talk about our data ingestion let me just give you some primer on our Technologies we use so we have Delta Lake and we use the library Delta RS in our realtime inje pipelines so what is Delta Lake I'm sure a lot of people here are familiar but for those who aren't Delta lake is a open source project and is essentially a file protocol or file format that allows you to have acid compliant trans actions on top of blob Stores um a lot of data Lakes before Delta Lake were just parket files on S3 or on um a different blob store and the downside to that is you can't really get any kind of assd compliance across multiple files right you can't have transactionality across multiple files because these systems only allow for atomicity for a single file and you might might have partial rights if clients are reading from multi multifile stores while you are also updating multiple files at once and we don't want that so with Delta Lake we have a transaction log and park files on S3 and as long as the client and the writer both uh follow the protocol which means that they both read from the transaction log before pulling the paret files we can ensure that all of the Delta rights are shown in a transactional way so you'll you'll never have partial rights seen by the client and the Delta l prot is implemented by Delta RS so Delta RS is a rust library that implements the Delta Lake protocol it allows for writing and reading of Delta Table stores so the Delta RS library takes care of both reading and writing the Delta Lake updating the transaction log doing Atomic commits across multiple files and so on um the Delta RS library is written in Rust it's incredibly performant and it has uh it's highly concurrent and it also has python bindings so you can import the Delta Lake Library uh in Python and you can run it locally on your machine you can run it in a stateless cluster and the developer experience is the same so we're going to talk about a use case for Delta Lake we have multiple Delta tables each of them have slightly different use cases at the company we're not going to cover all of them but I do want to cover one interesting use case that we have so this is real time or close to real-time data ingestion into our Delta tables so on the left you can see that we have the and one implant being used um and while it's being used we have data being emitted we have events being emitted from the device these may contain neural signals and we also have events being emitted from the usage of a computer so the individual using the neuralink device is going to be playing a game on their computer they're going to be moving a cursor in some way and information about that cursor and about uh their neural signals will be being sent um into an ingestion API and this ingestion API stages these events onto a message queue then which are then consumed by a writer in batches written to Delta Lake stores and downstream those are consumed by rust based query engines and the purpose of this is to train a model to better calibrate to the neural signals by a user so as we have neural signals leaving the device we also have events on the game itself from the cursor movement and we want to align the time stamps of these and create labeled data that can be trained on so this whole pipeline uses Delta Lake as the main store for the um label data which is then queried via Delta RS or um uh other rust based query engines and that data can then be trained upon and this whole pipeline makes it so that the label data is available in seconds so in a few seconds we have all of the data available for training and this is is updated live and the reason we want such real-time availability is because during a session our Engineers might train multiple models and we want to be respectful of somebody's time if we're we're engaging in a session with them and we want to make sure that we can train a model as quickly as possible to try to converge on an optimal solution so let's really focus on the message cu the writer and the Delta Lake instance so we're going to talk about how the writer actually writes to the Delta Lake instance while consuming from the from the message CU and how the endend flow works for data to arrive in the Delta Lake store so the neural leg writer instance has three primary processes we have a writer process a compaction process and a vacuum process now these three processes each have a different function the writer process as you might guess consumes batches of messages from the message q and writes them in batch to the Delta L table now there's a problem here we have messages arriving very rapidly um in real time and if we write a new right to Delta Lake for every batch of let's say like 10 messages then we'll end up with a lot of small files in our Delta table and if we have a lot of small files in our Delta table then we have a problem because Downstream readers need to read a bunch of small files on S3 and that means that we have high latency for our reads because we need to do individual get requests for multiple files so instead of doing that we have a compaction process that once files are written to S3 at some Cadence right now we have it configured to be once a minute um but this can be tuned we have a compaction process that reads all of the files within a partition and compacts them into a single file now in Delta Lake this is possible because again we have assd compliance across multiple files this wouldn't be possible POS if we had a bunch of just park files on S3 right so in this case we read all of the files that are uh in the Delta table compact them and write back a single file and now when clients read that single file um it's uh it's it's very fast to be able to um get that single file into into memory now there's a problem though so with a compaction process running alongside a writer process we need to make sure that all of the commits themselves are serialized now remember Delta lake has a commit file and it has parquet files and both readers and writers access the um the the commit file the transaction log before they access the parquet files now if two simultaneous processes are trying to write to the same file you could have a conflict and this could lead to data loss right so we need to make sure that we have a serialized commit even if our rights are happening simultaneously so the compaction process takes a long time it takes some time for compaction to run but that can run in parallel while rights are continuing to come in and once the compaction is complete and the compacted file is written to S3 we can then serialize a commit and commit to our transaction log and we do that by we have a there's a custom put if absent semantic in um in the in the Delta RS Library where we use Dynam DB and we take a lock on on the um on on the commit file itself and once we take a lock on that commit file we can then commit and then release the lock and then the other process can can write and there's some more complexity here because there uh when a process has a lock for a critical section um it takes a time to live for that lock and while holding the lock a writer Can it can crash right or it can pause indefinitely for any reason you know you could have other processes on the machine then you could be thrashed by the scheduler on the OS and so on so when this happens we need to have a custom put if absence semantic with a repair mechanism on the Dynam DP lock itself to make sure that if a process crashes or pauses while holding the lock we don't have um we don't have data corruption um I have a link in here for the actual for another talk that describes the put of absent semantic in detail so I recommend you all take a look at that and if you have questions on this please find me after so that's how data gets into our system we have writers we have low latency we have uh High latency batch writers and both of these kinds of writers are writing to our Delta stores and all of this data is available so let's talk about data Discovery and data access so like I said we have a data catalog we have a neural likee python client and we have visualization tools so all of this allows internal users to be able to access our data without needing to ask a bunch of people where data is stored or know all of the different Delta tables all of the different Park files that lie in uh on our blob store and so on we want to make data access really easy internally at the company so I'm going to start off by talking about the Nur Lake python client so the Nur python client um has a very simple structure we have a catalog which can have many databases which Each of which can have many tables so this is a very standard uh data catalog structure and a table is really just a way to get a data frame via the neur like python client so it's just an abstraction to be able to get a data frame and when we use the python client it looks something like this we have a very clean API we have the ability to get a data frame for a specific database for a specific table in this case this table is called normalized band power you can pass in some filters and you can collect the data frame and the return data frame is actually a subass of the Polar lazy frame so polar is another rust based data frame library and polar is very similar to pandas but much much more performant um because it's written in Rust it allows for very very powerful um concurrency and it is able to process data very quickly and the lazy frame allows us to LA execute these data processing queries so queries are only run once collect is called and evaluated lazily and we've subclassed that so every table returns a Polar Polar lazy frame that can then be executed on and something we have planned is to actually do push down queries where we actually filter on the parket files themselves before even pulling them over the network so we can lazily evaluate and op even our Network calls so how do you add tables like so you saw that table it was uh normalized band power right here right so how did somebody add that well it's super easy so all a developer has to do once they've set up their data inje pipelines once they have data being written is they just have to write a single line of code they we have declarative syntax that looks like this where they can declare a paret table or declare a Delta table we support both uh and they can essentially just give a name the URI the partitioning scheme and some documentation information for the documentation that's automatically generated so this is a single line of code it can take a developer maybe a few minutes to write and their table will be available in the catalog we also have custom functions that can be a part of our catalog so you might imagine a view that involves consuming for multiple Delta tables or multiple paret tables um you might have a function that Crees a Google sheet or a relational database and get some data back we want to make that data also available for internal users so in this case we have a function it's called robot insertions the doc string is automatically pulled into our documentation and all they have to do is add the at table annotation at the top and once they add The annotation that is automatically registered as a table in our catalog and available inside of our catalog it really could not be easier and the catalog looks like this so you can see we have an autogenerated catalog we have all of our databases all of our tables and internal users can just go onto our internal site they can access this catalog this catalog does not hit a database to get its um information this catalog is a static site that's generated um in our C and deployed and does not have um any any kind of backing to it it's autogenerated from the code that you saw earlier and in the catalog we have the view of a single table and you can see here that we autogenerate the code for the python client as well so an internal user can literally go to the catalog browse a few tables looking for the data they want once they find the table they can just hit that copy button they can go to jupyter HUB which we host internally and they can just paste that code into a jupyter notebook and sorry and once it's in the Jupiter notebook the um the code automatically can be executed and the jupyter Hub instance is already authenticated to S3 it's already authenticated to the buckets it needs to access so an internal user doesn't need to Wrangle with credentials or logins they don't need to they don't need to worry about where the DAT is stored just copy paste analyze data so in addition to our catalog we have readon apis that are also generated from our table definitions so when developers Define a table in Python we create readon apis and these readon apis accept HTTP SQL queries they also accept the post wire protocol and this means that dashboarding tools like grafana can be configured to point to R readon apis so just to really emphasize the Simplicity here a developer all a developer has to do is add a single function like this or a single line of code a single command like this to Clare atively and that is already available in the catalog automatically and it's also available in the dashboard the developer doesn't have to do anything else and internal users can then set up their own dashboards for this data source so I want to Deep dive into how exactly this works and at the center you can see that python client really is the center of all of this the python client which contains the table definitions generates everything you see here and no additional action is needed from the developer again from the from a developer experience this is simple I just add a function I don't need to worry about anything else so the python client itself uses polers and P arrow in order to query the Delta tables and the parquet files so with a single declarative command a a developer can create a table definition and we've already written the code to automatically configure that to be a pi Arrow query that fetches from uh from S3 or from whatever data source you have in mind and the result is a polar's lazy frame that we have that lazily evaluates the query once the user actually wants to do something useful with it and once that table is defined in the catalog then we generate the catalog website and all we do is we take the table definition that you saw where we have the annotated function or we have the declarative Syntax for that table and we just serialize that to Json in CI so once you merge your code to to the main branch we serialize the result to Json and then we generate the static site and we deploy the static site it's easy no database to manage for the catalog nothing the site is up and the catalog is super responsive and doesn't require any additional maintenance and users can query use the python client directly so internally somebody wants to just pip install neural Lake they can do that on their own local machines they can use Jupiter Hub Jupiter Hub comes pre-loaded with a Nur Lake Library they don't need to install it on their notebook it's just everything is just ready one click to copy one click to paste one click to run very easy and in CI we also autogenerate a row API config so some of you might have heard about row API but for those who don't know row API stands for readon API and uses Apache data Fusion which is a um which is a query library that executes um executes logical plans on data and is written in Rust and what we do here is we generate a row API instance and row API accepts HTTP requests containing SQL it accepts graphql queries and it also accepts the Press wire protocol so you can literally configure a PC cool client connect it to our neural Lake stores which is autogenerated from a single line of developer uh a developer wrote to add a table and you can pretend like you're interacting with a postest database so that is incredibly powerful and Ro API stateless you can scale it up scale it down just by adding more uh more instances and Roy API allows for really fast querying and then grafana the dashboarding tool can simply plug into Roy API because Roy API takes HTTP requests um containing SQL it can be configured as a grafana data source and the grafana dashboard can then just be built by users so if I'm a developer and I just created for example a new way to ingest manufacturing data from our manufacturing machines on our line and all of this data is being streamed into my Delta lake table all I have to do is add a single line and I can tell the manufacturing team hey you guys get dashboards you guys get jupyter notebooks you guys get an HTTP API you can access data from and it's all really easy people who aren't software Engineers but know a little bit of python know a little bit of scripting can use the python a python client they can use Jupiter Hub people who don't know any python at all can use grafana and configure their own dashboards and this just makes it really powerful for us to access all of our data internally so this allows us to build the best product we can because our data is so complex that we really need to be able to make data access at the company incredibly easy otherwise we would not be able to make good decisions and we would not be able to help millions of people so in conclusion the way we do this is we Define tables in code everything is defined in code One Time One command one function and we can generate a catalog API and dashboarding tools and we don't need to maintain a catalog database at all and read and writs both scale down to a laptop so you can run the neural client on your laptop you can if you're developing a writer you can run that on your laptop too the neur lake writers that write data to Del Lake can run as a single process on your laptop very easy there's no overhead of a jvm there's no overhead of trying to run spark locally and this means that we can also scale it up to a stateless cluster and these clusters scale right because they're stateless you can just add more add more instances and this flexible design makes it really easy to extend all of our systems so you can add offline batch processing you're not limited to just Delta Lake and real-time event uh ingestion you going to have B offline batch processes running an airflow which we do you can have any kind of backend for your data you can have a Google sheet and just have the neur lake python client read from that Google sheet if that's useful to you you can read relational data we have relational databases we have highly transactional workloads that we need to have in relational databases we don't want to exclude those from our data Lake and all you have to do is just add another line to your python client very easy and the rust based systems that I talked about so polers Apache data Fusion Delta RS really allow for high performance data access um writing code in Rust means that you can write write code with high concurrency without worrying about race conditions because of the inherent safety of the language and it means that we can build systems that are composable libraries that can just be imported into a developer machine and then run in production and scale up and down without needing to worry about the overhead of larger systems and you know the the real uh Champion here is Delta l which allows for us to do all of this because it allows us to have asset transactionality on blob stores we can have commits across multiple files on our blob stores and we don't need to maintain a database server we don't need to maintain a big cluster that stores our database that we need to maintain so I do want to thank the Nur Lake team so we have some of the most brilliant people that I've ever worked with in my career on this team and they've contributed immensely to this effort and I feel incredibly privileged to be able to work with people like this every single day I think we've built a really unique culture at our company where we have a really high bar for talent we have really talented individuals who are incredibly mission-driven and determined to help millions of people around the world and who are interested in building simple systems to really manage our uh really manage our data and again simple isn't easy and everybody here has worked incredibly hard to build these systems and finally uh before we go to Q&A I I want to leave you all with some quotes um these are from uh participants in our clinical trials we have started clinical trials in the United States as of this year and there are people that benefit from these devices whose lives have been changed by the by the use of our device and this is honestly what keeps me going this is what keeps most people at our company going is knowing that we can help people in this way and knowing that we can change people's lives and plan to change many many more people's lives one more quote and to be honest I get a little emotionally even thinking about it because it really is it really is a privilege for us to be able to work on such amazing engineering work and work on bleeding edge Technologies and also help people in the most fundamental and meaningful of ways possible and to really have that be the way I spend most of my waking hours is an incredible privilege and and I I I personally feel incredibly lucky and most of my colleagues also feel incredibly lucky with with the way um with the way to spend their time so if that kind of an engineering culture interests you if you want to help billions of people around the world if you want to work on these kinds of bleeding gadge Data Systems we are hiring so I encourage you all to go to ning.com careers and I will be available after this talk so please come and find me as well and I think we're out of time thanks everyone for all your questions great thank you all so much [Music]